<html>
<head>
    <!-- Global site tag (gtag.js) -- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153791322-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-153791322-1');
  </script>
    <!-- endof Global site tag (gtag.js) -- Google Analytics -->

<style>
.accordion {
  background-color: #000;
  color: #fff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: 1px solid white;
  /* border-top: 0.1px solid black; */
  text-align: left;
  outline: none;
  font-size: 15px;
  transition: 0.4s;
}

.active, .accordion:hover {
  background-color: #000; 
}

/* panel css and js at bottom */

li.sick {
  color: rgb(38, 150, 255);
}

li.audited {
  color: rgb(52,168,83);
}

</style>

  <link rel="stylesheet" href="/static/style.css">
  <meta charset="utf-8">
  <title>Basis Pursuit Â· Tanishq Kumar</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="menu">
<span class="title">Tanishq Kumar</span>
<ul>
  <li><a href="/index.html">Home</a></li>
  <li><a href="/about.html">About</a></li>
  <!-- <li><a href="/culture">Culture</a></li> -->
  <!-- <li><a href="/books.html">Bookshelf</a></li> -->
  <li><a href="/essays.html">Writing</a></li>
    <!-- <li><a href="/notes.html">Notes</a></li> -->
    <!-- <li><a href="/articles.html">Articles</a></li> -->
      <li><a href="/courses.html">Courses</a>
        <li><a href="/papers.html">Research</a>
</ul>
</div>
<div id="left"></div>
<div id="content">

    <h3>Basis pursuit in AI scaling: a defense of human ingenuity</h3>

    <p>
    The release of the o3 model series by OpenAI has garnered much attention. 
    This essay will center around two questions:
    <ol>
        <li>(Q1) What does OpenAI understand that we outsiders don't?</li>
        <li>(Q2) What does "scaling" really mean? Is it fundamentally about having the biggest compute budget?</li>
    </ol>
    </p>
    <p>
        The claims will be that 
    <ol>
        <li>(A1) The story of modern deep learning is one of basis pursuit</i>. 
        That is, modern deep learning is fundamentally about finding the approaches ("bases") that allow for <i>predictable</i> scaling with compute.</li>

        <li>(A2) "Predictable," not "compute," is the operative word in a good modern definition of scaling. 
        Building and using large compute clusters is the <i>easiest</i> part of scaling.</li>
    </ol>
        
    We'll look at how naively scaling up compute and resources often actually <i>fails</i>, but 
    clever (not complicated) methods that allow networks to profitably make use of more compute do win the day. 
    The resulting thesis will be that in AI, 
<blockquote><strong>compute does not unlock progress, progress unlocks compute</strong></blockquote>
    </p>

        <h4>Basis Pursuit: The Matrix Multiplication Analogy</h4>
    <p>
    I mean "basis" in the sense of linear algebra. Let me make this concrete with matrix multiplication.
    Consider multiplying matrices \(A\) and \(B\). Via singular value decomposition, we can write:
    </p>

    <p>
    \[A = U\Sigma V^* \text{ and } B = W\Lambda Z^*\]
    </p>

    <p>
    where \(U\),\(V\),\(W\),\(Z\) are unitary matrices (the basis changes) and \(\Sigma,\Lambda\) are diagonal matrices of singular values.
    The multiplication \(AB\) then becomes:
    </p>

    <p>
    \[AB = (U\Sigma V^*)(W\Lambda Z^*)\]
    </p>

    <p>
    If \(V^*\) and \(W\) align well (\(V^*W \approx I\)), this simplifies to:
    </p>

    <p>
    \[AB \approx U(\Sigma\Lambda)Z^*\]
    </p>

    <p>
    The middle part \(\Sigma\Lambda\) is just scaling -- multiplying diagonal entries. This is the "easy" part, analogous 
    to throwing compute at training. 
    The hard part is finding good basis matrices \(\mathbf{U}\),\(\mathbf{V}\),\(\mathbf{W}\),\(\mathbf{Z}\) that make the 
    multiplication simple.
    </p>

    <p>
    I sense many believe deep learning is about having the biggest clusters. 
    I think this is necessary but far from sufficient. 
    In the matrix multiplication analogy, it's just the "rescale singular values" -- the least interesting 
    or conceptually difficult part! The hard part is finding <i>the right basis transformations</i> -- the architectures and training 
    methods that make scaling <i>predictable</i>. Just like finding \(U\), \(V\), \(W\), \(Z\) that diagonalize the matrices 
    is the key to efficient matrix multiplication, finding the right innovations that stabilize training is key to scaling neural networks.
    </p>

    <p>
    Consider how we've built up this "basis" for stable training one clever innovation at a time: Skip connections allow gradients to flow 
    cleanly through very deep networks. Batch normalization stabilizes activations by adaptively rescaling them. Layer normalization does the same 
    but without batch dependencies. AdamW combines adaptive learning rates with proper weight decay to prevent models from growing unstable. 
    Each of these innovations is relatively simple in isolation, but together they form a powerful basis that lets us scale models predictably.
    </p>

    <p>
    This is exactly like iteratively finding the right basis matrices - each innovation is like finding one more good basis vector that makes 
    the next step of scaling possible. The capability was always there, waiting to be unlocked. But we needed human ingenuity
     to find the right "basis transformations" that would let us actually use that compute effectively. 
    Clever ideas unlock compute, not the other way around.
    </p>

    <p>
    The word "predictable" in "make scaling predictable" is critical because most ways of throwing compute 
    at deep learning systems actually <i>do not improve them</i>:
    </p>

    <ul>
        <li>Making a convNet 100x larger <i>does not make it much better</i> on ImageNet</li>
            <ul>
                <li>However, making a pretrained Transformer 100x larger <i>does make it much better</i> on C4</li>
            </ul>
        <li>Adding more layers to a GAN <i>does not improve</i> sample quality much</li>
            <ul>
                <li>But scaling up diffusion model size and compute <i>can reliably improve</i> generation quality</li>
            </ul>
        <li>Scaling network width to infinity naively <i>makes your model performance worse</i> by converging to a kernel method</li>
            <ul>
                <li>Scaling width and updates in a way that ensures that hidden representations evolve O(1)
                    every training step independent of network width 
                    (muP) lead to <i>predictable</i> performance improvements with size. </li>
            </ul>

        <li>Pretraining on pure common crawl gives terrible returns to additional data, where models often begin to plateau after only a few hundred billion tokens</li>
            <ul>
                <li>The idea that you should filter documents using LM-perplexity based heuristics or, even simpler, 
                    small statistical classifiers (FastText) and that these can tell good data from bad much better than the human eye 
                    is a clever one. 
            </ul>
    </ul>
    
    
</p>
        <h4>Complicated vs clever: The role of human ingenuity in AI scaling</h4>
            
        <p>
        It's often said in jest that the dumb ideas win in deep learning, and 
        clever ideas don't. I think really what people mean to say is that the 
        <i>complicated</i> ideas don't win. There's a big difference between <i>clever</i>
        and <i>complicated</i>. Clever ideas are those that are simple, subtle, and robust. 
        Complicated ideas are hefty, unwieldy, and brittle. 
        </p>

        <p>
        Most seminal ideas in deep learning <i>are clever but not complicated</i>. 
        <ul>
            <li>AlexNet's key insight was deceptively simple: use GPUs effectively. While everyone was chasing algorithmic breakthroughs, 
            they realized the bottleneck was computational. They had to carefully design the architecture to split across 
            two 3GB GPUs since a single GPU couldn't hold the model. The fact they were doing hardware-architectural 
            codesign of neural nets in 2012 is amazing to me. </li>
            
            <li>ResNets' skip connections operationalize a trivial dictum: "only learn what you have to."
            The profound simplicity was in seeing that networks should learn residuals rather than full transformations - 
            this one architectural tweak unlocked reliable signal propagation that had eluded far more complex solutions.</li>
            
            <li>Chain of Thought prompting is laughably simple: just ask the model to show its work.
            Who would have thought that treating language models like schoolchildren would lay the first brick in
            what would become the Church of inference-time compute?</li>

            <li>LoRA's insight feels obvious in hindsight: model updates live in a low-rank subspace,
            so why not just update that directly? This subtle realization about parameter efficiency
            unlocked easy adaptation of pretrained models, making aggressive pretraining scaling practical.</li>
            
            <li>DPO found something hiding in plain sight: human preferences are just binary classifications.
            By stripping away the complexity of RLHF and reframing it as simple binary classification,
            alignment became effortless.</li>
        </ul>
        </p>
    </p>

        <p>
        My internal model is that good ideas in deep learning have a similar aesthetic texture to 
        good literary prose. The best prose is neither purple nor excessively multi-syllabic. It is 
        simple, subtle, and shapely. In deep learning, scale and compute are the judge and jury: techniques that do well as models 
        and systems scale as "the right ones" that nature (and NVIDIA) are nudging us toward. One of the most 
        cutting examples of this is multi-token prediction, in my opinion -- Figure 3 of <a href="https://arxiv.org/pdf/2404.19737">this paper</a> is truly 
        one of the most surprising and delightful figures I have ever seen in deep learning. No wonder every pretraining team 
        is using this technique. 
        </p>

        <p>
The art of scaling in deep learning, then, is not about brute-forcing our way through compute barriers, but about finding 
those elegant basis transformations that tame neural networks and make them predictable. 
Just as matrix multiplication becomes trivial once you 
find the right decomposition, AI progress becomes inevitable once you find the right primitive to scale. 
And if history is any guide, that primitive will probably be 
sitting right under our noses, waiting for someone to have the clarity (not complexity) of thought to recognize it.
</p>

<!-- the examples are mostly simple, not clever -->

<h4>Future avenues for scaling ("bases")</h4>

<p>What does this mean for the future of scaling, and what might be important to work on? 




    Here are some directions I'm excited by and thinking about. </p>


<ul>
    <li>Data quality via synthetic data
        <ul>
            <li>Synthetic data generation techniques like SCPT</li>
            <li>Optimal formats for learning from synthetic data</li>
            <li>Balancing real and synthetic data during training</li>
        </ul>
    </li>
    <li>Heterogeneous multi-agent systems
        <ul>
            <li>If pretraining ~ an individual evolving, and inference-time search ~ an individual thinking, 
                then what is cooperation between individuals toward some goal? 
            </li>
            <li>The point is that net productivity increases nonlinearly with number of humans, 
            partially driven by comparative advantage. I'm very interested in what "comparative advantage"
            between heterogenous foundation models will look like. 
            </li>
            <li>
                "Heterogenous" just means they are models made for different things. This could be anything 
                from models pretrained differently to just conditioned on very different prompts. 
                Heterogeneity is key for a meaningful notion of comparative advantage. 
            </li>
            <li>The most naive instantiation is to just make a multi-agent system 
                debate over rounds, and study how performance scales with rounds or agents, and people like 
                <a href="https://arxiv.org/abs/2305.14325">Yilun</a> have taken canonical first steps in this direction.
            </li>
            <li>    
                This is sort of brute force and a great first thing to study. But I really wonder how 
                two different models A and B can co-operate to produce things no number of 
                copies of A could produce. I don't know quite what is the right primitive to 
                "scale" here, but I think there's something there. 
            </li>
            
        </ul>
    </li>
    <li>RAG (parametric vs nonparametric knowledge)
        <ul>
            <li>Figure 1 in <a href="https://arxiv.org/abs/2407.12854">this</a> paper is a cool example of how performance scales with nonparametric knowledge</li>
            <li>The question of which and how much knowledge we want to store parametrically in our foundation model weights will be an increasingly pressing one in the coming years</li>
            <li>Parametric knowledge can be retrieved fast. I think of this in terms of a cache hierarchy: you choose what you put in model weights just like you choose what put in L1 cache </li>
            <li>What you want to put in weights is determined by what facts are most frequently used in the deployed use-case</li>
            <li>Another analogy is in how FlashAttention works by minimizing IO. I imagine we'll have to make similar design choices around choosing knowledge to keep in weights just like FlashAttention intelligently chooses what to keep in SRAM/HBM</li>
        </ul>
    </li>
</ul>



</div>
