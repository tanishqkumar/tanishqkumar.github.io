
<html>
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153791322-1"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-153791322-1');
  </script>
    <!-- endof Global site tag (gtag.js) - Google Analytics -->



<style>
.accordion {
  background-color: #000;
  color: #fff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: 1px solid white;
  /* border-top: 0.1px solid black; */
  text-align: left;
  outline: none;
  font-size: 15px;
  transition: 0.4s;
}

.active, .accordion:hover {
  background-color: #000; 
}

/* panel css and js at bottom */

li.sick {
  color: rgb(38, 150, 255);
}

li.audited {
  color: rgb(52,168,83);
}

</style>


  <link rel="stylesheet" href="/static/style.css">
  <meta charset="utf-8">
  <title>Noets on synthetic dataÂ· Tanishq Kumar</title>
</head>
<body>
<div id="menu">
<span class="title">Tanishq Kumar</span>
<ul>
  <li><a href="/index.html">Home</a></li>
  <li><a href="/about.html">About</a></li>
  <!-- <li><a href="/culture">Culture</a></li> -->
  <!-- <li><a href="/books.html">Bookshelf</a></li> -->
  <li><a href="/essays.html">Writing</a></li>
    <!-- <li><a href="/notes.html">Notes</a></li> -->
    <!-- <li><a href="/articles.html">Articles</a></li> -->
      <li><a href="/courses.html">Courses</a>
        <li><a href="/papers.html">Research</a>
</ul>
</div>
<div id="left"></div>
<div id="content">

<h3>Notes on synthetic data</h3>

<h4>Etched Notes</h4>
<ul>
    <li><strong>Zuck</strong>
        <ul>
            <li>Open question: we'll want to generate a bunch of data (inference) to use for pretraining in the future, and getting this balance right is an open question.</li>
        </ul>
    </li>
    <li><strong>Dario</strong>
        <ul>
            <li>Post-training is the difference between all the companies.</li>
            <li>"Synthetic data" is just a wrapper on real data when used for pretraining.</li>
            <li>"It is possible to get much more than you started with if you just add small amounts of mutual information"
                <ul>
                    <li>AlphaGo just taking "rules of Go" is enough to go from scratch to mastery</li>
                    <li>ie. wrappers or some combo w/ real and synthetic data is probably optimal</li>
                </ul>
            </li>
        </ul>
    </li>
    <li><strong>Demis</strong>
        <ul>
            <li>Science of synthetic data is being actively worked on, just not in public.</li>
        </ul>
    </li>
    <li><strong>Neil</strong>
        <ul>
            <li>A friend at X said his synthetic CPT is "very, very relevant to what modern pretraining teams are thinking about."</li>
        </ul>
    </li>
</ul>

<h4>Question</h4>
<ul>
    <li>What do you do when you run out of data?</li>
    <li>Two approaches to synthetic data: distillation vs paraphrasing.</li>
    <li>Neil thinks latter is more interesting. Can't really</li>
    <li>Question: how is synthetic data actually being deployed for frontier model pretraining? What synthetic data do you use when training Ll3-405b? You surely can't use things generated by previous generations/smaller models, right?
        <ul>
            <li>But maybe you can rephrase, etc?</li>
            <li>EDU stuff shows there does exist an "optimal format" for learning, and it's not natively what is found on the internet...</li>
            <li><strong>Should really read the Llama-3 technical report carefully to iron out how they are using synthetic data for the biggest model size...</strong>
                <ul>
                    <li>Note that SCPT is really a more sophisticated "paraphrase operator" -- that's probably what synthetic pretraining looks like at frontier labs...</li>
                    <li>Ll3-405b CAN be trained with its own output if "info is added to the system" (Meta does this by static analysis, etc, on its generated code + filtering, but for instance the paraphrase operator or some SCPT like mechanism may be equally good).</li>
                </ul>
            </li>
            <li>They also translate between coding languages (a "paraphrasing" of sorts, ie. bootstrapping using content of the data and capabilities of the model).</li>
            <li>We can think of synthetic version of real data as incurring some "percentage translation cost" compared to an oracle real version. For translated code examples, this is close to zero because a translated C++ version of a Python function would be very similar if a human wrote it and put it on the web... if not even better in terms of quality.</li>
            <li>Studying scaling around the settings which Llama-3 used for 405b seems most relevant to frontier pretraining (and probably already exists).
                <ul>
                    <li>The phi-like distillation studies are another possible angle people are guaranteed to care about.</li>
                </ul>
            </li>
            <li>Basically SD is a "bag of tricks for information injection," and those that work near losslessly allow for a huge increase in the amount of data we have access to.</li>
            <li>Read the report more carefully and make more encyclopedic notes tomorrow... there's a lot to think about here in terms of scaling frontier self-improving synthetic data and seeing how far it can get you (this must be done at frontier labs, and I'm sure a simple scaling study could shed light on some interesting properties of such behavior....)</li>
            <li>May be worth reading the entire report carefully.</li>
        </ul>
    </li>
</ul>

<h4>Nikhil Notes</h4>
<h5>Key intuitions</h5>
<ul>
    <li><strong>Synthetic data works because NTP is incomplete and imperfect as an objective compared to what we want models to do and know at inference-time, so there are things to "patch."</strong>
        <ul>
            <li>Doesn't fully specify/capture what we want at inference-time, so there's still "abilities left on the table." In particular, there is maybe ~80% overlap in skills required for what we train models to do and what we <em>want</em> them to do at inference-time.
                <ul>
                    <li>There is an asymmetry in <strong>ability</strong> in how models reason with data they know given it 1) in context and 2) in weights. The performance IC (eg. via RAG) is an upper bound on performance with data seen during pretraining. We can reach this upper bound by seeing facts/knowledge in various configurations during pretraining.
                        <ul>
                            <li>Since IC models generate the data and IW models consume it, the models generating the data are actually "much stronger" (wrt the datum being reasoned/learned about) than the ones consuming it, so that DPI-like effects need not apply.</li>
                        </ul>
                    </li>
                    <li>We need thousands of examples and augmented presentations of a fact before we learn its natural deductive relations. If models learned all logical relations concerning (A, B) after just presented it once, rephrasing things would add zero value. We can think of SCPT-like methods as "distilling the gap" between the "two different models."</li>
                </ul>
            </li>
            <li>You can't overfit on online synthetic data like you can with offline training, so fragility induced by lack of diversity is only downside, and this is fixable! He said: what's the analog of overfitting, being able to predict the next token in any relational/compositional presentation of knowledge? Well, that's general reasoning, and a desired outcome!</li>
        </ul>
    </li>
</ul>

<h4>Pratyush Notes</h4>
<ul>
    <li>Thinks the main reason synthetic data works is because "LMs are by default OOD" ie. the style of discussion and relations between facts. The big gain from training on synthetic data is you can forcibly reduce the distribution shift compared to inference-time, ie. increase the overlap between pretraining and inference-time distributions.
        <ul>
            <li>Instruction tuning is a "band aid" as if you finetuned an off-the-shelf classifier on a small amount of your data rather than having a large amount in your pretraining corpus.</li>
        </ul>
    </li>
    <li>Synthetic data gains decay much faster than real data (sort of similar to how the highest quality data decayed faster under repetition in DCuratedSL). Cosmopedia and phi folks found literally zero gains after 30-60B tokens, to the point where it was not worth generating more data.
        <ul>
            <li>The intuition is that continued sampling after a certain point from such model weights is like repeating data since you're just repeating "the spirit of the data."</li>
        </ul>
    </li>
    <li>There's levels of decay in utility:</li>
    <li>Ideas that we may want to include are
        <ul>
            <li>ICL degrades due to thinner long-tails</li>
            <li>Incorporating synthetic reasoning traces during PT</li>
            <li>What is the real reason for SD working -- compression, distribution shift, fact augmentation?</li>
        </ul>
    </li>
    <li>Misc
        <ul>
            <li>Training on just code is not optimal for producing a code expert. You want some math and general internet data too to avoid being brittle. This is true for many domains in general (eg. see Skill-It).</li>
            <li>Google wasn't using synthetic data before, but part of the Character acquisition was understanding the know-how for synthetic pretraining, since that was used a lot at Character for persona generation.</li>
            <li>Dataology produces synthetics 2-3x more effective than WRAP.</li>
            <li>Not obvious you even want to ever use the lower quality C4 data at any reasonable compute. Finite capacity; high-qualtiy task-relevant and in-distribution synthetics from a big model would probably be much better than the worst of C4 for almost every pretraining run.
                <ul>
                    <li>Probably data scaling will look like { top 20% of C4, task-relevant and ID synthetics, rest of C4 } in terms of priority.</li>
                </ul>
            </li>
        </ul>
    </li>
</ul>

<h4>Karpathy</h4>
<ul>
    <li>Current models are "silently collapsed" -- they don't generate diverse data, we really need to fix the entropy problem correctly.</li>
    <li>Thinks synthetic data is the future and we don't run out of data, but we have to be careful about it.</li>
    <li>"If we had billions of reasoning traces, AGI would be here"
        <ul>
            <li>Synthetic data ~ generating reasoning traces</li>
        </ul>
    </li>
</ul>

<h4>Model Collapse</h4>
<ul>
    <li>Cool, but contrived. Shows retraining repeatedly leads to tails vanishing.</li>
    <li>Rylan's paper shows if you study instead the related but more realistic setting where outputs "accumulate" then you don't see collapse, so in practice this never really happens or will ever happen even though it gets a lot of PR.</li>
</ul>

<h4>Persona Hub</h4>
<ul>
    <li>This was a surprisingly cool paper. Synthetic generation at scale essentially reduces to diverse prompt generation, where prompts are often just wrappers on some seeds (topics, words, parapraphs, etc) from an existing corpus. This is how people "bootstrap" synthetic data.</li>
    <li>Making synthetic data diverse is hard; people try increasing temperature and all sorts of heuristics, but these only mitigate rather than solve the problem.</li>
    <li>These folks consider an unusual approach: they take internet text and extract from each document in eg C4 a "persona," ie. short paragraph-long description of the kind of person who would read/write that document. They can semdedup these and after going through C4 end up with ~1B fairly distinct personas.
        <ul>
            <li>Each is now a more detailed/powerful version of a "prompt" as you can use a persona as a seed: ie. generate a math problem that persona {X} would enjoy solving. This can be a good way to ensure diversity.</li>
            <li>The hope is that the library of personas represent the library of possible "archetypes" of people that might engage with (and generating the training data for) a language model, so that prompting in "while wearing the shoes of every possible type of user" ensures you extract "all the diverse perspectives" stored within it.</li>
            <li>Another example of a trick to enforce diversity is from the TS paper: they select random pairs of words from the dictionary and force each prompt to include them so that the generative model has to "work hard around the required words" and so diversity has to be in some sense "deeply baked" into the generations.
                <ul>
                    <li>90% of synthetic data papers are just new techniques to eek out more diversity (ie. keep the tails as fat as is the case for natural language) in generations.</li>
                </ul>
            </li>
        </ul>
    </li>
</ul>

<h4>Skill-It!</h4>
<ul>
    <li>Identify primitives somewhere between sample-wise and dataset-wise data selection.</li>
    <li>Study how these skills (eg. "spanish QA") are related to each other in terms of how training on one affects loss of another.</li>
    <li>Propose a data selection algorithm that: estimates the relation between latent skills based on how training on one affects loss on the other, then uses this to estimate the optimal data mix to learn a particular skill as fast as possible (by quickly learning its dependencies, for instance).
        <ul>
            <li>Shows you CAN outperform vanilla online, in the sense that training on spanish grammar, then English QA, then spanish QA, achieves better loss when token matched with training on ONLY spanish QA and doing evals on spanish QA at the end.</li>
            <li>Reminiscent of "biasing the data distribution."</li>
            <li>The "skills graph" that defines the order we select data in has edges between skills (associated with a data set) of how much training on set X improves val loss on Y. If X improves val loss on Y a lot, then we may want to include it in our data sampling method when optimizing val loss on Y.</li>
        </ul>
    </li>
    <li>Broadly, I think that using "val loss reduction on Y by training on X" as a primitive and reasoning only in terms of this level of abstraction is a good idea, and the main contribution of this paper.</li>
</ul>

<h4>Data Filtering Scaling Laws</h4>
<ul>
    <li>One can measure quality as just the reduction in overall C4 modelling loss if you just train on a given subset.</li>
    <li>The key difference of this paper from Niklas' is the modeling of heterogenous data with different data scaling exponents, and being able to predict a mixture's performance with arbitrary repetitions from just its composition.</li>
    <li>I think we want a very similar type of analysis as what we have here, except instead of the scaling axes being { quality (filtering strength), number of repeats } we want to look at the same scaling wrt { intervention strength, generator size } and try reason compositionally in a similar way (motivated by the fact that both control the diversity of generated data). Notice they omit training model size entirely; this may be smart to do.</li>
    <li>They chose VLMs because you have a clear notion of filtering/quality -- CLIP scores. Also interesting that CLIP training involves tens of epochs normally.</li>
    <li>The key ideas are that utility of a data bucket depends on both 1) its quality and 2) how many times you can repeat it (its "diversity")
        <ul>
            <li><strong>A fascinating finding is that even though these are held as independent fitted parameters, in practice they more inversely to each other.</strong></li>
        </ul>
    </li>
    <li>They model both b, Ï for various filtering thresholds (ie. 10-20th percentile, 50-60th) and then reason compositionally about the data scaling of the resulting model.</li>
</ul>

<h4>Allen-Zhu Video</h4>
<ul>
    <li>Knowledge { storage, extraction, manipulation } are three distinct things we'll reason about. Pretty much every sentence/response by an LM will involve this.</li>
    <li>They train on BioS (synthetic) and BioR (real, passed synthetic into Llama to "realify") biographies, each being around a paragraph giving 6 facts about a person. The paper will focus on when and why the model pretrained on such biographies (and QA versions of them) can generalize relationally OOD in a way we'd want. We'll arrive at certain conditions we predict are important during pretraining, and failure modes we expect if we don't include them, and validate them on public models. We train from scratch models of ~100M params on these biographies. They train for ~100 epochs on their dataset of 100k paragraph-long biographies, for a total of a few billion tokens.</li>
    <li>Finding 1. Mixed pretraining.
        <ul>
            <li>Include instruction-tuning data in pretraining to allow models to extract knowledge from what they see in a format you'll ask them to use at inference-time. This is literally "closing the gap."</li>
            <li>The intuition is that normally models store information in weights to optimize NTP accuracy if they see the same or similar set of words in the future. However, this is different from if you expect an entirely different set of words testing the same concept. The QA format includes these different sets of words and pushes us towards weights in the landscape that predict the correct next token under many permutations of the phrasings of the same question.
                <ul>
                    <li>Idea is increasing QA/IT data during pretraining "regularizes us" to go towards these more generalizing weight areas.</li>
                </ul>
            </li>
            <li>They actually suggest the majority being in the desired format, and a smaller fraction being uncompressed representations of raw facts, is the ideal way to enable extraction/manipulation.
                <ul>
                    <li>They find when tracking the learning dynamics that the models when trained this way learn knowledge from the QA then "consolidate" it from the raw fact store and associating it with the relations in BIO. It's kind of like starting to learn knowledge by doing past exams, then really getting an associated comprehensive understanding by consulting the textbook afterwards.</li>
                    <li>Notice they use 80% QA format and 20% corpus; Llama for instance uses 2% QA and 90+% raw corpus, show's there's a HUGE distribution shift that we are trying to make up for using synthetic data.</li>
                </ul>
            </li>
        </ul>
    </li>
    <li>Result 2-3. BIO pretraining and QA finetuning.
        <ul>
            <li>Cannot do QA no matter how much training on only BIO data. ie. knowledge "cannot be extracted." This is even true if you finetune on half the people's QA.
                <ul>
                    <li>This changes if you augment data during pretraining!</li>
                    <li>Suggests finetuning afterwards may not be enough to enable flexible use of language.</li>
                    <li>Intuition is that optimizing under rephrasing "turns overfitting on the augmented data into generalization on the original data."</li>
                </ul>
            </li>
        </ul>
    </li>
    <li>Result 4-5. Probing interp. How is knowledge stored?
        <ul>
            <li>We can probe accuracy for 6 QA tasks at 6 token positions at test time when the model is trying to answer questions. "Compositional" reasoning = being able to extract all 6 answers at the first test token ie. right after seeing the name Anya. "Flawed logic" = not being able to do so (ie. memorizing spurious correlations/associations like going from MIT-Meta to Princeton, etc; technically valid way to predict for this formatting, but may not generalize).
                <ul>
                    <li>They find that models trained on augmented data can compose but not otherwise.</li>
                    <li>If you augment slightly, you can reach a "finetunable range."</li>
                </ul>
            </li>
            <li>Augmentation "makes spurious correlations vanish so that the only way to do well on the training data is to generalize" -- it's a kind of regularization.</li>
        </ul>
    </li>
    <li>Result 6. Celebrity can help minority.
        <ul>
            <li>Including a fraction of "celebrity" personas who have heavily augmented can improve compositional generalization of minority personas even if their data remains unchanged.</li>
            <li>This is maybe why LMs are so good at reasoning -- they have seen augmented data (for celebrities).</li>
        </ul>
    </li>
    <li>Knowledge Manipulation. So far we considered just <strong>storage and extraction.</strong> (ie. pass a QA in flexible formats) Now we ask if/when it can do more complicated manipulations, like inverse or partial manipulation tests.
        <ul>
            <li>Some new tasks: partial retrieval means return just a part of the stored answer; dual is get two answers at once; inverse is obvious.</li>
            <li>One interesting result is that you can often get models that can correctly say the full birthdate of someone, but not just the year.
                <ul>
                    <li>This suggests the prefix "October 2" in front of the year "1989" is an important hint and that its mention prompts the correct year answer. It suggests CoT is really just amplifying the probability of mentioning and hint that will lead to the correct answer by associative means.</li>
                    <li>In their setup, work company determines location, but if they are asked for company then location they can give it, but they can't answer location if they don't see company first. Presentation of facts determines the graphical associative relationships stored, and augmentations make this "logically complete" so models interact with knowledge in all the ways we'd want them to.</li>
                    <li>Especially as tasks get harder, having CoT (intermediate associated tokens resembling patterns seen during pretraining) both during training and inference is crucial for good performance.</li>
                    <li>When you ask for the person's name given their attributes ("inverse search") models have only seen things in forward order, they can't do it. You can check this is robust by flipping the pretraining presentation order and finding the original "inverse search" (now forward search) is suddenly doable.</li>
                </ul>
            </li>
            <li>Bible example is cool: it can do forward and reverse search because it's seen augmented data naturally on the internet.</li>
        </ul>
    </li>
    <li>Key takeaways from the Knowledge scaling paper are
        <ul>
            <li>1000 exposures are needed for full comprehension of relational knowledge</li>
            <li>This number is proportional to data quality: a majority of junk data means even more exposures are needed to understand a subset of the high-quality data relationally/compositionally.</li>
        </ul>
    </li>
</ul>

<h4>Isola</h4>
<ul>
    <li>Main takeaway is that gains from synthetic data vanish really really fast compared to real data (ie. plateaus super fast and aggro).</li>
    <li>They also kept axes {type of generator, temperature, prompt}.</li>
    <li>Sort entirely summarivzed by the above figure. They don't give good reasons or detail on why poor scaling happens.</li>
    <li>A follow up to this by other folks found you can beat the synthetic scaling if you train on the corpus that diffusion generator was trained on.</li>
</ul>

<h4>Synthetic Continued Pretraining</h4>
<ul>
    <li>In practice, a common usecase is finetuning LLM on proprietary data so it can act as a "soft search engine" to improve productivity of employees</li>
    <li>But LLMs kinda suck at memorizing new facts after pretraining, ie. they need to be shown a given fact in 10-100 different ways to fully understand it.</li>
    <li>This paper just takes any small corpus O(1M data points) containing facts we want our LM to understand, and uses that small corpus as a "seed" to generate a big corpus O(100-1000M data points) on which the LM can be continually pretrained on to learn those facts better</li>
    <li>It does so by getting an oracle LM to list the "concepts" in the seed corpus as bullets, then another oracle LM to enumerate all "relationships" between those concepts (edges in a knowledge graph)
        <ul>
            <li>And then using each relationship/concept to generate a document explaining that concept, and adding that to the pretraining corpus.</li>
        </ul>
    </li>
    <li>They find continually pretraining this way on some seed corpus X results in a model that does almost as well as having X available (via, eg, RAG) at inference time as context when answering questions, etc.</li>
    <li>Interesting that a strong intervention with a large model might have similar diversity to a weak intervention. Maybe something like diversity can be modelled as (1-e^(-N/C_N))(1-S)D_0 where D_0 is unit diversity of web text (maximum possible), N is generator parameter count (with C_N a fitted parameter) and S â [0,1] is intervention strength. This reflects how "just rephrasing/fixing grammar" of web text using a 1-2B model has a similar effect on diversity of generated text as "rewriting from scratch only loosely inspired by the seed text" if the generator is GPT4o.
        <ul>
            <li>We can surely construct a phenomenological model of some flavor. Reasoning about scaling wrt "intervention strength" = "generated data diversity" would be the key thing here.</li>
        </ul>
    </li>
    <li>The fact that SCPT approaches RAG accuracy suggests optimal synthetic pretraining isn't far off.</li>
</ul>

<h4>Datacomp-LM</h4>
<ul>
    <li>You can match SOTA frontier models with just choosing the right data. They use ~5x less compute and match the small models from the Llama3, Gemma Mistral series by just very carefully curating data at a large scale.</li>
    <li>They release a dataset with 240T tokens (unfiltered/deduped) from the internet. I didn't even know there was that many!</li>
    <li>Common heuristics for data filtering include reference model-based, human quality heuristics, small statistical classifiers.
        <ul>
            <li>A common technique on pretraining teams is just training the vertices of various data mixtures and doing classical statistics to infer the best one. Over half the energy and talent pretraining teams have is spent on scraping, filtering, cleaning, and organizing data at scale.</li>
            <li>They find that a simple classifier `fasttext` is the best way to select good vs bad data. This is just a linear classifier on n-gram features, for instance a logistic classifier. This often matches or beats reference models or human heuristics.</li>
            <li>There are also other "tricks" that are only privately known; for instance the fact that `gzip` can predict data scaling exponents is a fact exploited by many on the data/pretraining teams at top labs.</li>
        </ul>
    </li>
</ul>

<h4>Perplexity-correlations</h4>
<ul>
    <li>This was interesting, gives me a flavor of the kind of thing that Tatsu Hashimoto likes to do: observational work on LMs in the wild, but with simple but thorough classical statistics as the motivation for the techniques proposed.</li>
    <li>The main claim is basically that the correlation between low loss on a dataset, and high accuracy on downstream evals, is a good measure of whether it's a good dataset (as measured by downstream evals).
        <ul>
            <li>So they sweep the in-domain losses for a bunch of open models on a bunch of datasets (NLL) and do a bunch of downstream evals on each model, and see which datasets have NLL that strongly predict performance on downstream evals.</li>
            <li>Then just sampling those datasets according to how strongly they correlate/predict downstream evals makes, they claim, a good data mixture.</li>
        </ul>
    </li>
</ul>

<h4>Cosmopedia</h4>
<ul>
    <li>Reproduction of Phi line of models from MSFT</li>
    <li>"Most time creating diverse prompts so generated content had minimal overlap"</li>
    <li>Clever ways to seed the model generating data to be diverse are the key. Recall TS did this by choosing random pairs of simple words and forcing the model to weave them both into a generated story to ensure diversity.</li>
    <li>They seek 20M prompts, bc each prompt will generate a 1000 token document that will be used for training. They find seed "topics" from online sources, and then generate multiple prompts by asking LMs to describe such topics at various levels of expertise. They find even this isn't enough to get fully diverse data, though.</li>
    <li>Even generating 25b tokens took thousands of GPU hours! Imagine how expensive to generate foundation-model scale datasets synthetically...
        <ul>
            <li>Am curious how the performance of the resulting model compares with just training on the documents on which the seeds were taken from...</li>
        </ul>
    </li>
</ul>

<h4>WRAP</h4>
<ul>
    <li>This is a good and dense paper.</li>
    <li>Does the "advantage" of WRAP vanish with scale (ie. would gap between methods close with tokens seen)? Is it like a curriculum in this sense?</li>
    <li>Drawback of phi-series is you can't tell if it's the data, or hardcoded narrow topic selection, that leads to good performance?
        <ul>
            <li>Want simple: "all else fixed" experiments benchmarking how synthetic data works across (N, D, mixture percentage)</li>
            <li>Key bottlenecks in synthetic data are <strong>generation cost and topic bias</strong>. Wrapping solves both (you can use a tiny model to wrap and bootstrap using topics on the internet as seeds).</li>
        </ul>
    </li>
    <li>Key ablation results
        <ul>
            <li>Including a significant chunk of real data is crucial (training on some "noisy" or hard to model data, maybe related to ICL, etc)</li>
            <li>The quality of the rephraser matters, but even small rephrasers can beat training on raw data.</li>
            <li>You can only "speed up pretraining" rather than get knowledge you wouldn't otherwise have.</li>
            <li>Synthetic data beats an augmented raw dataset (not convincing experiments).</li>
            <li>Style of rephrasing is significant, but no one style dominates.</li>
            <li>Gains from training longer saturate very fast, so 300B (real+synthetic) may outperform 1T (real) which is very surprising to me.</li>
        </ul>
    </li>
    <li>Key numbers
        <ul>
            <li>Use vLLM to generate 3M tokens/hour on one A100 with Mistral-7B.</li>
        </ul>
    </li>
</ul>

<h4>ORCA 2</h4>
<ul>
    <li>Just a finetune of Llama2 ...</li>
</ul>

<h4>Textbooks are All You Need</h4>
<ul>
    <li>Another canonical paper. These guys train for 8 epochs on (6B filtered code data, 1B synthetic CodeTextbook). I'm suspicious they overfit: not literally in the sense of data contamination, but to the task (simple Python functions).
        <ul>
            <li>I'm not surprised you can learn such a narrowly scope topic</li>
            <li>Feels like most of the params/heavy lifting in LM pretraining is being spent exposing a model to an important fact (ie. how to use a standard API) hundreds of times in different ways.</li>
        </ul>
    </li>
    <li>Emphasis during data-generation for them is on diversity, and being self contained. They get GPT4 to rate a bunch of training examples, train a linear classifier on it and use that for future data filtering (reminiscent of `fasttext`).</li>
    <li>They find finetuning on exercises afterwards extremely crucial for boosting performance. They double check for decontamination, and find none.</li>
    <li>Their 1.4b model that achieves near SOTA for (simple python) is only trained for ~50B tokens, ie. 8 A100s for 4 days!</li>
    <li>Finetuning is an "art" in that it can "unlock" base capabilities out of nowhere.</li>
</ul>

<h4>Best Practices and Lessons Learned on Synthetic Data</h4>
<ul>
    <li>Mostly focused on post-training/FTing.</li>
    <li>"Format of synthetic data is crucial for final performance"</li>
    <li>Synthetic data is particularly powerful when you can verify: that adds MI into the system.</li>
    <li>Mostly focused on post-training/FTing.</li>
    <li>"Format of synthetic data is crucial for final performance"</li>
    <li>Synthetic data is particularly powerful when you can verify: that adds MI into the system.</li>
    <li>Synthetic data scaling shows impressive performance of many over-trained small language models (e.g., Mistral series models, Gemma series models) demonstrates the necessity of training with large amount of tokens (even passing the compute-optimal chinchilla law). However, whether we have similar conclusions on the training with synthetic data is still an open question, as the quality of synthetic data may not be as consistent as real-world data. Future research should investigate the scaling laws for synthetic data and determine the optimal balance between quantity and quality.</li>
</ul>

<h4>Reversal Curse</h4>
<ul>
    <li>One thing that is interesting (ie. appeared in Neil's CPT paper as well) is that using knowledge presented IC is an upper bound on that presented during pretraining. Sort of like "working" vs "long term" memory.</li>
    <li>Intuitively, this means that attending to something IC modifies the forward pass in a certain way. Ideally, pretraining should move the weights to have a similar effect on the forward pass, but it doesn't maybe because the incentive induced by NTP isn't quite right?</li>
    <li>This reversal failure seems pretty robust: they sweep params and models, open and closed, and all exhibit this failure mode. They first try finetuning on new facts and see it holds, then check existing common knowledge facts and also notice that it holds. Simple failure mode that any symbolic program wouldn't make.</li>



</div>

