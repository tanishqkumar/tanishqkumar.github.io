<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153791322-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-153791322-1');
  </script>
  <!-- endof Global site tag (gtag.js) - Google Analytics -->
  
  <!-- MathJax for LaTeX rendering -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  
  
  
  <style>
.accordion {
  background-color: #000;
  color: #fff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: 1px solid white;
  /* border-top: 0.1px solid black; */
  text-align: left;
  outline: none;
  font-size: 15px;
  transition: 0.4s;
}

.active, .accordion:hover {
  background-color: #000; 
}

/* panel css and js at bottom */

li.sick {
  color: rgb(38, 150, 255);
}

li.audited {
  color: rgb(52,168,83);
}
  </style>
  
  
  <link rel="stylesheet" href="/static/style.css">
  <title>Laundry list · Tanishq Kumar</title>
</head>
<body>
  <div id="menu">
    <span class="title">Tanishq Kumar</span>
    <ul>
      <li><a href="/index.html">Home</a></li>
      <li><a href="/about.html">About</a></li>
      <li><a href="/essays.html">Writing</a></li>
      <li><a href="/courses.html">Courses</a></li>
      <li><a href="/papers.html">Research</a></li>
    </ul>
  </div>
  <div id="left"></div>
  <div id="content">
    
<h3>A laundry list for AI research</h3>

<p>
  This is a miscellany of partially fleshed-out but promising (in my opinion) AI research ideas that someone should work on. 
  I would like to know the answer to these questions, but don't have the time to run these experiments myself. If you are 
  looking to break into AI research (eg. as an undergraduate, or a software engineer in industry), these could be some low hanging fruit. 
  Feel free to shoot me a 
  <a href="mailto:tanishq@stanford.edu">message</a> with experimental results if you want to collaborate. 
</p>

<p>
    <ul>
        <li>When should you start a pretraining run from scratch vs from a past checkpoint? Should you lean toward the former over the latter 
          as available compute goes to infinity? The more general question here is how best to use a previous-generation base model $B_T$ when 
          starting a pretraining run for a new generation of models $B_{T+1}$?
            <ul>
                <li>Figure 5[d] in <a href="https://arxiv.org/pdf/2401.03048">this paper</a> shows starting from an ImageNet-pretrained checkpoint 
                  becomes less helpful as compute increases in the setting of latent diffusion models for video generation.</li>
                <li>At the beginning of training, should you start with a distillation-type objective like logit-matching against $B_T$ but then 
                  anneal into a pure next-token prediction objective as compute increases or as $L(B_{T+1}) \to L(B_{T})$? Or do you want to 
                  include gradients from both objectives throughout but just at different mixture ratios? 
                </li>
                <li>Obviously, one way $B_T$ is already being used for sure is in data curation and synthetic data generation for the pretraining corpus 
                  of $B_{T+1}$. But this is not what I'm asking about in this context. 
                </li>
                <li>
                  Do models become "less malleable"
            during pretraining in some way that can be made precise? <a href="https://arxiv.org/abs/2503.19206">This paper</a> of ours gives one answer. 
                </li>

            </ul>
          
          
         </li>
        <li>Measuring scaling laws for environment-time compute in LLM RL</li>
        <ul>
          <li>Traditional scaling laws fit model performance vs compute required to train or serve the model</li>
          <li>But LLM RL involves not just a model, but an environment. Recently, it has become common for the environment to 
            <i>itself</i> to be a foundation models. Examples include LLM-as-a-judge with a rubric, or action-conditioned video 
            generation models, often called "world models."
          </li>
          <li>I'm interested in knowing how the performance of RL improves when the actual model being trained (architecture, hypers, etc) is held fixed, but the compute used to simulate the environment increases.</li>
          <li>This environment-time compute could be either inference-time compute or pretraining compute for the environment model.</li>
          <li>Concretely</li>
          
          <li>And while I'm proposing fitting such a scaling law in the foundation model setting, it is more general than that. In simple and classic tasks learning robotic control (eg. MuJoCo), the environment is typically just a physics simulation. In other words, 
            it's often literally just an ODE solver. Such an environment admits a very simple way to simulate compute: just 
            vary the amount of steps for which you run the solver, which constrains the fidelity of the physics simulation and thus 
            upper bounds the performance of an oracle agent (on the ground truth environment). 

          </li>
        </ul>
        <li>Power-law scaling laws are wrong in many ways, and someone needs to understand why.</li>
          <ul>  

            <li>The power-law form comes from the <a href="https://arxiv.org/abs/2203.15556">Chinchilla paper</a>, but if you look at the justification for that functional form in the Appendix, 
              it's basically a vague gesture to past theory work in small neural networks. Basically it's chosen heuristically. 
            </li>
            <li>As the data-to-parameter ratio $D/N$ of pretraining increases, this functional form seems to be a worse and worse fit for $L(N, D)$ as 
              <a href="https://arxiv.org/html/2403.08540v1">this paper</a> shows. </li>
            <li>This shows that loss decreases slower than you'd expect at large token budgets. Why is this? Is there some theoretical reason? What is the true 
              underlying functional form for pretraining loss? The best theoretical work in this direction I've seen (and which is wildly underrated) 
              is <a href="https://arxiv.org/abs/2402.01092">these</a> <a href="https://arxiv.org/abs/2409.17858">two papers</a>. But even purely empirical work here would be valuable. 
            </li>
          </ul>
        <li>New unsupervised objectives for pretraining that are not just next-token prediction</li>
        <ul>
          <li>
            There have been some cool variants of NTP developed in the literature, and they do work. Examples are 
            <a href="https://arxiv.org/abs/2404.19737">multi-token prediction</a>, and <a href="https://arxiv.org/pdf/2508.19228">token order prediction</a>. 
          </li>
          <li>
            Here is one I tried a while ago that seemed to work (but was only a small win). I'm sure there are many such variants 
            waiting to be discovered. I was interested in improving $k$-shot performance on a task, and training a model 
            directly for that are pretraining/finetuning time. 
          </li>
          <li>
            While ultimately we want to optimize $k$-shot performance on a sequence/generation level at inference-time, I wondered 
            if one could just optimize a similar objective on a token-level to get a next token prediciton variant that has more 
            "diverse" generations at inference-time. 
          </li>
          <li>
            Here's an obvious formulation. If the probability of the true next token is $p_i$, then the typical NTP loss 
            is $-\log p_i$. We can construct an alternative "k-shot" loss as follows. The probability of sampling that true token 
            is $p_i$, and the probability of sampling it <i>at least once in $k$ samples</i> is $1-(1-p_i)^k$. We can decide then 
            to optimize <i>this</i> loss instead: $-\log[1-(1-p_i)^k]$.
          </li>
          <li>This is not mathematically equivalent to optimizing NTP (it is a nonlinear transformation). And it's also not equivalent 
            to optimizing NTP but with rescaled (eg. by temperature) logits. So it's a genuinely new objective, and training on this 
            did improve $k$-shot performance at inference time. I stopped pushing on this because the gains were modest and only appeared at $k \gg 1$. 
          </li>
          <li>But I'm sure I did not mine out this line of thought fully, and would love to see someone else try it.</li>
        </ul>
        <li>Overfitting in latent space during synthetic pretraining</li>
        <ul>
          <li>It's well known you can overfit data by training a language model on it for many epochs. This means train loss will 
            vanish but test loss will diverge.
          </li>
          <li>When training on synthetic data, things get trickier to reason about. I conjecture there exists a notion of "overfitting on concepts" that can have similar effects. The prediction is that if you train on a corpus of real data $D$, you will get loss decreasing in token count as a power law. </li>
          <li>Suppose instead of you have a subset $D' = 0.1D$ of $D$, and use a rephrasing LLM to amplify it back up to $D$ tokens. This means you have some content, and 9 "rephrases" of this content. Of course, the rephrases differ from the content on a token-to-token level, so you cannot think of loss scaling on the rephrased corpus in purely online vs offline (repeating data) 
            terms. 
          </li>
          <li>
            Nonetheless, loss on this new corpus will decrease faster than on the real corpus (an empirical fact). I conjecture 
            this is because there is repetition in some "concept space" where modelling some information is easier once models see 
            that information in many different ways. This is well-documented in <a href="https://physics.allen-zhu.com">this lovely line of work</a>, of course, but I'm interested 
            if one can quantify this effect, and I would <i>love</i> to see someone introduce a notion of actual <i>overfitting on concepts</i> where test loss <i>increasese</i> even when no tokens are repeated. 
          </li>
        </ul>

        <li>Predicting emergence via BoN</li>
        <ul>
          <li>
            This <a href="https://arxiv.org/pdf/2411.16035">fantastic paper</a> shows you can predict emerge of a skill in LLMs in advance by just finetuning on data relevant to that domain. That is to say, 
            the models for whom that skill will emerge fastest (at highest val loss) are exactly those that are most easily finetuned on that data domain. 
          </li>
          <li>
            The cons of that paper are that finetuning is annoying and the fits are very noisy and not always convincing. I think the same predictive method could be 
            applied with best-of-N sampling instead of finetuning, ie. my claim is that the pass@k performance for a model for $k \gg 1$ is a good predictor of 
            the pass@1 performance of that same model as it's trained longer/with more compute. You can predict if a model will have a certain ``emergent" capability 
            later on by simply measuring whether that ability is <i>ever</i> supported in rollouts. 
          </li>
        </ul>
        <li>
          Attention-based synthetic data generation.
          <ul>
            <li>
              Current synthetic data generation methods are super compute intensive in the sense of generating trillions of tokens from a language model, usually 
              rephrasing some (real) seed text or document.
            </li>
            <li>
              Create "synthetic" data by permuting sentences in documents in a way that preserves semantic meaning.
            </li>
            <li>
              "Preserve semantic meaning" = construct a DAG with sentences as vertices with edge A→B iff some token in sentence B attends strongly to some token in A (top-k%).
            </li>
            <li>
              Consider all topological sorts of this DAG as new data points, since sentences are in different orders but semantics (as defined by attention matrix) are preserved.
            </li>
            <li>
              Upshot: can create several permutations of a sequence without decoding, i.e., just one forward pass, so is O(seqlen) faster/cheaper than usual synthetic data generation.
            </li>
            <li>
              I tried this last week: trained a 1B parameter model to 20B tokens with just 1B unique seed text with and without permutation-augmentation. 
              It kind of works as far as loss is concerned (reaches lower minimum) but does poorly on downstream evals compared to repeated real data baseline.
            </li>
            <li>
              Reason for underwhelming gains: attention matrices are less sparse than expected on average, so most semantic DAGs are "almost linear" where almost all permutations kill meaning.
            </li>
          </ul>
        </li>
        
        <li>RL toy synthetics: can we find an algorithmic task with multiple solutions (e.g., clock and pizza) s.t. models of 
          two diff sizes (random init) learn to solve them with two qualitatively different ways? E.g., if 500M discovers clock and 4B discovers 
        pizza, that'd be fascinating and evince how "more is different" in a compelling way.</li>
        <li>I think that one of the most under-rated and surprising empirical results of this year was the fact that <a href="https://arxiv.org/pdf/2405.15618">MLPs can learn in-context</a>. 
        </li>
        <ul>
          <li>This is surprising because the attention mechanism is usually thought to be the key for this (induction heads in MSHA, etc).</li>
          <li>I ran some experiments replicating those findings in small MLPs that had just one hidden layer and as few as 32 hidden units, and 
            found the weight matrices learn a fascinating and structure pattern that matches the nature of the task the authors outline in the paper. 
          </li>
          <li>It showed an interesting mechanism for how MLPs learned the in-context classification and regression tasks outlined in the paper, that amounted 
            roughly to a very clever memorization pattern of the training data. However, I did not go deep enough to formalize the mechanism mathematically and provide a wealth of evidence one might want. But I think there's something interesting here, and someone working on interpretability should definitely 
            take a look and try to write a paper on this, aiming for <a href="https://arxiv.org/pdf/2301.05217">this kind of style</a>. 
          </li>
          <li>MLP-only architectures have the benefit of using hardware much better (high GPU MFU) since matmuls are their only operation. Obviously people have tried 
            stuff like this before (see gMLP or MLP-Mixers) but find that adding attention really is necessary to get maximal performance in the end. 
          </li>
          <li>How does one square these findings with the fact that MLPs can do simple in-context classification and regression tasks? What exactly is then failing in realistic 
            settings making attention necessary?</li>
        </ul>


        <li>Why does MLA match / outperform full multi-head attention, as shown in Section 2.1.1 of the <a href="https://arxiv.org/pdf/2412.19437v1">DeepSeek V3 paper</a>. Shouldn't attending in latent space be strictly worse or less expressive? I don't believe 
          "regularization effects" could be at play here, and I want a scientific/mechanistic answer to this. 
        </li>
        <li>
          Context-as-a-Tool / Learning to Forget 
        </li>
        <ul>
          <li>
            We know that model performance on tasks depends on how much context has been used so far (<a href="https://research.trychroma.com/context-rot">context rot</a>), 
            which is particularly relevant for long-horizon tasks. What if we could RL a model to be aware of when 
            it's getting confused by irrelevant or even adversarial context?
          </li>
          <li>
            The idea: add a tool that allows the model to modify its own context/prompt (e.g., "delete lines A to B 
            of your prompt before embarking on this task") before rolling out. This is "Context-as-a-Tool" or 
            "Learning to Forget."
          </li>
          <li>
            The general paradigm: reasoning is to generating helpful context to condition on as learned forgetting 
            is to omitting context that makes rollouts worse. Both are forms of meta-cognition about what information 
            to use.
          </li>
          <li>
            Surprisingly, this seems unexplored despite being a natural extension of tool-use paradigms. Could be 
            particularly powerful for long-context scenarios where models need to selectively attend to relevant 
            parts of their input.
          </li>
          <li>
            For the reward signal, several approaches could work: (1) Random context edits followed by measuring 
            downstream performance differences—e.g., the delta in perplexity on ground truth completions vs. 
            corrupted ones after the edit. (2) Starting with CoT-based proposals where the model uses reasoning 
            to suggest edits within <code>&lt;edit&gt;&lt;/edit&gt;</code> tool calls, then measuring task 
            performance improvements. (3) Direct optimization on task-specific metrics after context modification, 
            where the model learns which parts of context help vs. hurt performance on the target task.
          </li>
        </ul>
        <li>
          What drives improvements in reasoning performance? Semantics of a worked solution or just idiosyncratic inference-time compute usage?
        </li>
        <ul>
          <li>
            What fractions of gains in reasoning come from the reasoning semantics itself vs the idiosyncratic inference-time compute used by a model 
            doing the reasoning? Put differently, if you conditioned Llama-3-70B on a reasoning trace from GPT-5, or vice-versa, would you still see 
            the same improvements on performance on reasoning-heavy tasks?
          </li>
          <li>
            If you got most of the gains, that suggests the literal semantics of 
            working through the problem are the main outcome of reasoning models, and if you don't that means that idiosyncratic inference-time compute usage 
            is the main reason performance improves.
          </li>
          <li>
            Concrete experiment: Take reasoning traces from o1 on MATH problems and condition GPT-4o on them. Compare performance to GPT-4o's native 
            reasoning traces and to GPT-4o without any reasoning. Repeat with traces going the other direction. Measure performance on held-out MATH problems.
          </li>
          <li>
            I imagine that the answer is somewhere in between, and also varies with "how similar" two models are 
            in their pretraining data/architecture, but I would like to see clean plots to this effect.
          </li>
        </ul>
        <li>
          An eval measuring anti-sycophancy.
        </li>
        <ul>
          <li>One of the capabilities I think most betrays "big model smell" is the ability for a language model to correctly 
          stand its ground when it is correct and the user is wrong. Of course, this must be balanced with the ability to admit and change its mind 
          when it is wrong and the user correctly points that out.</li>
          <li>One can think of the failure to do either of these as a Type I or Type II error of a certain form.</li>
          <li>One simple eval for these kinds of capabilities can be constructed as follows: construct a dataset of user-model interactions involve a technical or 
            factual discussion between a user and a model (so that the correctness of user vs model is verifiable and objective). For instance, a discussion about 
            plot details in Melville's <i>Moby Dick</i> or a discussion about an obscure mathematical theorem. 
          </li>
          <li>
            The last two interactions should be constructed to 
            involve either a model being correct and the user falsely claiming it is wrong, or a model being wrong and the user falsely claiming it is correct. 
          </li>
          <li>Then, give the model you want to evaluate this conversation as context (condition on it) and ask it to complete the system response for the last interaction. </li>
          <li>
            Use an LLM-judge to see if the model acts correctly, or makes a Type I or Type II error. For instance, if the model's answer to a factual question about Moby Dick was correct but the user falsely (but authoritatively) claimed it was wrong, and then the model stood its ground, and reiterated that it was correct, 
            that would increase its overall score for anti-sycophancy on the eval. 
          </li>
          <li>This is closely related to <a href="https://arxiv.org/pdf/2404.00474">linguistic calibration</a> of language models, but I think not quite the same thing.</li>
        </ul>




    </ul>
</p>

  </div>
  
  
  <script>
var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    
    if (panel.style.maxHeight && panel.style.maxHeight !== "0px") {
      panel.style.maxHeight = null;
    } else {
      panel.style.maxHeight = panel.scrollHeight + "px";
    }
  });
}
  </script>
  
</body>
</html>
