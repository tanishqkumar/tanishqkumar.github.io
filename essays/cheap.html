<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153791322-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-153791322-1');
  </script>
  <!-- endof Global site tag (gtag.js) - Google Analytics -->
  
  <!-- MathJax for LaTeX rendering -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  
  
  
  <style>
.accordion {
  background-color: #000;
  color: #fff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: 1px solid white;
  /* border-top: 0.1px solid black; */
  text-align: left;
  outline: none;
  font-size: 15px;
  transition: 0.4s;
}

.active, .accordion:hover {
  background-color: #000; 
}

/* panel css and js at bottom */

li.sick {
  color: rgb(38, 150, 255);
}

li.audited {
  color: rgb(52,168,83);
}
  </style>
  
  
  <link rel="stylesheet" href="/static/style.css">
  <title>Talk is cheap Â· Tanishq Kumar</title>
</head>
<body>
  <div id="menu">
    <span class="title">Tanishq Kumar</span>
    <ul>
      <li><a href="/index.html">Home</a></li>
      <li><a href="/about.html">About</a></li>
      <li><a href="/essays.html">Writing</a></li>
      <li><a href="/courses.html">Courses</a></li>
      <li><a href="/papers.html">Research</a></li>
    </ul>
  </div>
  <div id="left"></div>
  <div id="content">
    
<h3>A Laundry List for AI Research</h3>

<p>
  This is a miscellany of partially fleshed-out AI research ideas that someone should work on. 
  I would like to know the answer to these questions, but don't have the time to run these experiments myself. If you are 
  looking to break into AI research (eg. as an undergraduate, or a software engineer in industry), these could be some low hanging fruit. 
  Feel free to shoot me a 
  <a href="mailto:tanishq@stanford.edu">message</a> with experimental results if you want to collaborate. 
</p>

<p>
    <ul>
        <li>When should you start a pretraining run from scratch vs a past checkpoint? Should you lean toward the former over the latter 
          as available compute goes to infinity? The general question here is how best to use a previous-generation base model $B_T$ when 
          starting a pretraining run for a new generation of models $B_{T+1}$?
            <ul>
                <li>Investigate the trade-offs between starting from scratch vs continuing from checkpoints</li>
                <li>Analyze how model "malleability" changes during pretraining</li>
                <li>Study the relationship between available compute and optimal initialization strategy</li>
            </ul>
          
          
         Do models become "less malleable"
        during pretraining in some way that can be made precise? <a href="https://arxiv.org/abs/2503.19206">This paper</a> I was a part of is one attempt at an answer.</li>
        <li>
          Environment-Time Compute 
        </li>
        <li>See if RL-CR works for humor.
            <ul>
                <li>Scrape a database of jokes from online sources, e.g., r/jokes subreddit.</li>
                <li>For each joke, use a Large Language Model to backtranslate a "joke prompt/context" for which the joke might be a good response.</li>
                <li>Supervised Fine-Tune Gemma 3 27B to take in a "joke prompt" and output a "joke plan" with a few labels generated from GPT-4o.</li>
                <li>Use Reinforcement Learning on Gemma to create a "joke planner" where the reward function for each joke is the relative increase in likelihood of the base Gemma-IT model generating the true joke when given both the joke prompt and joke plan.</li>
                <li>Now that the RL'd version can generate good joke plans, we can feed these plans into the vanilla model along with new joke prompts for the types of jokes we want to generate.</li>
                <li>Compare the jokes outputted by the RL'd model to Gemma-3-IT in a double-blind human trial (or simply perform a vibe check).</li>
            </ul>
        </li>
        <li>Data-time compute.</li>
        <li>Power-law scaling laws are wrong in many ways, and someone needs to understand why.</li>
        <li>New unsupervised objectives, eg. ITC of using $\log[1-(1-p_i)^k]$ as the objective for each token $i$.</li>

        <li>Predicting emergence via BoN</li>
        <li>Learning semantic boundaries to permute docs at DAG-topsort style. Can be learned like tool calls, or just have an LLM insert them 
          on one forward pass. 
        </li>
        
        <li>RL toy synthetics: can we find an algorithmic task with multiple solutions (e.g., clock and pizza) s.t. models of 
          two diff sizes (random init) learn to solve them with two qualitatively different ways? E.g., if 500M discovers clock and 4B discovers 
        pizza, that'd be fascinating and evince how "more is different" in a compelling way.</li>
        <li>RL toy synthetics: find a setting where we can vary $\epsilon$ and construct $\epsilon$-hackable reward models. For instance, 
          unit tests are $\epsilon$-hackable bc they aren't formal verification of programs but do approach golden rewards as they get more comprehensive 
          (in the limit of infinite tests they check every input/output pair and thus are equiv). Want to understand how reward hacking behavior varies 
          (e.g., across scale) as we vary $\epsilon$. This tells us: when and why do we need golden rewards, will RMs always be hacked and does it only 
          depend on $\epsilon$, or on model scale, or both in some interesting way, etc? Sort of a modern "scaling laws for reward hacking" focused on 
          science of synthetics (see the Gao et al paper from a few years ago). 
        </li>
        <li>Science of MLP-ICL and what that means for all-MLP LLMs (these saturate tensor cores and lead to better 
          MFU on avg, so would be good candidates to win the hardware lottery)? 
        </li>
        <li>Why does MLA outperform MHSA? This baffles me, and I want to know the answer, since MLA is supposed to be in some sense 
          a strict compression of MHSA into a latent space. I want a scientific/mechanistic answer to this. 
        </li>
        <li>
          Context-as-a-Tool / Learning to Forget 
        </li>
        <li>
          Can you use another model's reasoning trace to give better answers? 
        </li>
        <li>
          Type I vs Type II eval (measuring "anti-sycophancy")
        </li>



    </ul>
</p>

  </div>
  
  
  <script>
var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    
    if (panel.style.maxHeight && panel.style.maxHeight !== "0px") {
      panel.style.maxHeight = null;
    } else {
      panel.style.maxHeight = panel.scrollHeight + "px";
    }
  });
}
  </script>
  
</body>
</html>
