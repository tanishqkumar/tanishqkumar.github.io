

<html>
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153791322-1"></script>
  <!-- <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> -->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>


  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-153791322-1');
  </script>
    <!-- endof Global site tag (gtag.js) - Google Analytics -->



<style>
.accordion {
  background-color: #000;
  color: #fff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: 1px solid white;
  /* border-top: 0.1px solid black; */
  text-align: left;
  outline: none;
  font-size: 15px;
  transition: 0.4s;
}

.active, .accordion:hover {
  background-color: #000; 
}

/* panel css and js at bottom */

li.sick {
  color: rgb(38, 150, 255);
}

li.audited {
  color: rgb(52,168,83);
}

</style>


  <link rel="stylesheet" href="/static/style.css">
  <meta charset="utf-8">
  <title>TOC, a short reviewÂ· Tanishq Kumar</title>
</head>
<body>
<div id="menu">
<span class="title">Tanishq Kumar</span>
<ul>
  <li><a href="/index.html">Home</a></li>
  <li><a href="/about.html">About</a></li>
  <!-- <li><a href="/culture">Culture</a></li> -->
  <!-- <li><a href="/books.html">Bookshelf</a></li> -->
  <!-- <li><a href="/essays.html">Writing</a></li> -->
    <!-- <li><a href="/notes.html">Notes</a></li> -->
    <!-- <li><a href="/articles.html">Articles</a></li> -->
      <li><a href="/courses.html">Courses</a>
        <li><a href="/papers.html">Research</a>
</ul>
</div>
<div id="left"></div>
<div id="content">

<ul>
<li>Information, intuition-focused notes on TOC.</li>
<li><strong>Big Ideas</strong></li>
<li>If we can represent something as a string, we can represent tuples or combinations of that thing as a sting.</li>
<li>A function is different from a program that computes it.</li>
<li>A program is just a sequential list (or DAG) of logical primitives with whatever basis functions \(\)\mathcal{F}\(\) you decide on. Letting \(\)\mathcal{F} = \{AND, OR, NOT\}\(\) is convienient because it's easy to get physical systems to implement logic like this.</li>
<li>Two models of computation (sets of primitive operations) \(\)\mathcal{F}\(\) are equivalent if they compute the same functions.</li>
<li>Every finite function (finite number of inputs and outputs) \(\)f: \{0, 1\}^n \to \{0,1\}^m\(\) can be computed by a Boolean circuit.</li>
<li>A program is a piece of text, and so it can be fed as input into other programs.</li>
<li><h3>Code</h3> defines computation and <h3>data</h3> is what we compute <h3>on</h3>, so this duality is actually saying <h3>these are in some sense the same. The map is itself the territory!</h3></li>
<li>Many programs exist to compute a certain finite Boolean function, but not all functions have a program that computes them. More formally, the map \(\)P \to F\(\) is neither injective nor surjective.</li>
</ul>
-
<ul>
<li><h3>Fundamentals (1-10)</h3></li>
<li>First note the distinction between a function (a list of input-outputs) and its <h3>implementation</h3> (algorithm/program to compute it).</li>
<li>This is important because some functions are incomputable.</li>
<li>We introduce Boolean circuits as formalized versions of algorithms to implement finite Boolean functions. A Boolean circuit can be thought of as a visual set of logic gates, or as a straight line program with one of the three AON primitives on each line. In this way, a program is not "modelled by a graph," <h3>but is literally defined as a computation graph</h3>.</li>
<li>We can think of an isomorphism between Boolean circuits and computer architecture. Transistors implement physical versions of Boolean circuits, and assembly with `add` or `load` instructions are just a bunch of NANDs under the hood that, as machine code, get executed by these transistors.</li>
<li>We choose to start with AON as primitives because it's particularly easy to see how physical processes can implement such a thing. We can implement operations like these with water flow, gene networks, marbles in pipes, crabs going through a maze, and more.</li>
<li>There's a duality between a Boolean circuit and an "algorithm." Once you fix a set of basis functions \(\)\mathcal{F}\(\) that you can use to transform inputs, a Boolean circuit is equivalent to a straight line program, which is really a set of steps to manipulate the inputs \(\)(x_1^n)\(\) according to the primitives \(\)f \in \mathcal{F}\(\) that you've defined.</li>
<li>To argue that Boolean circuits are enough to compute all finite functions, we need some syntactic sugar. That is, we need to establish Boolean circuits can compute some functional intermediates we'll use again and again.</li>
<li>First note that it's fine to use (non-recursive) function definitions in a straight line NAND program, since we can just plug those into the block where they're used.</li>
<li>We can also use \(\)IF(cond, a, b) = NAND(NAND(a, cond), NANDA(b, NAND(cond, cond)))\(\), which allows us to use branching statements along with function definitions willy nilly.</li>
<li>It turns out we can write ADD and MULT of two length \(\)n\(\) inputs in \(\)O(n)\(\) time (ie. in an AON-circ of size \(\)<cn\(\)).</li>
<li>This seems obvious knowing our grade school algorithms to do these, but it will turn out a surprising fact because our proof that a Boolean circuit can in generality compute any finite function will use an exponential amount of space (we'll essentially memorize the function specification).</li>
<li>The final thing we need to show a Boolean circuit can do is LOOKUP. That is, take in a long bit string of length \(\)2^k\(\), so that a position in this string can be indexed by a string \(\)\text{pos} \in \{0,1\}^k\(\), and return the element at that position. We can define the base case for such a function as just an IF over two elements and then define longer ones recursively based on the first digit of \(\)\text{pos}\(\).</li>
<li>Suppose we have a function \(\)G: \{0,1\}^n \to \{0,1\}\(\) that we seek to implement in a Boolean circuit. Put all \(\)2^n\(\) bit strings and their desired output in a line, and all the line of outputs in order \(\)AS\(\) for "answer string." Then see that \(\)G(x) = LOOKUP_n(AS, x)\(\) by construction. We know this has circuit size \(\)2^n\(\), so we have found a circuit of size \(\)O(2^n)\(\) to implement this arbitrary function. With some work, we can get it to size \(\)O(2^n/n)\(\).</li>
<li>This is the key trick. If we can have our AON circuit implement LOOKUP out of the box, then we can just enumerate a finite function's truth table and construct a Boolean circuit that, given some input will just LOOKUP the right output by construction. This takes four lines in NAND straight line program, and the recursion means at most exponentially many NANDs are present in the full straight-line program that implements the desired finite function.</li>
<li>This might feel a bit like cheating or vacuously true, but the statement that any finite function can be computed using an exponentially large Boolean circuit is a very weak one: using the duality between circuit size and algorithm time complexity, this is just saying we can always compute a finite function in at most exponential time. Remember from algorithms how exponential time complexity (number of primitive steps) was terribly inefficient for <h3>infinite</h3> functions (the usual type), so of course it's going to be abysmal for finite functions.</li>
<li>To be precise, it is a theorem that every finite function \(\)f: \{0,1\}^n \to \{0,1\}\(\) can be computed with \(\)O(2^n)\(\) sized Boolean circuits.</li>
<li>Alternatively, we can just extract the delta function we need from the truth table of the desired Boolean function, cook up a circuit for each delta and put it in a DNF to get the function we wanted. There are at most \(\)2^n\(\) such deltas we need, and the circuit for each is \(\)O(n)\(\) (play around to convince yourself), to get an \(\)O(n2^n)\(\) upper bound. This is a weaker upper bound than above, but still fundamentally exponential in character.</li>
<li>We also note that the exponential bound above is <h3>tight</h3> in the sense that there are some functions we can cook up that require the full \(\)2^n\(\) gates to compute. This requirement for some functions is sometimes referred to as the <h3>counting lower bound</h3>.</li>
<li>Suppose we wanted to prove a particular gate is <h3>non-universal</h3>. How would we do this? We typically find a property that a particular function has, and show that that gate maintains another property that means it can never achieve the first function. For instance, XOR is not universal because it preserves affiness, and AND is not an affine function, so XOR will never be able to simulate AND.</li>
<li>This is called finding an approppriate <h3>invariant</h3>.</li>
<li>Now we move on to the deep code-data duality. Noting that a program is both a set of instructions and a string that can be processed by <h3>any other</h3> set of instructions is very deep. It's like if you had a blueprint for a house <h3>that was also the hammer with which you build the house</h3>. For instance, DNA both encodes instructions but also acts as chemical substrate in reactions, in the words of Schrodinger.</li>
<li>We will appreciate in hindsight that it is this fact that allows us the hope of <h3>building general purpose computers</h3> (and AGI) in the first place.</li>
<li>Chapter 5 (code vs data) is one of the most important in the book so we'll summarize it explicitly.</li>
<li>Key theorems here are: some \(\)f\(\) cannot be computed by anything less than exponentially large circuits, universal circuit evaluators (programs to evaluate other programs on inputs) exist, and size hierarchy (ie. as we allow more gates, we can indeed express strictly more functions).</li>
<li>With that, let's begin with the fact that an \(\)s\(\)-gate circuit can be expressed in \(\)O(s\log s)\(\) characters. This description of a circuit in terms of the characters of its program will be useful to us, even though it might seem strange at first.</li>
<li>Here's an example. Define the set \(\)SIZE_{n,m}(s)\(\) as the set of <h3>functions</h3> \(\)f: \{0,1\}^n \to \{0, 1\}^m\(\) who can be computed by \(\)s\(\) gates (ie. an AON-circ of size \(\)\le s\(\)). We will be able to use the program length characterization to reason about this object, as we'll see shortly.</li>
<li><h3>Theorem 5.2. \(\)|SIZE_{n,m}(s)|<2^{O(s\log s)}\(\)</h3>.</li>
<li>This gives an upper bound for the number of functions a size \(\)s\(\) circuit can be used to compute.</li>
<li>This is important because to show that the upper bound \(\)O(2^n/n)\(\) is tight for some (most) functions, we'll show that the number of functions that can be in \(\)SIZE_{n}(s)\(\) is much, much smaller than the total number of functions \(\)f: \{0,1\}^n \to \{0,1\}\(\), so most of them must not be computable with just \(\)s\(\) resources for various values of \(\)s\(\) that we'll discuss.</li>
<li>In some sense the bound above is obvious. The quantity on the right is the number of possible strings of length \(\)s\log s\(\), of which a tiny amount will correspond to programs for an \(\)s\(\)-gate circuits that compute functions \(\)f \in SIZE_{n,m}(s)\(\). Thus the number of strings is much larger than the number of functions in the set.</li>
<li>We use the above theorem to arrive at <h3>Theorem 5.3. (Counting argument lower bound). There is a constant \(\)\delta> 0\(\) such that for all large \(\)n\(\), there is a function \(\)f: \{0,1\}^n \to \{0,1\}\(\) such that \(\)f \not\in SIZE_n(\frac{\delta 2^n}{n})\(\). That is, the shortest AON-circ to compute \(\)f\(\) requires more than \(\)\delta 2^n/n\(\) lines.</h3></li>
<li>The proof is basically that there are many more than just than \(\)2^{O(s\log s)}\(\) functions.</li>
<li>Formally, let \(\)c\(\) be the constant so that \(\)O(s\log s) = cs\log s\(\). Then let \(\)\delta = 1/c\(\) and using \(\)s = \delta 2^n/n\(\) we use the above Theorem to get that \(\)|SIZE_n(\frac{\delta 2^n}{n}))|<2^{cs\log s} = 2^{2^n/n} < 2^{2^n}\(\), where the RHS is the total possible number of functions \(\)f: \{0,1\}^n \to \{0,1\}\(\).</li>
<li>In this sense, <h3>most</h3> functions require exponentially large circuits to compute. This makes sense because it's rare for a function to have "low dimensional structure" as opposed to being just a random assortment of maps from input to output that can't be described succinctly. ie. for most functions \(\)f\(\), the "best we can do to compute them is memorize them."</li>
<li>We now move onto the <h3>Size hierarchy theorem.</h3></li>
</ul>
-
-


</div>

