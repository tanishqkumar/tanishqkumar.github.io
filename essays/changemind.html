
<html>
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153791322-1"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-153791322-1');
  </script>
    <!-- endof Global site tag (gtag.js) - Google Analytics -->



<style>
.accordion {
  background-color: #000;
  color: #fff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: 1px solid white;
  /* border-top: 0.1px solid black; */
  text-align: left;
  outline: none;
  font-size: 15px;
  transition: 0.4s;
}

.active, .accordion:hover {
  background-color: #000; 
}

/* panel css and js at bottom */

li.sick {
  color: rgb(38, 150, 255);
}

li.audited {
  color: rgb(52,168,83);
}

</style>


  <link rel="stylesheet" href="/static/style.css">
  <meta charset="utf-8">
  <title>Things I've changed my mind about in deep learning· Tanishq Kumar</title>
</head>
<body>
<div id="menu">
<span class="title">Tanishq Kumar</span>
<ul>
  <li><a href="/index.html">Home</a></li>
  <li><a href="/about.html">About</a></li>
  <!-- <li><a href="/culture">Culture</a></li> -->
  <!-- <li><a href="/books.html">Bookshelf</a></li> -->
  <li><a href="/essays.html">Writing</a></li>
    <!-- <li><a href="/notes.html">Notes</a></li> -->
    <!-- <li><a href="/articles.html">Articles</a></li> -->
      <li><a href="/courses.html">Courses</a>
        <li><a href="/papers.html">Research</a>
</ul>
</div>
<div id="left"></div>
<div id="content">

<h3>Coming up for air</h3>

<p>
I've been underwater, focused myopically on my research, over the last several months. Now I want to come up for air and reflect a bit on the lay of the land in deep learning 
as I think about what's most important to work on. Below are some things I've changed my mind about over the last several months, others are just things I've been thinking about over the last week. 
Most are things I can't put into an SOP since they will make me look too scatterbrained. And yes, the title is a reference to one of my favorite novels. 
<br>
LM = language model; FM = foundation model. 
</p>

<ul>
  <li>The role of knowledge vs reasoning in LMs</li>
  <ul>
      <li>I used to think the storage of knowledge in model weights (subject-relation-fact) was a bug, not a feature</li>
      <li>We have databases for keys and values, surely we want FMs to be big-brain reasoning machines instead? My naïve dream was a ~1b model that can reason and plan like GPT4 and just use the internet/RAG for knowledge</li>
      <li>Then I started paying attention to how I use LMs myself on a day to day basis and realized that I mostly use them as a soft interpolants of internet content, ie. an even softer version of search than keyword/semantic web search</li>
      <li>Also, MMLU (knowledge) seems to be the major axis that determines general capabilities, including reasoning. Knowledge and reasoning seem intertwined in FMs, unlike in humans where for instance people who can memorize a lot may not be the best at hard math problems. 
      </li>
      <!-- <li>The long tail of the data distribution is interesting: </li> -->
    </ul>
  <li>Architecture vs data
    <ul>
      <li>Used to think architecture was the first order concern, now I think it's the data. In some sense this is obvious in hindsight because in an estimation problem, the target function usually affects the 
        learned function more than choice of estimator</li>
      <li>Have started some work on synthetic data; mixtures of real-synthetic usually outperform real on most metrics. This means there are some statistical properties the models "want" in their data that web text doesn't fully have</li>
      <li>What is the platonic data distribution models "want" to learn world representations on? Is there any definition for "quality" and "compute" such that the statement "the quality per token of synthetic data improves predictably with compute used to generate it" is true?</li>
      <li>In some sense, model distillation is a trivial example where bigger generators are better, but this is less interesting to me because you can't distill your way to a frontier model. 
        I think the most interesting synthetic corpora are comprised of less invasive "rephrases" or augmentations of real seed text by a generative model.</li>
    </ul>
  </li>
  
  <!-- <li>Neuro vs cognitive level of abstraction in both brains and machines
    <ul>
      <li>Years ago when I became interested in brains I felt like "only neurons are real" and this word "mind" is just one used by armchair philosophers</li>
      <li>Over the last few years I've mostly worked at the "neuroscience" level of abstraction with both literal brains and artificial networks. I think now it's probably more important to understand FMs' "minds" instead.</li>
      <li>Okay, these computations lead to some behavior by these foundation models. What are the tendencies? How can we make this next-token prediction 
          map into a joyful experience for billions of people that will have their next token predicted? What are the "personalities" of these models
          and why are they the same or different across labs? What are the invariants?
        </li>
        <li>These are all questions at the cognitive, not neuro, level of abstraction (Marr Level 3, not 1&2, is where to be in 2025)</li>
      </li>
    </ul>
  </li> -->


  <!-- cultures use these models differently. converge on base frntir practices, deploy extremely diffrently -->
  <li>I really want to learn about HCI in the few weeks after grad school apps. 
    <ul>
      <li>How models are post and thus pretrained -- and thus what is most important scientifically for me to work on in the next few years -- 
        is determined by how consumers and enterprises find it 
        most intuitive to interact with them. The "GUI moment" for FMs still hasn't happened and I really wonder what it will be. 
      </li>
      <li>Maybe it's agents, maybe that's the wrong mental model? I'm sure they will play a role but I don't think it's the full story, or at least specific enough for what I'm looking for.</li>
      <li>I think the design space for FM interfaces is extremely underexplored. But I also know nothing about HCI, so who am I to say. Some things that might be extremal points of this space:</li>
      <ul>
        <li>Will websites reduce to being just endpoints that can be hit by agentic API calls, and will humans stop interfacing directly with the web via URLs as a result?</li>
        <li>Will voice/language models become the primary medium through which we interface with computation of any sort (e.g., like the desktop of a home computer is for many now)?
          Many revolutions in interface design came from one thing doing many things (eg. iPhone). 
        </li>
        <li>Will the startups that have the best models package them into "superapps" that are not just 
          models and search engines, but companions and agents? For instance, in China, billions of users do literally 
          everything online (shopping, food, bookings, communication, etc) through a few "superapps" like WeChat. </li>
        <li>Will we ever interact with single language model forward passes like we do now, or will each response be a sort of ensembled output from the deliberation of a committee of models?
          If inference-time compute truly works, I see no reason most outputs shown to a user will not be the result of lots of low-latency MCTS/repeated sampling/multi-agent debate. 
        </li>
        <li>Will we have dashboards on the models we interact with, telling us real-time information about how honest or sycophantic they are being by monitoring their internals, like we have for 
          cars or stoves?
        </li>
      </ul>      
      <li>I should go to an elementary school and see how kids interact with these models. Alan Kay wanted the GUI so kids could interact with computation. 
        Everyone is so obsessed with automating the enterprise. What about automating the elementary school? (this is satire) 
      </li>
    </ul>
  </li>
  <li>I also really want learn about distributed systems</li>
  <ul>
    <li>The fact I don't know what this NCCL etc jargon means or how it works under the hood annoys me.</li>
    <li>Understanding (computer) networks and data centers seems like an evergreen skill that will never go out of style.</li>
    <!-- <li>For reasons I cannot explain, I feel a moral duty to do this if I want to call myself a deep learning researcher.</li> -->
  </ul>

<!-- <li></li>
      <li>It feels like things get qualitatively much more annoying/complicated as you scale from single to multi GPU, and a significant portion of folks employed by frontier labs are put to work 
        making sure this orchestra of hardware/software operates seamlessly under the hood. I feel a moral duty to share in their suffering 
        if I am to call myself a deep learning person. 
      </li> -->

  <li>Hardware vs math for mental models of what neural networks actually are</li>
  <ul>
    <li>Over the last year my mental model of what a neural network *is* (like, in its bones) has oscillated between "parametric function class" and "spicy matmuls on tensor cores"</li>
    <li>Learning about GPUs, performance engineering, CUDA, and more from an awesome collaborator (shoutout BFS) as a reformed theory person has been revelatory</li>
    <!-- <li>I feel like my undergraduate machine learning classes kind of lied to me? Neural networks do not have the flavor I thought they did after I've learned a bit about how GPUs work</li> -->
    <li>"Deep learning theory" sometimes feels like a mirror where you get out (theorems) a slightly transformed version of what you put in (assumptions you deliberately made to be able to prove things). 
      I'm more than happy to use my own work as an example:
    </li>
    <ul>
        <li>In <i>No Free Prune</i>, we used arguments from high-dimensional probability to reason about why pruning networks at initialization is doomed. 
          I think it contains a cute and interesting mathematical result, but -- since it relies on overparameterization -- says nothing about FMs, which are underparameterized. 
           Also, over time, I have come to feel that negative/pessimistic results of this form are spiritually not the kind of work I want to do long-term. 
        </li>


    </ul>
    
</ul>
  <li>More confused about optimization than ever before - how do we get anything done here?</li>
  <ul>
    <li>Does anybody alive understand high-dimensional non-convex optimization? How do optimizers for pretraining keep improving if we don't? (Adam -> Shampoo -> SOAP, etc)</li>
    <li>What's even more perplexing is often the theory used to motivate these new optimizers (that do work!) is from <i>convex optimization</i> even though as I understand it, 
    these loss landscapes are not even locally quadratic!</li>
    <li>Do adaptive/preconditioned methods really even share the texture of vanilla gradient descent? Are they even phenomenologically the same class of 
        object? Are second-order methods inevitable? I am a naïve tourist in the wonderland of optimization
    </li>
  </ul>
  <li>Will SSMs win? From no chance to maybe
    <ul>
      <li>Inference really matters, and constant factor improvements/just making ASICs cannot be the end of the story</li>
      <li>If O(1) inference is possible, then it feels inevitable, even if only on the order of decades</li>
      <li>Even if traditional SSMs (LTI) don't win, I think hybrid models have a real chance. The notion of clamping n-gram heads into an SSM in the ICLL paper was hilarious and brilliant.</li>
      <li>Inference is the speed of thought for agentic workflows and reasoning with LMs. If inference-time compute gains are loglinear and inference speed goes up by 100x overnight
         when we switch to SSMs, eval performance within a fixed wall clock inference time will shoot up out of nowhere. 
      </li>
      <!-- <li>When something is quite important, constant factors are optimized (ASICs, etc); when something is really, really important, the asymptotics will win</li> -->
    </ul>
  </li>
  <li>This is a less sciency comment, but the fact that different cultures use deep learning very differently is exciting to me</li>
  <ul>
    <li>For instance, China has a huge on-demand delivery market, and robust last-mile robotics and mobile app infrastructure built around this. 
      A key use case of AI there is powering this flourishing ecosystem of food delivery and shopping apps. 
    </li>
    <li>I think video models will be huge in India, maybe even uniquely so; the culture is enormously reliant on video and film and TV as a focal point of communication 
      and there's a lot of latent creative energy I suspect can be released. I can't wait for slum teenagers to be making viral clips and 
      short movies that genuinely compete with Bollywood when diffusion models become open and cheap for all. 
    </li>
    <li>Japan has really bought into AI as an emotional companian. I would hear my Japanese friends swear by their pet chatbots on Character.ai and wax lyrical about 
      their intimate bonds. I know Japan historically has a culture of framing AI as by defult helpful and human-like in contrast to the typical depiction in 
      Western sci-fi, maybe this made adoption easier. 
    </li>
    <li>This makes me optimistic every culture will make AI its own. The idea that 
        AI can be useful to small companies aiming to build "culture-aware" products and win market share this way is amazing to me. It means AI is accelerating 
        heterogeneity of preferences and products instead of the opposite. I think this is a very good thing. 
    </li>
  </ul>

</ul>




</div>

