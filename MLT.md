- [Deep learning theory, a physicist's perspective](https://arxiv.org/pdf/2106.10165.pdf)
    - **Chapter 0 -- philosophy and motivation**
        - [Companion essay: why AI is harder than physics](https://arxiv.org/pdf/2104.00008.pdf)
            - Prior motivation for using physical tools in ML: it's fundamentally about studying the emergent properties of strongly interacting systems whose microscopic details we understand. This is precisely the job of statistical mechanics!
            - Also, it should be noted that physics is not just a tool for studying __natural__ phenomena. The theory of thermodynamics was invented __after__ steam engines, so understand how to make more principled and bigger improvements to their design. 
            - A first point is that AI is hard because there's no free lunch. The number of possible labelings for a set of images grows very fast. There is often structure in the world we can exploit to compute these labels efficiently, but the fact that deep learning models can perform well when this structure is not human-interpretable makes them difficult to reason about. 
            - Conversely, he argues that there __is__ a free lunch in physics. It is an empirical and surprising fact that there is so much structure in the world that the laws of physics have a low algorithmic complexity.
                - We can make this more precise. A QFT is characterized by its elementary particles and their interactions. The sum of all possible interactions is exponential in $$p$$, the number of particles. More generally, if we have a system with $$N$$ dof (eg. spins in a solid), generically we can have $$\sum_i \binom{N}{i} \sim 2^N$$ interactions between the two. 
                - If we had to have a parameter for each dof, this would mean we'd need exponentially many parameters for a system with $$N$$ dof. This would mean that theories don't tell us much: they are just lookup tables for all possible interactions, where an experiment was done to determine each parameter that characterizes each different interaction. The "unreasonably low algorithmic complexity" point refers to the fact that this is __not__ the case. 
                    - This should remind us of how, generically, images can be exponentially many combinations of labels, so that generically learning labels in the absence of structure is hard. 
                    - The key point is that actual laws and theories of physics are __sparse__. We can describe exponentially many interactions with a small number of parameters. If instead of trying to introduce a parameter for all possible $$(n \in [N])$$-wise interactions, we limited to just interacting at most $$k$$-wise interactions for $$k \ll N$$, then we'd only need $$O(N^k)$$ parameters to characterize all possible interactions. This is saying that we might enforce that any interaction can be between no more than $$k \ll N$$ particles.
                    - However, the universe has even stronger sparsity than this! In fact, we have __spatial locality__, no just $$k$$-locality. This means the maximum amount of interactions of particles is capped at how many other particles can physically be near them. Due to the properties of 3D space, this isn't all that many! Space is only a meaningful notion because of this locality -- if a particle on earth and on Saturn interacted just as strongly and easily and quickly as two neighboring particles on Earth, "space" would have no meaning at all. 
                        - Note that with this constraint, a particle in $$d$$ dimensions can only have $$2d$$ neighbors, so that now the interactions we need in our look-up table are linear in the number of particles, not even polynomial!
                    - Our final empirical sparsity condition is __translation invariance__. We don't even need to study every particle separately, since we observe that the laws governing one particle are the same as the laws governing all particles. This means that size of our look-up table reduces to a constant size, $$O(1)$$!
                - This is what is meant by physics having a surprisingly low algorithmic complexity. Our simple mathematical descriptions have a complexity as objects that is __stunningly smaller__ than the objects and dynamics they describe. The universe has been kind to us!
                - __The key point of the essay is that physical systems are interacting multi-body systems whose dynamics we want to understand, and this is made easier by the plethora of sparsity constraints the universe has given us to understand these systems. Neural networks, as mathematical objects, are under no such sparsity constraints, so as coupled dynamical systems, they are strictly more difficult to reason about! Physics is easy compared to AI!__
    - **Chapter 1 -- tools for Gaussian integrals and perturbation arguments**
        - This is important because it introduce some objects common in Blake's arguments: Wick's theorem, perturbation arguments, source terms, observables, correlation functions, moment generating functionals, the action, and more. 
        - ^^Keywords, key ideas and dictionary^^
            - **Gaussian integrals**
                - These are key primitives in physics, since everything is Gaussian by CLT of lots of interacting particles. An example is $$I = \int_{-\infty}^\infty \exp(-z^2/2)$$, which can be computed by changing to co-ordinates. To see this, observe that $$I^2 = \int_{-\infty}^\infty \int_{-\infty}^\infty dxdy \exp(-(x^2 + y^2)/2) = \int_0^\infty rdr \int_0^{2\pi} \exp(-r^2/2) = 2\pi$$, so that $$I = \sqrt{2\pi}$$. The extra factor of $$r$$ comes from the chain rule in the change of variables to polar co-ordinates. 
                - Now for a general Gaussian rv with mean $$s$$ and variance $$K$$ we write $$I_{K,m}$$ as the corresponding Gaussian integral $$I_{K,m}$$. Note this is the normalizing factor in the normal distribution with that mean and variance. We can indeed check that for such a rv  $$z$$, we have $$E[z] = \int_{-\infty}^\infty dzp(z)z = \frac{1}{\sqrt{2\pi K}}\int_{-\infty}^\infty dz \exp(-(z-s)^2/2K)z = s$$ by doing the integral at the end out explicitly using a change of variables $$z \to z - s$$ and using that odd integrals from $$(-\infty, \infty)$$ vanish. Thus we've sanity checked that the normal PDF for a set mean and variance does indeed give that mean and variance. 
                - Now consider centered Gaussians. Let $$O(z)$$ represent some observable of an underlying physical random variable; we will often be concerned with reasoning about expectations and higher order moments of observables. Of course, moments $$O(z) = z^M$$ are special values of observables. From polynomial approximation, we know that characterizing the expectations of these observables characterizes $$E[f(z)]$$ for any function $$z$$, and thus the distribution of $$z$$ in its entirety. Thus a rvs distribution (and general behavior) is characterized by its moments, motivating their study. 
                - We note odd moments of the standard normal vanish, so we're more concerned with efficient ways to compute even moments for $$M = 2m, m \in \mathbb{N}$$. There exist a neat trick seeing that $$I_{K, m} \equiv \int_{-\infty}^\infty dz \exp(-z^2/2K)z^{2m} = \left(K^2\frac{d}{dK}\right)^m = \int_{-\infty}^\infty dz \exp(-z^2/2K) = \left(2K^2 \frac{d}{dK}\right)^mI_K = K^m(2m-1)!!$$ where we note we've abused notation so that now $$I_{K,m}$$ is the Gaussian integral we're computing to find $$E[z^{2m}]$$. 
                    - Note how higher order moment calculations have thus been reduced to evaluating double factorials. This is known as (the single variable) __Wick's theorem__, and is absolutely fundamental in probability and SFT for computing higher order moments. 
                - Now we'll rederive this another way that extends more easily to the multivariate case, by adding a dummy term to our original Gaussian integral called a __source term__, $$J$$. This has a physical interpretation in physics, but here it's just a dummy quantity including which will make the integral easier for reasons we'll see. 
                    - Define $$Z_{K,T} \equiv \int_{-\infty}^\infty dz \exp(-z^2/2K + Jz)$$ and see that this partition function is just $$E[e^{Jz}]$$. Since it's an MGF for the rv of interest, $$z$$, we can read all the higher order moments for $$z$$ off the Taylor expansion of this integral around $$J=0$$, which is precisely what Wic's theorem aims to do. 
                    - Doing this out, we get that $$\begin{aligned}
\mathbb{E}\left[z^{2 m}\right] & =\frac{I_{K, m}}{\sqrt{2 \pi K}}=\left.\left[\left(\frac{d}{d J}\right)^{2 m} e^{\frac{K J^2}{2}}\right]\right|_{J=0}=\left.\left\{\left(\frac{d}{d J}\right)^{2 m}\left[\sum_{k=0}^{\infty} \frac{1}{k !}\left(\frac{K}{2}\right)^k J^{2 k}\right]\right\}\right|_{\substack{J=0 \\
(1 .}} \\
& =\left(\frac{d}{d J}\right)^{2 m}\left[\frac{1}{m !}\left(\frac{K}{2}\right)^m J^{2 m}\right]=K^m \frac{(2 m) !}{2^m m !}=K^m(2 m-1) ! !
\end{aligned}$$ which is precisely Wick's theorem in the single-variable case.  Note that the term $$\exp(KJ^2/2) \equiv Z_{K,J}/I_K$$ is where the partition function enters, and we take the $$2m$$-th derivative because that's the moment we want (recall how we read moments off of MGFs from Stat 210). 
            - **Partition function as MGF**
                - Note that a crucial observation is that in thermodynamic systems, that probability of a particle being in a state $$s$$ is given by the Gibbs measure $$\exp(-\beta H(s))$$. The partition function, as we know, just averages over this measure $$Z = \int ds \exp(-\beta H(s))$$. The crucial observation as we saw above is that the Gibbs measure precisely implies $$Z \equiv E_s[e^{-\beta H}]$$ that the partition function is the moment generating function over the random variable $$H(s)$$. Thus knowing the partition function means knowing the distribution of $$H$$, which is knowing the distribution over states, which is precisely the goal of statistical mechanics. This is what is meant by saying "knowing the partition function is equivalent to solving out the properties of the system." 
                    - This is also why derivatives of the partition functions encode important quantities. For instance, we expect the first moment to be the mean of the random variable, ie. the average energy of the system over movement through random states. This is exactly the relation between the partition function and free energy. Then 
            - **$$M$$-point correlators**
                - Instead of calling $$E[z_1z_2 \cdots z_m]$$ a moment of order $$m$$, physicists (not unreasonably) call it an $$m$$-point correlator, since it's a function that measures the correlation between, well, $$m$$ points. Of course when we examine the dynamics of a strongly correlated system, these will appear in the "Taylor expansion" of all possible interactions, which is why being able to compute them easily is important, motivating the theory above. 
            - **Proof of multivariate Wick's theorem**
                - We established the single-variable theorem above, and here we state the general version. 
                - Recall first the PDF of the MVN. The determinant factor comes from the fact that we diagonalize when computing the multidimensional integral with $$K$$ in it, so we should expect its spectrum to appear in the normalizing factor. Thus we recall $$p(z) = \frac{1}{\sqrt{|2\pi K|}} \exp\left[-\frac{1}{2}\sum_{\mu \nu}^N (z-s)_\mu K^{\mu \nu} (z-s)_\nu\right]$$ where superscript on the covariance matrix denotes inverse, and so the mean vector and covariance matrix for this MVN are $$s, K$$. 
                - Now we seek the moments $$E[\prod_{i=1}^M z_{\mu_i}]$$. We reason by analogy to the single variable case. Moments with odd numbers of terms vanish and so let $$M = 2m$$. To find this, we need to compute the generating function $$Z_{K,J} \equiv \int d^Nz \exp\left(-\frac{1}{2} \sum_{\mu \nu} z_\mu K^{\mu \nu}z_\nu + \sum_\mu J^\mu z_\mu \right)$$. 
                    - Side note: this kind of reminds me of the partition function of the Hamiltonian of an Ising model, but I'm not sure exactly what the relation is. 
                    - We can find this in closed form by integrating by hand and eventually getting that it's equal to $$\sqrt{|2\pi K|} \exp\left(\frac{1}{2}\sum_{\mu \nu} J^\mu K_{\mu \nu}J^nu\right)$$. 
                - As before, we can get the moments by taking many derivatives of the partition function above. We have that $$\begin{aligned}
\mathbb{E}\left[z_{\mu_1} \cdots z_{\mu_{2 m}}\right] & =\left.\frac{1}{I_K}\left[\frac{d}{d J^{\mu_1}} \cdots \frac{d}{d J^{\mu_{2 m}}} Z_{K, J}\right]\right|_{J=0} \\
& =\frac{1}{2^m m !} \frac{d}{d J^{\mu_1}} \frac{d}{d J^{\mu_2}} \cdots \frac{d}{d J^{\mu_{2 m}}}\left(\sum_{\mu, \nu=1}^N J^\mu K_{\mu \nu} J^\nu\right)^m .
\end{aligned}$$
                - This is the key bridge between moments of a normal and combinatorics: the combinatorics arise by examining how many terms in the binomial-type expansion in the brackets "survive" after all the derivatives, and what form they take. Thus we should expect a combinatoric formula in terms of the covariance function's indices (that will be some mixture of $$\mu_1, \cdots, \mu_{2m}$$, since only terms containing all of those terms will survive under the successive derivatives), so for instance $$\left(J^{\mu_1} \right)^m K_{\mu_1 \mu_1}\left(J^{\mu_1} \right)^m$$ would not survive the derivatives, so won't show up in the moment. 
                - The final form is $$E[z_{\mu_1}\cdots z_{\mu_{2m}}] = \sum_{p \in \text{partitions of } [2m] \text{ into } m \text{ pairs }}\prod_{i, j: \text{elements in the pair,} p} K_{\mu_i, \mu_j}$$ where each term in the sum is the product of $$m$$ covariance entries, each called a __Wick contraction__. 
                    - If you do the counting, there will be $$(2m-1)!!$$ terms in the sum, as in the single variable case. 
                    - We used the normal PDF to derive these things, sure, but the point is this is special to the normal because we're characterizing all higher moments (and thus the distribution) purely in terms of the covariance. Recall that only a normal is characterized by its first two moments. 
            - **Cumulants, or, connected correlators**
                - The goal on the horizon is being able to extend the parsimony of distributional description that Normal rvs have to nearly-Gaussian settings. First note that physicists call cumulants "connected correlators" as opposed to the $$M$$-point correlators we saw above (which will sometimes be referred to as "full correlators").
                    - To refresh ourselves, remember that a CGF is just $$K(t) = \log M(t)$$. For instance, the famous $$\log Z$$ is a cumulant-generating function, since the partition function is itself a moment generating function. These were invented because they have many nice properties that moments don't. Recall this section from the Stat 210 book: 
                        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Ftanishq%2F34_MIBnhdj.png?alt=media&token=6d544ef3-a0db-4a36-944e-eb2141af7566)
                    - The first cumulant is the mean, and the second is the covariance. A key nice property that many nearly-Gaussian rvs have (that's what we mean by "nearly" Gaussian) is the __parity symmetry__, or invariance of distribution under sign flip of the rv, that a normal rv enjoys (ie. its PDF is symmetric about the vertical axis). For any rv that has this property, all odd moments and cumulants vanish, as expected. 
                    - Thus, after the covariance -- the second cumulant -- the natural object of study is the fourth cumulant, the kurtosis (up to scaling, really we need to divide the fourth moment by the variance to get the Kurtosis). It turns out this is a fantastic measure of Gaussianity, since Normal rvs have unusually thin tails due to exponential decay in the PDF. The Gaussian has zero Kurtosis, and its deviation above (fatter tails) or below (thinner tails) tells us about how non-Gaussian a rv is. 
                    - The inductive definition of cumulants mean that we can express higher order cumulants as sums of the moments. Recall we can compute all moments of a Gaussian easily by Wick, so that we can compute all cumulants easily, too. 
                        - It turns out for the Gaussian, the sixth moment vanishes, so that all cumulants after the covariance are actually zero, so the values of the higher cumulants are a natural measure of non-Gaussianity. 
            - **Near-gaussianity and the action**
                - Now that we have some measure of non-Gaussianity, what can we say, given the cumulants, about the functional form of a near-Gaussian distribution? We will do this using a quantity $$S(z)$$ called the action. 
                - This is just the negative log probability, ie. a function of our rv $$z$$ such that $$p(z) \propto \exp(-S)$$. For instance, for $$z$$ being the states of a thermodynamic system -- say a spin glass -- the action is the Hamiltonian since it defines the probably of a state through precisely the Gibbs measure. 
                    - Because we defined it as above, we get that $$Z \equiv \int d^Nz \exp(-S(z))$$ so that $$p(z) \equiv \exp(-S(z))/Z$$.
                    - Of course, this means given a probability distribution $$p(z)$$ we can define an action $$S(z) \equiv - \log p(z)$$ up to proportionality due to the additive effect of the partition function in log space. 
                    - In particular, for a Gaussian, we of course have the action is the thing in the exponent of the PDF by construction, so that $$S(z) = \frac{1}{2}\sum_{\mu, \nu} K^{\mu, \nu}z_\mu z_\nu$$. This is called a quadratic action. To study near-Gaussians, we'll add a coupling term that will make this deviate from Gaussianity. 
                        - In particular, we let $$S(z)=\frac{1}{2} \sum_{\mu, \nu=1}^N K^{\mu \nu} z_\mu z_\nu+\frac{\epsilon}{4 !} \sum_{\mu, \nu, \rho, \lambda=1}^N V^{\mu \nu \rho \lambda} z_\mu z_\nu z_\rho z_\lambda$$. 
                        - The term $$\epsilon$$ controls the degree of coupling, and in neural net in fact $$\epsilon \propto \frac{1}{N}$$ which gives convergence to a Gaussian distribution as width blows up. 
            - **Perturbation theory**
                - To quantify how much the distribution with action $$S^{\text{quartic}}(z)$$ deviates from Gaussian, we should check its fourth cumulant (roughly its Kurtosis) to see how large that is. To calculate this, we seek the MGF as usual, now given by $$\begin{aligned}
Z & =\int\left[\prod_\mu d z_\mu\right] e^{-S(z)} \\
& =\int\left[\prod_\mu d z_\mu\right] \exp \left(-\frac{1}{2} \sum_{\mu, \nu} K^{\mu \nu} z_\mu z_\nu-\frac{\epsilon}{24} \sum_{\rho_1, \ldots, \rho_4} V^{\rho_1 \rho_2 \rho_3 \rho_4} z_{\rho_1} z_{\rho_2} z_{\rho_3} z_{\rho_4}\right) \\
& =\sqrt{|2 \pi K|}\left\langle\exp \left(-\frac{\epsilon}{24} \sum_{\rho_1, \ldots, \rho_4} V^{\rho_1 \rho_2 \rho_3 \rho_4} z_{\rho_1} z_{\rho_2} z_{\rho_3} z_{\rho_4}\right)\right\rangle_K .
\end{aligned}$$
                - Noting that $$\epsilon \ll 1$$ is small, we can Taylor expand the exponential in the Gaussian expectation and drop all but the first term. This approximation is accurate because $$\epsilon$$ is small, and it is precisely this Taylor expansion that is referred to as a __perturbation argument__ since $$\epsilon$$ is the magnitude of the perturbation away from Gaussianity. Then we can use Wick's theorem to bash the fourth cumulant as we'll see we  get something in terms of a Gaussian 4-th moment. 
                    - $$\begin{aligned}
Z & =\sqrt{|2 \pi K|}\left\langle 1-\frac{\epsilon}{24} \sum_{\rho_1, \ldots, \rho_4} V^{\rho_1 \rho_2 \rho_3 \rho_4} z_{\rho_1} z_{\rho_2} z_{\rho_3} z_{\rho_4}+O\left(\epsilon^2\right)\right\rangle_K \\
& =\sqrt{|2 \pi K|}\left[1-\frac{\epsilon}{24} \sum_{\rho_1, \ldots, \rho_4} V^{\rho_1 \rho_2 \rho_3 \rho_4}\left\langle z_{\rho_1} z_{\rho_2} z_{\rho_3} z_{\rho_4}\right\rangle_K+O\left(\epsilon^2\right)\right]
\end{aligned}$$
                    - This higher order coupling trick should remind you of how we'd induce arbitrary dependencies between nodes in a graphical model by putting a multiplicative $$\psi(x_i, x_j, x_k)$$ term in the factorization of their PDF. 
                - We can use the partition function to them compute the Kurtosis, which will be in terms of the second and fourth moments, which we can compute using Wick's theorem and the same partition function. After a ton of bashing, we'll get the final Kurtosis (fourth cumulant, really) to be $$-\epsilon \sum_{\rho_1, \ldots, \rho_4} V^{\rho_1 \rho_2 \rho_3 \rho_4} K_{\mu_1 \rho_1} K_{\mu_2 \rho_2} K_{\mu_3 \rho_3} K_{\mu_4 \rho_4}+O\left(\epsilon^2\right)$$. 
                    - Importantly, note that the moment structure at __all moments__ is slightly different due to this perturbation from Gaussianity, including the covariance, which is not exactly $$K$$ for the near-Gaussian random variable. 
                    - One way to see this is that for a truly Gaussian rv with arbitrary $$K$$, we can find a basis in which the random vector has independent components (ie. $$K$$ is diagonalizable). However, inducing higher order correlations like we just did means that no linear transformation will "cleanse those," and the variable is fundamentally non-Gaussian now. Thus we have a breakdown of independence alongside this non-Gaussianity. 
                    - Also importantly, we can't compute closed forms for the partition function or correlators of a non-Gaussian, which is why we had to resort to perturbation theory (a Taylor approximation) here. 
                - In the way outlined here, we can induce couplings of arbitrary degree that quantify how non-Gaussian our random variable is. These couplings control the __interactions__ between terms. 
                - These ideas are relevant downstream in neural networks because wide (but not infinitely wide) neural nets have their activations and preactivations (whose dynamics the book argues are the key quantities in neural networks) given by nearly-Gaussian random vectors. The key contribution of the book is that it does the same perturbation theory for these networks, accounting for first-order deviations from Gaussianity in these random vectors (ie. they have Kurtosis), which accounts for most of the deviation from the infinite width limit. It turns out that the $$2m$$-th cumulant in these networks is order $$O(\epsilon^{m-1})$$. 
                - In practice, when using perturbation theory, we stick with a quartic action as anything beyond that becomes too unwieldy to reason about and often those corrections are vanishingly small and sometimes can't even be resolved!
    - **Chapter 2 -- Neural network basics**
        - A key idea here is to not think about weights and biases as the primitive rvs but instead to think of the induced distribution over the preactivations as the key primitive. We fix a dataset and the randomness over parameters leads to randomness over the preactivations $$z^{(\ell)}_i$$ for neuron $$i$$ in layer $$\ell$$. These are what this book mostly reasons about, since tracking preactivations characterizes network dynamics (and predictions) completely. 
        - It's interesting to note that many parameterized function classes can be used for function approximation. For instance, we can approximate functions as sums of many Gaussians, where the parameters are means and variances. But we don't do this because NNs were a function class initially motivated by the brain. 
        - We can think of biases as just the threshold for firing in a binary firing rate setting. 
        - A cool view on inductive biases is that they induce constraints on, or relationships, between the weights. For instance, weight tying is when you constrain weights in different places to be the same, and we can see the convolutional architecture as being defined by a tied set of weights across space (translational symmetry of weights). This bias reduces the effective number of weights we need to learn by a lot. 
        - He makes the point that scale invariance is a key property in an activation function, and proves that only linear and ReLU activations satisfy this. Many people try to make ReLU smooth by finding close smooth approximations, but none of these have exact scale-invariance, which is useful both to reason about networks and to be able to easily tune parameters to avoid blow-up. 
        - A cool view on why we don't initialize all params at zero is that then they are symmetric, so the network only behaves as it has one effective neuron, since all the weights would move together as one under GD. Note that the resulting goal of moving to initializing weights from a probability distribution is to fin a distribution so that __the resulting ensemble of networks are usually well behaved with respect to the function approximation task at hand__. 
        - Part of the goal of deep learning theory is to find exact prescriptions for hyperparameter tuning (which we know is absolutely crucial to network performance) that are provably optimal across a wide range of settings. 
        - The ultimate goal of the book is to compute $$p(z^{(L)}|D)$$ the distribution of output preactivations given a dataset (where the distribution is over randomness in weights). 
        - A note: a __self-averaging__ random variable $$z$$ is one where $$E[f(z)] = f(E[z])$$ for all $$f$$. A random variable obeys this property if and only if it is Dirac delta distributed 
    - **Chapter 3 -- MLPs at initialization**
        - This is a key chapter for developing conceptual tooling and intuition for how non-Gaussianity increases with depth of the network. It studies the deep linear network, which the author interestingly refers to as the "simple harmonic oscillator" of DL. 
        - A cool point is that deep linear networks represent a smaller set of functions than general linear transformation. This can be seen by imagining a one-neuron-hidden layer that necessitates some compression. It doesn't seem like enforcing a bottleneck would maintain our ability to represent any matrix. 
        - We will ultimately compute $$p(z^{(L)}|D)$$ for a deep MLP __at initialization__. We will do this by computing all the moments of the final layer preactivations (by computing all the moments of every layer's preactivations and inducting on layer). It turns out once we have the right setup and primitives, "inducting" this initialized setup based on new data will not be too hard, so treating the case of trained MLPs evolving in response to data will be the "inductive step" to this chapter's "base case."
        - A key fact we'll use is $$\mathbb{E}\left[W_{i_1 j_1}^{(\ell)} W_{i_2 j_2}^{(\ell)}\right]=\delta_{i_1 i_2} \delta_{j_1 j_2} \frac{C_W}{n_{\ell-1}}$$. 
            - This basically says the entries of a layer's weight matrix are independent, and have variance $$\frac{C_W}{n_{\ell -1 }}$$. 
        - First we begin by fixing a data set $$D$$ that we want to evaluate our network's output on (we __are not training on this data set -- this is an important fact to remember__). Since we seek to work towards understanding all the moments of $$z^{(L)}$$, let's begin by understanding the moments in the first layer. Also note that $$E[z_{i;a}^{(\ell)}] = 0$$  where $$i$$ is neuron and $$a$$ is index in the dataset of the data point fed in to get this preactivation. This is true because the weights are centered. 
        - Let's now move to computing the second moment, which will illustrate some important techniques. 
            - We can write $$\begin{aligned}
\mathbb{E}\left[z_{i_1 ; \alpha_1}^{(1)} z_{i_2 ; \alpha_2}^{(1)}\right] & =\sum_{j_1, j_2=1}^{n_0} \mathbb{E}\left[W_{i_1 j_1}^{(1)} x_{j_1 ; \alpha_1} W_{i_2 j_2}^{(1)} x_{j_2 ; \alpha_2}\right] \\
& =\sum_{j_1, j_2=1}^{n_0} \mathbb{E}\left[W_{i_1 j_1}^{(1)} W_{i_2 j_2}^{(1)}\right] x_{j_1 ; \alpha_1} x_{j_2 ; \alpha_2} \\
& =\sum_{j_1, j_2=1}^{n_0} \frac{C_W}{n_0} \delta_{i_1 i_2} \delta_{j_1 j_2} x_{j_1 ; \alpha_1} x_{j_2 ; \alpha_2}=\delta_{i_1 i_2} C_W \frac{1}{n_0} \sum_{j=1}^{n_0} x_{j ; \alpha_1} x_{j ; \alpha_2}
\end{aligned}$$
            - And then define $$G_{\alpha_1 \alpha_2}^{(0)} \equiv \frac{1}{n_0} \sum_{i=1}^{n_0} x_{i ; \alpha_1} x_{i ; \alpha_2}$$ to see that $$\mathbb{E}\left[z_{i_1 ; \alpha_1}^{(1)} z_{i_2 ; \alpha_2}^{(1)}\right]=\delta_{i_1 i_2} C_W G_{\alpha_1 \alpha_2}^{(0)}$$. 
                - In particular, the $$G$$ term tells us about the correlation structure of the dataset. This is just the $$G$$ for the inputs. The next layer's $$G$$ is defined similarly, but based on the first layer, so that $$G^0_{\alpha_1, \alpha_2} \equiv \frac{1}{n_1} \sum_j^{n_1} E[z_{j,\alpha_1}z_{j,{\alpha_2}}]$$. It turns out this is a fundamental quantity we can write all moments in terms of, so in some sense this is the right primitive to determine the distribution of preactivations. 
                    - Intuitively, we should think of the $$G$$ term as representing the average over neurons of "how correlated is the neuron's activity for input $$\alpha_1$$ to its activity for input $$\alpha_2$$." 
                    - In short, we can think of $$G^{\ell}_{\alpha_1, \alpha_2}$$ as representing the "mutual information" that $$\alpha_1, \alpha_2$$ have on predicting each others firing rate for neurons in layer $$\ell$$. 
                - The final recurrence turns out to be $$G_{\alpha_1 \alpha_2}^{(\ell)}=\left(C_W\right)^{\ell} G_{\alpha_1 \alpha_2}^{(0)}$$. 
                    - What does this mean? For one, if $$C_W \neq 1$$ then we blow up or vanish, so that $$C_W = 1$$ is a __critical point__. Modern DL operates in this regime since networks would otherwise be useless. 
            - So now we have a form for the second moment of the desired distribution $$p(z^{(L)}|D)$$. Let's find the fourth moment to see how this deviates from Gaussianity. It turns out to take the form $$G_4^{(\ell)}=\left[\prod_{\ell^{\prime}=1}^{\ell-1}\left(1+\frac{2}{n_{\ell^{\prime}}}\right)\right]\left(G_2^{(\ell)}\right)^2$$. The task of the rest of the chapter is cleanly interpreting this. 
            - Note first that for a Gaussian, the product evaluates to one as the fourth moment is just the second moment squared. Also note that this is exactly what happens as we take all widths to infinity. This is why wide networks behave as Gaussian processes (roughly, this means that the joint distribution of all layers is MVN for a large vector). 
                - Note that this is not obvious a priori because each neuron in a later layer is the sum of many Gaussian terms, __some of which are raised to a power__, and we know of course that Gaussian rvs are not closed under polynomial transformations.
            - This tells us that each layer is an "opportunity to deviate from Gaussianity," so that it should not be surprising that later layers are non-Gaussian in their joint distribution. Crucially, it's important this is the case, because if I'm classifying cat images and my final layer gets preactivations that are perfectly Gaussian, that earlier networks __didn't do their fucking job__ of finding useful __non-random__ features that make classification easy. Thus the development of non-Gaussianity is a feature, not a bug, even if it makes theoretical analysis harder. It's ironic that the extent to which a network is useful is precisely that to which it resists theoretical analysis. Maybe this tell us the whole project of closed-form MLT for MLPs is epistemically misguided? 
            - Let's investigate this more closely by setting all the widths equal to some $$1 \ll n \ll \infty$$. 
                - Plugging into the above equation, we get exactly that $$\begin{aligned}
G_4^{(\ell)}-\left(G_2^{(\ell)}\right)^2 & =\left[\left(1+\frac{2}{n}\right)^{\ell-1}-1\right]\left(G_2^{(\ell)}\right)^2 \\
& =\frac{2(\ell-1)}{n}\left(G_2^{(\ell)}\right)^2+O\left(\frac{1}{n^2}\right)
\end{aligned}$$
                - This has a few key interpretations on the nature of finite-width corrections
                    - First, that non-Gaussianity is a function of the emergent scale/aspect ratio $$\ell/n$$
                    - Second, that it increases linearly with layer depth. In particular, if we remember the quartic coupling in the action that caused deviation from Gaussianity in section 1, we can interpret this as saying that this coupling increases with layer depth. (In fact, we say this "runs" with layer depth, in the language of renormalization group flow which will be introduced later; damn physicists and their jargon). 
                    - Third, this is equivalent to saying that the deviation of $$z_j^{(\ell)}z_j^{(\ell)}$$ from its expectation on a fixed neuron $$j$$ is correlated with the deviation of neuron $$k$$ from __its__ mean. Thus finite width differences control intralayer interactions between neurons, where these intralayer interactions get stronger with depth. 
                        - The overall intuition to take away is that width plays a stabilizing role in reducing preactivation distribution's variance, and depth increases it. This is kind of like a bias-variance trade-off. 
                        - As depth increases, the typical magnitude of the preactivations deviates from its mean, so that if we take a thin, very deep network and try to use it in practice, it'll suffer convergence issues on many random initializations of weights. 
                        - This underscores the key theme of the book: __the leading finite-width contributions at criticality grow linearly with depth, despite being suppressed by the inverse of the layer widths.__
                - Importantly, since $$0 < L/n \ll 1$$, we can use $$\epsilon = n/L$$ as our perturbation parameter to do Taylor expansions around. Precisely the regime in which deep learning is effective is the regime where effective theories from physics that make use of perturbation approximations, are valid. 
            - Just as we computed the second and fourth moment for the final layer preactivations over random initialization weights, we can compute higher moments using Wick's theorem in the same way. Turns out higher moments have higher order dependencies on the aspect ratio $$L/n$$, so they roughly vanish as we expect that to be much less than one. That wraps up the chapter!
    - **Chapter 6 -- Bayesian Learning**
        - TODO
