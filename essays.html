<html>
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153791322-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-153791322-1');
  </script>
    <!-- endof Global site tag (gtag.js) - Google Analytics -->



<style>
.accordion {
  background-color: #000;
  color: #fff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: 1px solid white;
  /* border-top: 0.1px solid black; */
  text-align: left;
  outline: none;
  font-size: 15px;
  transition: 0.4s;
}

.active, .accordion:hover {
  background-color: #000; 
}

/* panel css and js at bottom */

li.sick {
  color: rgb(38, 150, 255);
}

li.audited {
  color: rgb(52,168,83);
}

</style>




  <link rel="stylesheet" href="/static/style.css">
  <meta charset="utf-8">
  <title>Essays Â· Tanishq Kumar</title>
</head>
<body>
<div id="menu">
<span class="title">Tanishq Kumar</span>
<ul>
  <li><a href="/index.html">Home</a></li>
  <li><a href="/about.html">About</a></li>
  <!-- <li><a href="/culture">Culture</a></li> -->
  <li><a href="/books.html">Bookshelf</a></li>
  <li><a href="/essays.html">Essays</a></li>
    <!-- <li><a href="/notes.html">Notes</a></li> -->
    <!-- <li><a href="/articles.html">Articles</a></li> -->
      <li><a href="/courses.html">Coursework</a>
</ul>
</div>
<div id="left">&nbsp;</div>
<div id="content">

    <h1>Essays</h1>
    <p >
It's in those moments when you're walking down the street, finishing up the last set of a workout, or taking a dump, that brilliance
strikes. As always, disagreements and opinions <a href="mailto:thetanishq@gmail.com"> are very welcome.</a> Essays marked with 
  an asterisk (*) are technical. 
</a>  
<br><br>
If unsure what to read, try 
<i>War and Business, Falsifiability, </i> or <i>Welcome to the Future.</i>
    </p>

<button class="accordion"><h3>Uniqueness and Leverage</h3></button>
<div class="panel">

  <p>
    In the Valley, it is common to hear people deliberating making life and career decisions through the framework of leverage. 
    People decide to found company X or work at company Y because "it's the highest leverage choice." That is to say, it's the position from which 
    they can exert the biggest raw impact on the world from their current place in life. High leverage choices have many multipliers between 
    actors and the effects of their actions. Being the engineering lead for Google's search team is high leverage work, being a lone engineer on that 
    team of 100 engineers is not. Leverage is purely about the scale of impact a single person can exert as a result of intentful choices that 
    multiply the effects of their actions (thereby acting as a "lever" on those actions). 
</p><p>
    One thing that I've found disturbing about this philosophy is that it completely omits the notion of uniqueness in one's work. 
    Let's caveat this with the fact that everyone share's different worldviews, and this is merely my own, etc etc. When thinking about 
    what you could do that has a highest leverage, you forget to think about what <i>only</i> you could do. To see how different uniqueness and leverage 
    can be, consider being an engineering lead at Facebook. Your team manages ad placement, which is business critical on the order of billions of dollars. This is 
    enormous leverage. However, if you weren't doing it, some other commodotized MIT-CS-PhD type (call them Dr Nurd) would be doing it instead of you. Now consider the scope of work for 
    an engineering lead: choosing a technology stack, architecturing an app or feature, recruiting new engineers onto the team, amongst other things. At most times there 
    exists a clear state of the art and standard practise on doing the first two things, and individual junior engineers you recruit onto the team are unlikely to 
    utterly transform its direction and atmosphere. That is to say, Dr Nurd and yourself have significant decision overlap -- the ad placements on Facebook would look mostly 
    the same under both of you, especially since the major product decisions are made by executives. 
    
  </p><p>
    And so on some level, there is very little fundamentally new 
    (where new means doing things very differently to how someone similar to you would do them) you add to the world by being in it. Conversely, 
    a mediocre writer with an audience of even a few thousand people is doing something that <i>could not</i> have existed without them. The books pumped out 
    by two similar writers can be vastly different. A good heuristic: would the world have lost something, subtle but fundamental and substantive, if you, 
    in particular, hadn't existed? 
</p><p>
  And so we must consider both uniqueness and leverage: the number of "forks in the road" -- 
  that is, opportunities to make a unique decision very different to what someone else would have made in your position -- presented to you per unit time 
  (say, per day), <i>and</i> how different the product/service you provide looks to the end consumer as a consequences of those decisions. For instance, 
  if you have an eclectic taste in coffee, and use your clout at work to get an obscure brand of coffee maker brought into work, you've contributed something 
  new to the world, but it has no meaningful impact for the consumer of the end product of the company (uniqueness but not leverage). In contrast, if you're choosing
  between two recently published sublinear network routing algorithms 
  for the streaming service on Netflix as an engineering lead there and you choose the one that is more amenable to systems optimizations, you're not making 
  any decisions unique to you (Dr Nurd would have done the same) but that choice (the packet transmission speed induced by the resulting systems optimizations) 
  is having a huge effect on the user experience (lag, loading times) for the consumer
  (thus, leverage but not uniqueness). 
</p><p>
  In general, work that has high uniqueness is that which presents many forks, as described above. And so intensely creative work -- from theoretical physics 
  to playwriting to architecture -- has high uniqueness because it represents the will and vision of its creator, manifest in the flesh. And two otherwise 
  similar people can produce vastly different pieces of prose, since the possibility space for different types of prose or plays is much, much bigger than the possible 
  decisions two engineering leads could make about choosing a technology stack (where there exists best practices for the most part). But there are other types of work
   that have high uniquness, work that you think is important but that almost everyone else doesn't. For a pedestrian example, consider 
  a YouTube channel on Russian stamp collection. Because nothing like this exists, if you were to start one and get even a few thousand subscribers, you would have 
  brought into the universe something fundamentally fresh and new, something that wouldn't exist without you. The universe would be a little worse off without you. 
  The same cannot be said for, say, college admission reaction videos that go viral on YouTube (if it wasn't yours, it'd be someone else's). For something a little grander, 
  consider entrepreneurship; because starting a company involves thousands of little decisions that two similar people may disagree on (eg. product features), two similar founders 
  can create two enormously different companies. Consider Brian Chesky and AirBnB -- one of the few examples of a truly contrarian bet. Everybody thought the idea was moronic, 
  as well as hundreds of investors they first pitched to, and, importantly, <i>no-one else was working on anything like it</i>. Therefore, AirBnB (or anything like it)
  would likely not exist if Brian Chesky didn't, and we'd all be worse off because of it. 
</p><p>
  Before people accuse me of criticising "conventional" jobs, I should note that the thing that matters in the end is not just uniqueness or just leverage, but the 
  product of the two. And so if I had to choose between an obscure stamp collecting channel and being an engineering lead at a big tech company, I'd probably choose the latter 
  (begrudgingly). But if I had to choose between being a niche writer with a few thousand (dedicated) readers, barely making a frugal living but getting by, 
  and an engineering lead, I'd choose the former in a heartbeat. As caveated at the beginning, this is predicated on the fact that something resembling uniqueness matters to you 
  at all. I think it intuitively does to most people, but I hadn't seen anyone quite articulate it in the way that made sense to me. 
</p>

<a href="#top">Back To Top</a>
    <br>
    <br>

</div>

<button class="accordion"><h3>The Butcher and the Brain Surgeon</h3></button>
<div class="panel">

  <p>
    When I took my first class on machine learning, one of the things that struck me was how "hand-wavey" most of the methods 
    presented were. By that, I mean that most methods at the state of the art are pretty crude, and worked out by trial and error as opposed to 
    clinical theorizing. Instead of deriving a general mathematical theory of neural network performance, machine learning journals are ripe with 
    people suggesting slightly tweaked architectures for some special case that have marginally better performance than the previous best. Machine learning 
    is not the only field that operates this way. Being the son of a vascular surgeon, being showed a picture of a partially amputated limb is more likely to 
    evoke curiosity than disgust. I've witnessed how even the most staggeringly intricate 
    surgeries are really nothing more than glorified butchery, and how surgeons are really nothing more than glorified butchers. And "butchery," here, 
    is both not a pejorative (as we will see), and far more than just a metaphor. 
    </p><p>
    Furthering this analogy, observe that when butchers meet (at some vegan restaurant, I presume), they discuss 
    the tricks of the trade; where to source the best meat, what makes for a good knife that delivers the cleanest cuts, and more. And when Papa Butcher 
    is teaching his daughter how good butchery is done, it's done by guiding her hand as she makes her first cut into that slab of beef and giving 
    her general heuristics on how to spot good meat from bad. Notably, it is not done by examining her on Dad's Grand Unified Equations Of 
    Incision Angles For Day-Old Chicken Breast. And it's by refining this word-of-mouth knowledge over generations and through the filters of 
    communities that it asymptotes to something optimal. Let's call this informal, word-of-mouth, heuristic based knowledge "process knowledge,"
   and its more formal, mathematical counterpart "theorizing."
</p><p>
   Both the butcher and brain surgeon have skin in the game, and the 
    heuristic (pragmatic and non-theorizing) approaches they take to decision-making under uncertainty refect the real-world risk they bear for their actions. If he 
    delivers bad mean consistently, the butcher loses his family establishment, and if she egregiously screws up a lobotomy, the neurosurgeon could lose her license 
    and means to make a living. 
    The fact that the frontiers of machine learning research, brain surgery, and butchery alike are all characterized by process knowledge as opposed to 
    theorizing is telling. This is true of other fields, too. Consider the humble cobbler that has worked the streets of Old Delhi for twenty years, 
    the young boulanger who left high school to apprentice the trade of bakery in the quiet town of Montpellier. Or consider the veteran A380 pilot who knows 
    everything about an airplane but fluid dynamics and kinematics. Or the solitary sculptor in renaissance Florence, tinkering to see how the 
    thickness of a chisel's head affects ease of sculpting. 
</p><p>
    The fundamental character of all these vocations stands as a stark contrast to grand theorists, most of whom walk the hallowed halls of the academy. 
    It's with elegant derivations and compelling prose that the daughter of the butcher (first in her family to go to college) sees her peers and convinces herself 
    that by immersing herself in this 
    brave new world, thereby distancing herself from that of her father, she is being educated. Over time, trained as a mathematician, she comes to harbor contempt
    against those that propose advances without fully justifying and proving them, dismissing them as "non-rigorous." She laments the state of the art in machine learning, 
    hoping to formalize the methods therein and obviate the need for tinkering by bringing the theory of neural networks 
    under a single mathematical umbrella in one fell swoop. She applies for a theoretical statistics PhD, waiting eagarly for that day when we "really understand machine learning
    and lay out its statistical foundations in full."
</p>

<p>
  TODO: Finish (maybe with inspo from GitHub copilot?)
</p>

<a href="#top">Back To Top</a>
    <br>
    <br>

</div>


     <button class="accordion"><h3>Grandfather Ideas</h3></button>
<div class="panel">

  <p>
    There exist a certain class of ideas in technology that feel so inevitable and transformative in hindsight that you can 
    imagine your grandkids sitting around you in 60 years time, starry eyed, and asking you how things could ever have been done 
    differently? Here are some examples. 
</p><p>
    Consider that once, not so long ago, software used to be sold in stores, on shelves, in physical boxes. You would go to a store, buy a box, 
    keep the receipt in case the software isn't compatible with your computer or some such thing, put the CD into your computer, and so forth. 
    And, now, it's easy to think: did we ever really live that way? Of course this was inevitable, and I'm sure people knew that even back then. The heart of 
    a "grandfather idea," though, is that -- post-facto -- we wonder how we could ever have lived that way. 
</p><p>
  Another example is paper money. The fact that we trade with sheets of paper with some guy's smiling face on them is a bit absurd. A more ambiguous example is 
  centralized fiscale policy. On one hand, it does seem crazy that a few old white guys do some hand-wavey calculations every year and use that to decide on 
  fiscal and monetary policy, as opposed to a more decentralized paradigm championed by cryptocurrency. On the other hand, it also seems plausible that 
  unforseen issues mean decentralization doesn't work as well as expected, and some novel implementation of centralization is what we asymptote towards as a society. 
</p><p>
      And now of course one could say: well, Tanishq, doesn't any major innovation like toilet paper or the AC qualify? Well sure, but that misses the point -- 
    grandfather ideas are in spirit meant to be those that one could have seen coming in advance, as well as ideas that changed the way things are done in a matter of 
    years. As it began to take off (and not before), the rapid growth of the internet, and the ensuing movement to digital software/downloads over networks was 
    not something too radical to envision. I don't think many people think building the AC was obvious immediately before it was built, hence it's not a grandfather idea. 
    But, really, I hope you get the spirit as we're playing fast and loose with definitions. Imagining your grandkids on your lap asking you these questions is, I think, a good 
    heuristic. 
</p><p>
    AWS & Cloud Compute [TODO]
</p><p>
    Netflix [TODO]
</p><p>
    Of course, many grandfather ideas are not just inevitable in hindsight, but in foresight as well. This means that the reason that any such ideas 
    do not exist is <i>not</i> because nobody has given them a good go, but instead because many people are bringing competing models to market, and one just 
    hasn't won out yet. That is to say, the bottleneck for these ideas is not conviction of founders in the face of rejection 
    and disbelief (like AirBnB), but instead just the fact that no-one has quite executed right as of yet, though lots of people are surely trying. 
    And the kicker is that many things that look like grandfather are ideas are not. They're just bad ideas. The difference is a few trillion dollars, 
    and a few lines in history textbooks. 
</p>

<p>
  TODO: Finish
</p>

<a href="#top">Back To Top</a>
    <br>
    <br>

</div>


     <button class="accordion"><h3>Opacity Bias</h3></button>
<div class="panel">
<p>
  I recently had a bit of a chat with someone I've looked up to for a long time over email, and here's a tid-bit from one of their responses.
  
   <figure>
    <blockquote cite="David Hilbert"><i>Much more goes into building a successful company than you're aware of. Yes, many people can build prototypes of Uber, or Fortnite, or Airbnb, or Twitter. But why do only certain companies survive? It must be either due to 1) nuances of the product that are opaque to you as an outsider, 2) other hard work in invisible areas that you don't see as a user, 3) just random luck (which is IMO less of a factor than laymen think).</i></blockquote>
        <blockquote style="text-align: right;">--Some Wise Tech Entrepreneur</blockquote>
      </figure>
</p>
<p>
  I'm particularly interested in his first point about opacity, as a few examples of such an "opacity bias" come to mind from my own experiences. 
  You think something is trivial (unimportant) in the naive sense as an outsider to the field, but when you learn a little bit about the field 
  you realize how important it is that that thing is done well. 
</p><p>
  <i>Household Savings</i>
</p><p>
  There is a Harvard professor I got to know who works in behavioral economics and macroeconomics. In behavioral economics, 
  there is a notion of "choice architecture,"
  where framing matters for human decision making. For example, changing 401k plans at companies from default opt-in, where you had to fill out an 
  annoying form to start your 401k retirement plan, to a default opt-out, has done a huge amount to increase savings in the United States (empirically). 
  It's not a profound idea, but it's important. Even more, this economist spent a great deal of time using ideas from psychology and behavioral economics 
  to think about how the form for opt-out should be designed, distributed, and presented, to make sure anyone who actually wanted to opt out, even a little bit, 
  had no beauracratic burden stopping them from doing so. I was flabbergasted to see that someone so intelligent could be spending their 
  time and intellectual energy on something so pedestrian as form design. It seemed to me that the work of this prodigious economist 
  was as unimportant as that of a lone accountant at an obscure small company. 
</p><p>
  Then I took a course on basic macroeconomics, and learned how the banking system actually works. It turns out (this was new to me; or, rather, it's 
  obvious but I had never actually given the topic serious thought) that the reason banking, as an institution, is so profound as a lever for 
  progress, is that it moves capital from useless places (under one's mattress) to useful places (as the up-front cost needed to kickstart a new 
  construction project). Banks are responsible for correctly (efficiently) allocating this capital (and the associated risk), and the fundamental idea: 
  all of this capital comes from savings. A bunch of it from corporate savings (residual profit companies have), but a lot of it also comes from 
  savings on the level of individual households. And so, any money an individual or household saves (such as in a 401k) is really a contribution 
  and bet on future progress (capital being earmarked for capital-intense projects like construction or loans to businesses) of society. 
  And there are robust (ie. reproducible) examples of how savings rate has much to do with future economic output, and, transitively, future 
  progress and prosperity (especially when it comes to technology). And most of all, when you look at the effect of form design on the choices people make 
  by examining huge datasets of millions of responses, you see that even small changes have big effects (this is part of the reason that 
  big tech companies A/B test their UIs so aggressively -- these things make a big difference) on how much people save. And so being an economist that is 
  part of the reason that opt-in 401ks became opt-out 401ks is something to be immensely proud of: it directly contributed to <i>billions</i> of dollars 
  of excess capital being invested into the future, as opposed to on jewellery or being stored under a mattress. It's just that this 
  contribution is opaque to most outsiders to the field, as it was to me. That's a lot of leverage this economist has had; 
  certainly one that Monsieur Lambda at Ye Olde Accountancy Firme doesn't have to his name. 
</p><p>
  And so it was really only by learning the foundational/basic modes of thought in a field (macroeconomics) that I gained the tools I needed to properly 
  appreciate the motivations and importance of certain work being done in that field. I think this is one example, amongst many else in my own 
  experience, and that of others, of how something seemingly trivial to outsiders can turn out to be of profound importance, once you think 
  and learn a little about it, ridding yourself of "opacity bias."
</p><p>
  [TODO: more examples]
</p><p>
</div>

    <button class="accordion"><h3>Physics and Psychohistory</h3></button>
<div class="panel">
  <p>
    There seem to be two types of mathematical models. There are those of convinience, which use heuristic arguments 
    and simple reasoning to create and justify equations that vaguely seem to fit a system, and often yield 
    surprisingly good predictions for their relative simplicity, but don't say anything about the 
    nature of reality that underlies the systems being described. The other type of mathematical model is one that makes strong assertions about underlying mechanisms, and, in doing so, 
    posits something concrete and deep about the underlying nature of the system it's trying to explain. These are often 
    found in physics.
    </p><p>
    <i>Lotka-Voltera equations</i>
</p><p>
    Here's an example of the first type of models. These are a system of differential equations we create to describe the growth and decline of certain populations. We justify them 
    based on our intuition that, initially, a population will grow exponentially in times of plenty, and will eventually plateau 
    due to constraints in the environment around them. This gives rise to the logistic curve we know so well, and it's that curve 
    that we use to make predictions. Amongst other things, these equations predict that growth per capita (or per individual)
    will plateau asymptotically to zero as we reach the "carrying capacity" of the environment. Of course, in practise, 
    we see that this doesn't hold. It provides a decent approximation some of the time, capturing the fact that 
    growth is initially fast, and slows down, but empirically quite wrong for most population systems. And we explain these 
    discrepancies away by mentioning that real systems are more complicated and we are missing variables, and it is a simple model, 
    and so on. The key point is, these are models of convinience that arise from "economy of thought," as physicist and 
    philosopher Ernst Mach put it. We do not believe that this model captures some deep truth about the universe, but instead 
    is more a mental crutch that helps us make sense of why population grows in the general way it does. 
</p><p>
    <i>General Relativity</i>
</p><p>
    Here's an example of the second type of model. Consider Einstein's general theory of relativity, which goes beyond giving some equations we can use to make 
    predictions, but actually asserts that reality itself is a certain way. Specifically, it asserts that mass 
    changes the curvature of space, measured via the energy-momentum tensor, and these changes in the geometric nature of 
    space manifest as visible effects of gravitation. It goes beyond Newton's equations, which are closer to the first type of modelling, 
    in explainging exactly <i>why</i> gravitation exists in the first place. Another example are the ideal gas laws. 
    They actually assume the existance of atoms to reason about the behavior of individual atoms on a mechanical level. The laws are not 
    entirely correct because of the assumptions they make, but they <i>do</i> reflect some underlying reality (atoms exist, or so we believe). 
</p><p>
    <i>Radioactive Decay</i>
    </p><p>
    Here's an example of mathematical modelling that is not obviously a model of "convenience" and neither one that reflects some 
    "deep natural truth." Consider the humble uranium atom. In high school, a typical setting in which differential equations is taught is 
    in the context of radioactive decay, since it is obvious that the rate of change of atoms depends on the number thereof. The more 
    atoms there are, for some fixed probability of decay per atom, the more atoms there are that decay. But what quantum mechanics makes difficult 
    is reasoning about whether probability is deeply embedded in reality, or whether there's an underlying structure we simply don't understand. 
    The fact that physicists showed that there is no "hidden variable" governing quantum mechanical behavior (which at this point we all agree 
    is <i>inherently</i> probabilistic, whatever that means) doesn't mean that there isn't some deeper mathematical structure (eg. M-theory) that is 
    at play causing this randomness we observe in quantum mechanical behavior (as well as atomic decay, etc). This has always been troublesome for 
    my intuition because it's not clear whether I should think of QM as a model of convenience like the Lotka-Volterra equations, or a model that 
    reflects the underlying nature of reality, like GR. 
</p><p>
    <i>Psychohistory</i>
</p><p>
    The reason I mentioned the ideal gas laws earlier is because in Asimov's classic series, <i>Foundation</i>, the protagonist 
    develops a statistical theory of macroeconomics called "Psychohistory." In it, he uses advanced mathematical statistics alongside 
    psychology to make predictions about societal behavior, in a time when the population of all humans exceeds one quintillion and is 
    spread across the "galactic empire." After reading some Taleb and perhaps even after taking some psychology courses and seeing how 
    crude our methods for understanding the human mind are, it's easy to believe that something so complex as the human psyche and its 
    behavioral consequences will never be understood well, let alone its emergent behavior when many such minds interact. 
</p><p>
    On the other hand, cognitive psychology -- and particularly the works of Kahneman and Tversky -- have yielded lots of 
    experimental and theoretical results that are exceedingly reproducible. The reason I'm not a fan of psychology at large is because 
    most results don't replicate. That is, most things published in papers are wrong; mere figments of chance and statistics abused by 
    people (psychologists) who have near-zero understanding of statistics. And so the fact that this work (mostly taken from the book 
    <i>Thinking, Fast and Slow</i>) hsa been so thoroughly reproduced and shown to be robust to replication is, to me, astonishing. 
    This was emphasized to me in my introductory economics course last year, where the professors ran several live experiments with our class 
    of a few hundred people, such as running a real-time prediction market, auctions, and asking people to make savings and investing decisions 
    with real money. Again and again, the results were almost exactly in line with those made by an entirely different group of participants 
    at a different place and time! 
    </p><p>
    This list of common behaviors and biases makes little use of grand theories from biology or neuroscience, 
    and instead humbly seeks to make predictions based on empirical evidence, without philosophizing. Much in the style of Taleb's "convex tinkering."
    This list reminds me of the ideal gas assumptions we make when deriving the famous pV = nRT equation and ideal gas laws. We use these 
    assumptions (which reflect some deep truth about individual atoms' behavior) and statistical reasoning to determine probability distributions 
    of outcomes of fluid systems, <i>and are approximately right, much of the time!</i> If we have solid results on the level of small groups, 
    results that reproduce, it doesn't matter that the bodies in question are sentient. We've already done much of the hard work in finding the 
    laws that determine the behavior of the unit (the small group of people, for example in an auction or prediction market). Now all that is left is to 
    apply probabilistic reasoning to make predictions about many such units (eg. a country or religion) in the framework of statistics. Perhaps the reason 
    we can't do this yet is because 7 billion people is not enough for such behavior to manifest robustly, and indeed we do need closer to a quintillion. 
    Or perhaps there are behaviors that emerge with scale (eg. differ greatly between 100 and 1 million people) and so these psychological results 
    from Kahneman and Tversky are rendered useless. Or perhaps we need a new kind of statistics to reason about such a thing, in the same way that 
    Boltzmann had to make strides in probability theory itself to formalize his laws of thermodynamics (the greatest scientific result ever published, 
    I should add -- alongside evolution, also a theory that is statistical in nature). 
</p><p>
    Therefore I hold two contradictory beliefs. One is that much of the work of the economist is doomed to failure because human behavior is too 
    complex to predict, even probabilistically. The other is that we <i>do</i> have empirical results that predict such behavior, and this means that 
    better statisticians than those currently living may be able to generalize these to larger groups, and then use them to make accurate predictions 
    about the behavior of a few million people, in response to some policy or conflict. I think there is more to be fleshed out in trying to ascertain 
    what a real theory of psychohistory would look like, and how it would be notably less "theoretical" and much more statistical than current 
    approaches to economics or social science. It would probably have to come from someone trained as mathematical statistician, who understands human 
    behavior on an practical (think bar fights and comedy clubs) and well as formal (Kahneman's work) level. 
    </p>
    <p>
      [TODO: Think about what such a theory might look like more specifically. Or try to build it yourself and see what roadblocks there are. ]
    </p>

    <a href="#top">Back To Top</a>
    <br>
    <br>

</div>


<!-- <button class="accordion"><h3>Is Psychology Legit?* (II)</h3></button>
<div class="panel">
  <p> This piece continues from the previous one. In this sequence, we review a bunch of (randomly selected) top psychology papers and see how much they 
    really have to contribute along three dimensions: 1) believability, 2) unexpectedness, 3) applicability. We do this to see 
    whether the disparaging claims of many hard scientists against psychology and similar social sciences are merited, as well as to 
    feel out what the state of the art in the field looks like. With that, let's begin our first paper review, keeping in mind that even a few papers from a top journal cannot represent the 
      diversity of any field, so any judgements we make may be more specific to the paper or the sub-field than relevant to 
      psychology at large. 
      </p>  
  
  <i>People systematically overlook subtractive changes.<br><br>
      Adams, G.S., Converse, B.A., Hales, A.H. et al. <br>
      Nature 592, 258â261 (2021). </i>
    


      <p>
      <i>Summary.</i>        

      </p>

</div>

<button class="accordion"><h3>Is Psychology Legit?* (I)</h3></button>
<div class="panel">
  <p>While reading a book about the Vienna Circle, I became intruiged by the prodigious reputation surrounding Sigmund Freud as a 
    legendary historical figure. Since I know very little about psychology and Harvard has a strong presence in the field, I thought I'd 
    go through the introductory psychology course, taught by none other than Steven Pinker (whose work I, broadly, respect). 
    </p>
    <p>
      Just two lectures in, I felt like puking. I generally have a rule that I see these sorts of things through to some sort of conclusion -- 
      reasoning that many (seemingly) smart people make this field their life's work, so their must be something to it, right? 
      And most of the time one has a knee-jerk reaction like this, it's typically incomplete and immature. I came away from my little foray 
      with a bitter taste in my mouth, feeling like the course was entirely devoid
      of thick intellectual meat. 
<p>
      For example, discussions around the visual system typically involve a bit of philosophy (eg. dualism as a past theory of sight), 
      a bit of neuroscience (rods, cones, and optic nerve and retina anatomy/mechanisms), a bit of machine learning (edge detection 
  similarities in CNNs as well as the retina, GPU vs visual cortex analogies). It's exactly this sort of broad shallowness endemic in 
  undergraduate psychology courses, coupled with the 
  intellectual smugness of psychologists in their books, and their <a href="https://en.wikipedia.org/wiki/Replication_crisis">abuse of statistics</a>
  that is offputting to me. While some social scientists (eg. great economists) seem to reasonably be jack of all trades, much of the 
  (admittedly, little) psychology I've read presents even the best psychologists as instead masters of none. 
</p>
      <p>
      A reasonable counterpoint: "but Tanishq, you've got it all backwards! Many of the advances made in fields like ML have come from psychologists! Consider McCulloch and Pitts, 
      who virtually pioneered the notion of convolution in the visual system long before Yann LeCun came up with the idea of a neural network!" This 
      is a fair point. Let's put aside the fact that Pitts was a logician and this was published in a journal of biophysics and 
      not psychology, and pretend it's a typical psychology paper. My response is two-fold. Primo, my criticism isn't really of psychology as an entire field. I think any criticism of 
      something so broad is bound to be missing something. My criticism is directed toward undergraduate level courses in psychology. Secundo, 
      and more importantly, these important contributions were made when psychologists acted like experimental biologists.
       And I sure love and respect biology, since it's a falsifiable natural science that is both beautiful and useful. 
      </p>
<p>
      To see if my initial impression was because I just surveyed an introductory course, I looked at the material taught in graduate courses, only to 
      find that they had more jargon, that, once unpacked, was equally devoid of nuance. Much of the prose hid behind the august reputation 
      of the human mind. Indeed, the human mind is fascinating, but I left with a feeling that I'd be better off studying it in the statistics, CS,  
      physics departments or at the medical school across the river, rather than in the psychology department. 
    </p>
    <p>
      One big motivation I had for taking the course was in hoping for a better understanding of human nature on the level of individuals. I 
      envisioned clinical psychologists making heavy use of their psychological training to read people like one turns through the pages of a 
      book. Instead, much of even relatively advanced courses in what seems to be the strongest psychology department on Earth seems to reduce to 
      giving names to experimental findings that can be understood by anyone who understands the scientific method and high school biology. Instead, 
      as Taleb puts it, the way to learn to "read people like books" is through, as he says, "working as a bouncer at a club, bartender, getting in street fights, 
      trying your hand at seduction, and at stand up comedy." 
      </p>
      <p>
      Another primary motivation for taking the course was to lay a foundation for more advanced courses that would allow me to read and understand 
      recent findings in the literature. And so in this series of pieces we examine some of the most recent papers in <i>Nature Psychology</i>; quite literally, a random 
      sample of the top papers that came up in the (top) journal when I looked it up. I'm going to walk through the experimental methods and findings 
      of each paper, rating them on a few dimensions: 1) believability (how much money I would bet that they can be reproduced, on demand, and reflect 
      some actual invariant truth about the world), 2) unexpectedness (good science uses experiments to infer mechanisms and 
      make surprising predictions using that understanding), and 3) applicability (could 
      a real life practitioner of uncertainty -- say, a mafia boss, software entrepreneur, or hot dog stand owner -- use any of these insights). 
      If a paper majorly lacks in more than one of these dimensions, I don't think it makes a real scholarly contribution. 
      </p>
      <p>
      Moreover, as we'll see, 
      pretty much all the literature is accessible to someone who knows very little about psychology, 
      but has a strong understanding of the scientific 
      method and statistical inference, as well as the fundamentals of biology. In this sense, most undergraduate psychology courses have turned out useless to 
      the end of reading and understanding the scholarly frontier of the field. Yes, yes, knowing about the Id and Superego may
      perhaps (if you squint sideways while standing on one leg under a full moon) serve as vital "broader context"
      to a paper even if it has nothing to do with psychoanalysis, but if I can grasp 95% of the contribution and limitations of a paper, I am content. 

      Note that this is not the same with other fields. You cannot understand the frontier and future of algorithmic game theory, operating systems, algebraic 
      number theory, or cosmology, without a strong understanding of the undergraduate fundamentals. In that sense, those courses have some 
      payoff downstream (as well as being generally more convincing along the aforementioned dimensions). 
      </p>
      <p>
      With that, we begin -- with a genuinely 
      open mind, hoping to see psychology at its best and have our prior biases proved wrong, as many knee-jerk reactions are. Each part of this 
      sequence will go through a top psychology paper in reasonable detail, seeking answers to some of these questions.  
  </p>      
      
    <a href="#top">Back To Top</a>
    <br>
    <br>

</div> -->

<button class="accordion"><h3>Knockoffs, Deep Dive*</h3></button>
<div class="panel">
  <p>
  The paper <i>Controlling the False Discovery Rate via Knockoffs</i> by Barber and CandÃ¨s in 2014 is, in my opinion, 
  a great example of statistical theory at its best. In it, they present a new way to identify causal covariates in high dimensional settings. 
  Put more simply, if you have 10,000 genes potentially contributing to a disease, how do you identify which ones are most causally important, 
  given all sorts of correlations between the genes, known, and otherwise? Minimizing the false discovery rate (FDR) of causal 
  factors is crucial in fields ranging 
  from the obvious (natural sciences like genetics) to the subtle (A/B testing web designs at big tech companies). Their new method for doing so
   is intuitive, 
  and in some sense, obvious (in hindsight). That's how you know it's the real deal; a graduate student in statistics tells me it's the sort of 
  thing that will be a standard part of inference courses in 30 years time. Here, we take this paper apart, 
  studying what the core contribution is, how the authors might have come up with it, why it's important, and the new questions posed in the 
  field (high dimensional inference) as a result. With that, let's begin. 
    </p>
    <p>
      [TODO]
    </p>

    <a href="#top">Back To Top</a>
    <br>
    <br>

</div>
<!-- <button class="accordion"><h3>The Riemann Hypothesis*</h3></button>
<div class="panel">
  <figure>
    <blockquote cite="David Hilbert"><i>If I were to awaken 
      after having slept for a thousand years, my first question would be: 
        has the Riemann hypothesis been proved?</i></blockquote>
        <blockquote style="text-align: right;">David Hilbert</blockquote>
      </figure>
  <p>
  If that quote doesn't give you chills, I don't know what will. One of my big "long-term" goals is to (at least)
   <i>understand</i>
  the Riemann hypothesis, and our best attempts to solve it thus far. One of the most famous open problems 
  in pure mathematics, it can be stated pretty easily: 

  <br>
  <br>
  The real part of every nontrivial zero of the Riemann zeta function is 1/2.
    <br>
    <br>

    Though unpacking the jargon is a bit more work. Proofs of course, 
    have eluded mathematicians for decades. Before I even contemplate <i>trying to understand</i>
    the problem, as it's phrased, much less attempts made at solutions, I'll have to take courses on the theory of 
    rings and fields, Galois theory, graduate level analytic number theory, as well as a graduate course on complex analysis. 
    This piece is just me carving out online real estate that I will, hopefully, at some point in the future use to 
    explicate and summarize (to myself) the current best attempts to solve the problem, as 
    well as potential future directions. Naturally, this won't happen for a few years, but having this in place is a 
    nice reminder that all these courses are leading up to something!
    </p>
    <p>
      [TODO]
    </p>

    <a href="#top">Back To Top</a>
    <br>
    <br>

</div> -->

<!-- <button class="accordion"><h3>Silent and Subtle</h3></button>
<div class="panel">
  [TODO]
  <p>Interesting how so much effort was put into products and systems we take for granted every day. This will be a short piece exploring some examples of innovations that 
    involve significant thinking behind the scenes but seem almost like "the natural way of things" from the outside. Examples include dynamic programming algorithms in text justification
    and extensive piping systems built inside newly created landfills to capture methane the trash gives out (then burn it for energy). 
    <br>
    <br>
    <a href="#top">Back To Top</a>
<br ><br />
  </p>
</div> -->

<button class="accordion"><h3>Statistical Inference in the Social Sciences*</h3></button>
<div class="panel">
  [TODO]<br>
  <p>One of the biggest determinants of whether I go deeper into economics or not will be how seriously I take the empirical results that populate the literature. I'd like to use this piece to 
    dig into a few examples of seminal work in the social sciences, by seminal authors (think Chetty, Kremer, Duflo), and think carefully about the mathematical assumptions their models make, 
    the confidence I have in their results (how much I would be willing to bet on them being reproduced on demand, tomorrow), and the limits of empiricism in complex, adaptive systems like economies. This 
    survey will involve everything from thinking about how much deviations from distributional independence affect predictions to whether use of p-values makes sense in context and reflects true 
    believability, in the Bayesian sense of the word, and more. It will also involve some allusions to, and analysis of, Nassim Taleb's critique of social scientists and their methods. 
    <br>
    <a href="#top">Back To Top</a>
<br ><br />
  </p>
</div>


<!-- <button class="accordion"><h3>Future Startups as Electron Movers</h3></button>
<div class="panel">
  [TODO] 
<p>
  With the recent boom in enterprise and B2B software, it's easier now to start a company than ever before. Because of Plaid 
  (and associated clones), teens can write apps that move millions of dollars securely, because of AWS, two undergrads sitting around a pizza 
  have access to the same infrastructure that Netflix does, because of Salesforce, etc. This poses a question: in 2133, when there are 
  companies that will offer services to automate every step of your business: onboarding, analytics, payments, manufacturing, CRM, packaging, 
  delivery, does your service then merely reduce to solving an information problem for the consumer, to hitting 10 different APIs in succession?
  Do they then just pay you to move electrons along a wire?
  <br>
  <br>
  <a href="#top">Back To Top</a>
<br ><br />
</p>

</div> -->

<button class="accordion"><h3>College and Technical Maturity</h3></button>
<div class="panel">
  [TODO: This isn't very clear.]
  <p>
  When living in the Bay Area and seeing what people need to know to build interesting things in various fields, 
  I started to believe that most of what you learn in college is (for most people) useless, 
  and that if you purely optimizing for 
  vocational expertise you're better off taking the foundational courses in your field and then leaving school. 
  My attitude on this has shifted slightly. I maintain that the above stance is, as stated, 
  probably accurate, but I also now appreciate where this thesis is incomplete. 
</p>
    <i>Where I Was Wrong</i>
  <p>
    The initial premise of my argument was that learning new material is only useful insofar as it 
    does one or more of these things.  
    <ol>
      <li>Stretches your cognitive limits (eg. Galois theory)</li>
      <li>Teaches you material you will directly use in the future (eg. intro CS)</li>
      <li>Is fascinating enough that you would regret not studying it (eg. Quantum Mechanics)</li>
    </ol>
    In fact, there's a fourth major 
    reason you'd benefit from a class: to build "technical maturity" in a specific field.
    </p>
    <i>Technical Maturity, Defined</i>
    <p>
    This means understanding the foundational modes of thought of the field well enough that you 
    could easily teach yourself adjacent material in the field. The "in a specific field" caveat 
    is important because it <i>do not</i> think that taking a hard graduate course on, say,  
    randomized algorithms will improve your ability to learn financial economics or solid-state 
    chemistry by making you "generally smarter." I think humans are too bad at applying knowledge 
    across domains (even related ones) for that to happen. Instead, what I mean is that taking a grad 
    course on randomized algorithms <i>will </i> help you learn computational complexity theory, 
    algorithmic game theory, and probability theory. In other words, fields that have substantial overlap 
    in content/keywords, and not just "modes of thinking." I elaborate on this, analogizing it with graph theory
    and giving examples, below. 
</p>
  <i>Knowledge as Graph Theory </i>
  <br>
  <p>
    To understand what I mean by "technical maturity," think of knowledge as comprised of nodes in a graph. 
    Each tid-bit of knowledge constitutes a node, and the connections between them are the edges. For example, 
    a class about line and surface integrals over vector fields would create a new node on the graph, linking it to,
     say, electromagnetism from one's physics class, which are prime examples of line integrals 
    that have physical meaning. That would be an edge between the two nodes. </p>
<p>
    Why is this relevant? The contention that many champions of the liberal arts have is that studying challenging topics in broad 
    contexts makes one "generally smarter" and improve one's "ability to learn," in generality. This is NOT what I mean by technical maturity. 
    This claim of theirs, in the context of our knowledge graph, is claiming that a node about commutative groups from abstract algebra, or 
    the portrayal of feminism in Austen's <i>Pride and Prejudice</i> has edges 
    going to things that people do on their jobs; say, writing code or hiring employees. I think there is a wealth of evidence supporting the 
    fact that humans are very poor at transferring knowledge across domains, even closely related ones. Therefore, this "liberal arts as building critical thinking skills" school of thought,
    I see as pretty much baloney. </p>
    <p>
    But they're on to something. Instead, I claim, knowing about, say, commutative groups is useful for 
    learning molecular representations in chemistry (think of these topics as knowledge nodes, I'm claiming most people would be able to connect the two). Indeed, it's quite different from abstract algebra, 
    but has clear and direct applications of group theory and so is <i>similar enough</i> for meaningful transfer to happen. 
    In other words, not as different to abstract algebra as, say, doing corporate taxes in Excel. Just different enough. 
    Similarly, knowing about the portrayal of feminism in Austen (again, think of this as a node) really could 
    concievably be directly useful when studying the Suffragete movement because, again, they're different yet similar <i> enough</i>. 
    </p>
    <p>
    Therefore what I mean for technical maturity is comfort around a set of keywords. In abstract algebra, one keyword might be "symmetric structure" and in feminist studies one might be 
    "intersectional theory." Having heard these words in other contexts -- even very different ones -- adds value and builds "technical maturity." However, groups and feminist theory is too 
    far from writing code or drafting speeches to reasonably possibly have any keyword overlap. </p>
    <i>A Concrete Example</i>
    <p>
    In the fall of my freshman year, I took my first rigorous math class. It studied linear algebra from an abstract perspective. One of the things we learned about were 
    unitary operators, which are linear transformations that preserve inner product structure. During the winter break 
    after that semester, I perused a textbook on quantum computation. And in quantum computing, the unit of logic/computation is the idea of a quantum 
    gate, itself a linear transformtion that takes some input and maps it to some output. Being a linear transformation, we represent such logic gates as matrices, 
    and they operate on vector inputs. 
    </p>
    <p>
    One thing that the author of the textbook really emphasized and spent a good amount of time explaining was why we used
    unitary operators for our logic gates. Evidently, he thought most people reading the book would find that unmotivated. But because I was lucky to take a relatively abstract linear 
    algebra course <i> and had taken a class on probability </i> beforehand, it didn't need any explanation: unitary operators preserve inner products, and random variables are vectors. 
    If you want to ensure the probabilities in the vectors coming in AND leaving the linear transformation (quantum logic gate),
  using a unitary operator is the obvious choice, since they preserve the inner product (sum of probabilities) of their input. This is the key point: taking the class on abstract linear algebra
  and one on probability firmly planted nodes (unitary operators, random variables as vectors), and the new topic I was learning (quantum logic gates) had <i> enough keywords in common that I could 
    reasonably create new edges between these knowledge nodes, on the fly.</i> This is the crux of technical maturity: increasing your threshold for what is "obvious". </p>
    
  <i>What Might This Look Like In Practise?</i>
  <p>
    If you see college mostly as I do -- a 4-year course on "learning to think like an X" 
     for many different X (mathematician, programmer, 
     physicist, economist, statistician, historian, psychologist), you should optimize for taking 
     difficult, foundational courses in many fields. It's important to emphasize that by 
     "foundational" I don't mean introductory. For example, I do not think 
     taking an introductory CS course or two suffices to "learn to think like a computer scientist."
     Instead, take intermediate/advanced courses in algorithm design and operating systems to get those 
     gains. 
     </p>

<p>
     At Harvard, you need to take 12-16 courses in a field and 32 courses overall to graduate with 
     a degree in that field. What if, instead, you took 
     the hardest/meatiest 6/12 in a several related fields? In math, perhaps that includes 
      group theory, rings and fields, real and complex analysis, differential geometry. In physics 
      perhaps classical and quantum mechanics, electrodynamics and thermodynamics?
       In economics, maybe the intermediate micro/macro sequence 
      alongside game theory, economic history, political economy. In CS, 
      algorithms and operating systems, programming paradigms, complexity theory and 
      compilers. You'd get 80% of the value of 3-4 different undergraduate degrees, and more importantly be equipped to at least understand parts 
      of the research frontier in each field. Teaching yourself would become much easier, and you wouldn't really be an outsider to any of the fields. 
</p>
<p> The obvious caveat here is if you're set on spending your life exploring and deepening a specific field. If you want to be a mathematical physicist, ignore my advice. Go ahead and 
  take quantum field theory and Lie algebra at the expense of understanding how economists or computer scientists think. That is the price you pay for mastery. And that's fine. But 
  most people are quite as pointed in their goals, and so my thoughts are probably more relevant for them (and myself). 
</p>

    <p>
To summarize, I think that building technical maturity in multiple fields equips you with the ability to learn new things <i>in those
  specific fields</i> (unlike the claims of the pure-liberal-arts charlatans, who claim it builds critical thinking in general). And this manifests itself as 
  deep comfort in novel sitatuations (again, in the fields you've cultivated maturity in only) which leads to very fast learning and growth of knowledge graphs. 
  Ultimately, the point of college is to set down as many nodes as you can in as many diverse fields, constrainted by the fact that you want node density within any specific field 
  (ie. part of your knowledge graph) to be high enough that they roughly approximate/sample the entire space so you can learn new things in new fields and slot them into the context
  of existing knowledge. 
  </p>
  <p>
  This mostly explains the big difference I saw between technical college graduates and high-school dropouts in the Valley -- the college graduates felt 
  comfortable in novel situations because they knew their graphs were wide and dense. The high-school dropouts mostly had no nodes to use as reference points from which to grow their graph, 
  and so struggled to keep up with new material. Of course, this does not always hold; you can create these graphs outside of the academy and most people in the academy fail to 
  do a good job at creating these graphs at all. But for me, school seems to work well for this, so I'm happy I'm here, for now. 
    </p>

  <a href="#top">Back To Top</a>
<br ><br />

  
</div>

<!-- <button class="accordion"><h3>Paradigm Shifts in Advertising</h3></button>
<div class="panel">
  [TODO]
  <p>
    It's pretty insane how advertising was the epitome of snake-oil-salesman-slime only 
    twenty years ago: think of cringeworthy salesmen flashing nauseating slogans on screen during 
    breaks between TV shows, or catchy theme songs for mundane products like canned Tuna advertised 
    on radio. But today the same word, "advertising," refers to something else entirely. Instead of 
    blanket slogans, we get informative and subtle nudges to explore
     <i> products we actually want </i>. It's clinical -- it sounds ludicrous to think, but I've 
     actually been 
  </p>
</div> -->

<!-- <button class="accordion"><h3>Fast Matrix Multiplication From First Principles*</h3></button>
<div class="panel">
  <br>
  [TODO]
  <br>
  <p>Finding algorithms for fast matrix multiplication is one of those rare problems that
     both 1) doesn't require advanced material to have a go at, and is therefore amenable to attack by measly 
    undergraduates (though the research frontier makes heavy use of sophisticated ideas from abstract algebra), and 2) is <i>vitally</i>
    important for myriad applications, from machine learning to computer graphics. This piece is my attempt at 
    outlining the current state of the art. 
  <br>
  <br>  
    We begin with naive multiplication and make some systems optimizations to see 
    how fast we can make it, then try Strassen's algorithm -- the most commonly used algorithm in practise -- and then 
    try to understand, and implement, more esoteric algorithms like the Coppersmith-Winograd algorithm. We finish off by 
    thinking about what approaches are likely to work, and even trying to come up with heuristics ourselves and seeing how well 
    they do compared to the current state-of-the-art. 
  </p>
  <i>Naive</i>
  <br>
  <br>
  <i>Strassen</i>
  <br>
  <br>
  <i>Coppersmith-Winograd</i>
  <br>
  <br>
  <i>First Principles</i>

  <br>
  <br>
  <a href="#top">Back To Top</a>
  <br>
  <br>
</div> -->


<button class="accordion"><h3>AlexNet: The Innards of a Revolution*</h3></button>
<div class="panel">
  <p>The 2012 paper by Alex Krizhevsky et al. in which he implements a deep convolutional neural network for 
    image recognition, is perhaps the most seminal academic paper that has been writtein in the last few decades, full stop. 
    It started the "deep learning revolution." Despite this, it's interesting to note that most of it's contributions are not novel. 
</p>
<p>
    It was not the first paper to show how deep networks can significantly outperform traditional neural nets for tasks like image recognition. A paper by 
    Ciresan et al. (2011) had already done that. It was not the first paper to show how GPUs can used to train nets much faster than CPUs. Ciresan's paper 
    also was written in CUDA to exploit GPU support. And of course neither has a claim to originality as far as the concept of convolution in image 
    recognition is concerned; Yann Lecun (2018 Turing) came up with the idea of a CNN in 1989! I AlexNet's success is an example of 
    "incrementalism as discontinuity" innovations. By this, I mean that it has no substantive novel contribution as far as theory is concerned, 
    but makes incremental improvements on existing ideas, combining several important existing ideas (depth, GPUs, dropout regularization)
    in a way that makes it pass an important threshold. The fact that AlexNet classified over 10% <i>more </i> images in the ImageNet
    challenge than the runner up is key because that meant it passed the "threshold" for computer vision being practicable. 
    Being the first to cross that threshold (via a well placed incremental improvement), its architecture became ubiquitous, instantly. 
    Alex Krizhevsky is now immortal because of this. 
    </p>
    <p>
    In this piece, we're going to pore over every page of the paper and see how the authors might 
    have come up with the ideas, and what goes into leaps of technical innovation that are so great that other innovations in the future 
    become analogized to them. For example, when DeepMind's AlphaFold 2 surpassed the 90% protein classification threshold in 2021, it was called the 
    "ImageNet/AlexNet moment of biology." That's high praise. With that, we begin. 
    <br><br>
    [TODO]
    <br>
</p> 
  <br>
</div>

<button class="accordion"><h3>Hesitations on Economics: Falsifiability</h3></button>
<div class="panel">

  <p> 
    <br>
    As I become more interested in economics and ponder the trade-offs associated with a career in 
    academic economics and, say, the high-technology industry, one thing that keeps coming to mind is the notion of 
    "falsifiability" in a job. I define it as the ability to see clear and immediate cause-and-effect between 
    the work you do and associated outcomes on the world that are a direct consequence. For example, 
    medicine has falsifiability; people trust doctors because if a doctor is not good at their job, 
    people will die and it will be clear to everyone. Similarly, business owners have falsifiability: there is an easy way 
    to tell if you're doing your job right--see if you're making a profit. A business owner always runs the risk of 
    going out of business. And they <i> own that risk</i>. 
<br><br>
    This isn't building up to a criticism of the academy. On the contrary: mathematicians and physicists have falsifiability, too. 
    If your proof of a proposition is incorrect, it will be clear to everyone and unambiguous that you have failed at proving the proposition. 
    If your theory of gravitation yields bad predictions for how a projectile or planet will behave, one can simply conduct an experiment and see 
    that your theory is manifestly wrong. It's not just the presence of clear right and wrong or lack of ambiguity, 
    it is also the <i>ownership of risk</i> and <i>coupling of cause and effect</i> in one's job. 
<br><br>
    One of the reasons I've become entranced with the study of economics is because it's the rare subject that requires a deep understanding 
    of a variety of fields. A successful economist must have a near-professional working understanding of mathematics, statistics, programming, but also 
    history, politics, philosophy. It's the rare field where studying measure theory and reading <i>Hamlet</i> can both be reasonably argued to be 
    relevant to your job. And it's really impressive to see someone go from proving a difficult theorem on a whiteboard to making a moving speech about 
    the future of a society. Finally, I'm convinced it's possible to have an <i> enormous scope of impact</i> when doing your job right. For example, 
    watching Michael Kremer (2019 Nobel) talk about his work on de-worming showcased to me how his experiments in developing countries gave strong evidence that
    de-worming policies in primary schools can have huge returns in enabling students to be healthy enough to stay in school for the long haul. As a direct 
    consequence of the work of <i>him and his collaborators</i>, hundreds of millions of students are benefiting from a policy that might have made the difference between them getting a 
    high school education, and not. <i>That,</i> is falsifiability. 
    <!-- [TODO: clarify] -->
<br><br>
    My problem with economics is that such examples of falsifiability are far and few between. It often feels like economists are commentators on the side-lines 
    of a football match making strong comments about the performance of players, where they themselves would struggle to play against a talented middle-schooler. 
    When an innovation economist makes strong claims about the conditions required for innovation and about the traits a successful innovator should have, it makes one think:
    <i>if you truly are correct, why don't you act on this knowledge and start a successful research lab yourself?</i> In other words, because the study of 
    societies is so complicated, with so many moving parts, it's easy to conjure up an explanation for why your hypothesis was incorrect, and difficult to 
    really know if you were, unambiguously, right, or wrong. 
<br><br>

    <!-- [TODO: film directors, writers also have falsifiability, not just scienc-y types] -->

    In <i>Good Economics For Hard Times</i>, the authors point out that medical professionals have very high trust ratings by the public, and economists very low trust ratings. 
    I think this is because of falsifiability. A doctor owns the risk they run from the predictions they make. If they predict a tumor being benign, and it's malignant, 
    someone dies. If an economist predicts a causal mechanism between a certain tax policy and patent rates, they are never held accountable because 
    they can always behind the shroud of "further study is needed", even if policies based on that research might have cost 
    their country decades in patent-years worth of innovation. In some sense, it's as if the "doctors of the economy", who have potentially the most 
    leverage, are held the least accountable and taken the least seriously. 
  <br><br>
    As a direct consequence, surprisingly little economics research makes its way into policy because, 
    as Banerjee and Duflo point out, "economists are not in the business of futurology". This is my problem. If economics isn't meant to make any predictions about the 
    future, what use is it? The entire point of science is to understand underlying mechanisms using the scientific method, and then use that 
    understanding to make falsifiable predictions about the future, and be held accountable for those predictions.  Anything else is, 
    as my father puts it, "intellectual masturbation". In short, <i>futurology is the very point of any scientific discipline</i>. It is the raison d'etre, it is the holy grail, it is the 
    single source of truth. 
<br><br>
    And so if I opt not to pursue economics in the academy, you'll know why. I want to be held accountable for the work I do. 
<br><br>
<a href="#top">Back To Top</a>
<br ><br />
</p>
</div>

<button class="accordion"><h3>Vaclav Smil</h3></button>
<div class="panel">
  <p>

  [TODO] <br>
Smil is a profilic author on energy economics and history, and his work is remarkably well evidenced and broad in its scope. His books are 
some of the best works of nonfiction I've read, where <i>Energy and Civilisation</i> literally changed how I look at the world. In this short piece, I'll try to 
articulate some areas of disagreement, where I think he's wrong, without reducing my stance into the sort of blind techno-optimism that is pervasive in 2020 Silicon Valley. 
Given below are some broad and strong claims he makes in either EaC or <i>Creating the Twentieth Century</i>. Each claim is either mostly unsubstantiated, or just a flat out an opinion 
disguised as fact. NOTE: CTTC is the title of the book, but I use it to refer to the group of innovations 
he discusses in the book, innovations made from 1867-1914 (Haber process, x-rays, automobiles, etc) that 
he believes are far more impactful and epochal than the late computer industry. 
<ul>

  <li><i>The late computational revolution is less impactful on human life than that of CTTC. </i></li>
My central argument against his belittling of modern innovation is this: itâs true that CTTC innovations were wide-ranging and also deep: energy, agriculture, 
mechanical/structural engineering, chemistry, electronics, and more, and that modern innovations have been largely limited to software/internet, but I 
think thatâs a reductionist argument/stance for him to take, that just because there were transformative innovations across more fields, it was a 
better time. I think a fairer lens would be to look at how peopleâs lives have been changed. Iâd argue that since most people spend most of their 
days in front of screens (like many would spend on the farm centuries ago), the seemingly pedestrian innovations like Google, FB, Uber, AirBnB, Amazon, 
have an enormous impact in how people live their lives on a day to day level, just as much as the automobile and electric lightbulb put together. I think 
just as how the liquefaction of air and mass production of electricity are seen as innovations in different domains, the widespread use of technology for transport and 
technology for e-commerce is also quite different in how users end up using them, despite both being classified as âsoftwareâ innovations.<br><br>
Sure, some innovations are lower down the abstraction chain and necessary to literally sustain life (Haber process) but I think that those innovations can 
tautologically only come about once: you canât revolutionize agriculture from shortage to insane surplus more than once. If itâs largely a solved problem 
(like food/urban lighting/etc) then thereâs no incentive for lots more innovation. I think innovations higher up the abstraction chain are important 
and revolutionary in the same way as long as they impact the daily life of billions of people in a nontrivial way, which they certainly do. <br><br>
I also think the idea of invention vs improvement is more subtle than Smil lets on. Take the Hall aluminium process--it was new/step function insofar as that 
particular manufacturing process hadnât been seen before, but we could manufacture aluminium before, the Hall process was really just a (massively) 
improved way. Itâs not like manufacturing aluminum was impossible beforehand. Yet Smil counts this as a step function improvement (doing something 
that literally could not be done before), but then contradicts himself discrediting the invention of the transistor as merely an âincremental 
computational improvement,â even though itâs a new technology that does something that could be done before, but massively better, just like the 
Hall/Haber processes. Either massive improvements on existing methods count as step functions or they donât. You canât selectively apply that definition 
and then use your mistaken assumptions to imply modern electronics is âpedestrianâ in its contributions. Itâs a subtle semantic game he plays, and I think, more 
generally, incessant use of data (sometimes somewhat irrelevant and distracting) helps him embed these logical inconsistencies with more ease. 
<br><br>
	<li><i>Consumerism and high-energy society is bad and we should not use any more than we need. Instead of trusting energy innovation, we should live within our means. </i></li>
	<li><i>Major artistic leaps might not have happened if not for this singularity of technology innovation.</i></li>
	<li><i>Human ability to harness increasing amounts of energy more easily is the most important proxy for technological progress. </i></li>
    I think this was true throughout history, but starts to break down in times of energy excess (which had never existed before a few decades ago). Even the poorest in the world have access to gas to cook and heat with, and lights to light their houses. If we have enough energy to live above a certain comfortable baseline (much like how Smil draws that baseline in nutrition as having enough calories/diversity to be healthy), then of course you shouldnât expect huge leaps forward, because itâs a solved problem. Iâm not saying energy is âsolvedâ certainly, but as with any system offering diminishing returns, you canât expect huge leaps in energy if itâs not the bottleneck for human prosperity and survival anymore. And if progress, for Smil, is definitionally tied to the ability to harness increasingly powerful prime movers, you definitionally wonât see progress, which is kind of the case he puts forth. 
    <br><br>
    <li><i>Using "how surprised a contemporary scientist would be if they were magically transported to more modern times" as a metric/proxy for technological innovation </i></li>
For all my cynicism that Smil undervalues modern computational technology, I think heâs probably right on this front: going from 1850 to 1920 would be more surprising to competent scientists than 1920 to 1970 or 1970 to today. But two points in defense of modernity: 
<br><br>
Firstly, I also think many modern innovations are very impactful but not visible or tangible in the same way that CTTC innovations were. While illuminating a dark city or taking flight off land are very memorable and iconic, searching web pages for accurate information and allowing people rent cheaper holiday homes pales in comparison, but I donât think itâs much less valuable just because itâs higher up the abstraction ladder: billions of people get access to exactly the right information they need to navigate every question or decision they face in life at the tap of a finger, for free. Millions of people who would never otherwise choose or be able to afford a memorable and thrilling holiday experience can now do so. From an economic perspective, tons of resources that were left empty before are now being utilised (empty homes via AirBnB, for example). I donât think these are âpedestrianâ just because theyâre less sexy. 
<br><br>
Secondly, I think tech innovation comes with diminishing returns. String theory is objectively harder than Newtonian physics. Going from starving on a dark farm to eating meat in air conditioning illuminated by mysterious electric light (a jump that happened in probably 20 years) is a bigger leap than going from eating meat in aircon to driving to an office job. Modern society has to work much harder to sustain the same amount of improvements in quality of life, just because most of the big improvements (getting people off dangerous, painful farm work and into houses) has already been done! In this sense, weâre fighting an uphill battle: and this will continue to become harder over the next few centuries, so any singularities of tech innovations from 2020-2500 will be that much more impressive (discontinuities despite most of the low hanging fruit having been picked). 
<br><br>
He also slips in somewhat of a straw man: who said that the CTTC era innovations will be gone soon? For the world to move forward, older inventions donât necessarily have to be supplanted, just built upon. If we go with his line of reasoning--that modern innovation is less than CTTC because we still use many things from that era--I could say we havenât innovated at all from Han China--we still use wheels, after all! Of course we can expect to use the Haber and Hall processes for the foreseeable future--and I wouldnât be surprised if they remain the backbone of high-energy civilisation for centuries to come--because they do their job so well there really isnât much need for additional innovation. Incentives for innovation constantly move to where thereâs a bottleneck: once feeding the world has been largely solved (as it has), it is no longer profitable to try and improve on existing solutions. Just because these fields (where basic innovations have been sufficient for human life to not warrant significant improvements) exist doesnât mean that innovation is dead. That would be silly, yet Smil makes it sound like itâs a legitimate line of reasoning. 
</ul>

<a href="#top">Back To Top</a>
<br ><br />
</p>
</div>

<button class="accordion"><h3>Pedestrian and Profound</h3></button>
<div class="panel">
  <p>

[TODO] <br>
For a long time I, like most people, thought that many of the great thinkers of eons past were so great
because they were brilliant and had stunning insights that revealed something fundamentally new about the world. 
This is true, but it presents a deceptive, reductionist about how they became famous. Most of the time, 
their work got the time of day not on its own merits, as if they simply published it and were lauded as geniuses who'd 
cemented their place in history henceforth. Many of them displayed startling and shocking amounts of what today we call 
hustle or resourcefulness. These entrepreneurial qualities are not typically associated with great thinkers, but 
I believe are central to why these people, and not other equally smart people, went down in history. In short, these 
people are so great not just of their ability to grapple with the profound, but also because of their ability to 
navigate the mundane and pedestrian obstacles that encumber modern "entrepreneurs" trying to enact change in the world. This is a list of 
such examples, added to as I come across them. 
<ul>
  <i>Nuclear Physics & the Manhattan Project</i><br>
<li>Einstein and Szilard patented a EM-based fridge, experimental physics is to real physics 
what a rails/django app is to computer science: success depends more on creativity and energy 
than knowing advanced material.</li>

<li>When Szilard mentions being âaboveâ joining wood pieces together âlike a painterâ on the experimental 
front in Berkeley, Fermi stops collaborating with him (importance of being very good at both pedestrian and profound). </li>

<li>Similar with Bohr spending long coordinating architectural plans for his new Institute in 
Copenhagen and haggling prices & recruiting people etc. </li>

<li>Also similar with Leo Szilard doing radioactivity experiments at Bartâs hospital--think of the 
persuasiveness/networking that must have 
gone on behind the scenes to convince a hospital to let a rando do experiments with radium 
(and this non-science-related-skill was crucial as it gave him the initial publications that led him to Oxford).</li>

<li>Alex Sachs (banker) delivering fission letter to President being crucial as kickstarting the 
whole process and it came down to the scientists knowing the right banker to make something happen 
(analogous to technical founders seeking funding).</li>

<li>When scientists werenât sure if Sachs delivered letter, Szilard arranged intro to Compon (MIT), 
contacted businessmen from before through refrigerator deals, found people to cold reach out to through
 newspapers, etc, all corresponding to hustle associated with getting intros to VCs in SV today which 
 is seen as such a big and important skill but to the masterful physicists was not something they cared about 
 thinking formally about, but was just something natural on their missionary quest to discover the 
fundamental reality of the universe.</li>

<li>Technical innovation in a resource constrained system was the theme during atom bomb wartime as well as 
SV today: Manhattan scientists had to ask for money from gov, Germany tried to source heavy water in context 
of invading Norway, all of the eng challenges associated with creating the first plant seem similar to the 
resource-constrained, people-intensive innovation that happens in software startups today--except of course, 
death was an option in the 20th century. </li>

<li>France persuading Norway to lend heavy water but Germany failing to, setting them behind in research. </li>

<li>Frisch planning the dragon experiment, with such a creative setup--allowing a ball of U235 to 
fall and be supercritical for a subsecond (reaction slowed down by making it uranium hydride, not pure U235), 
not scientifically advanced but so inventive to think up. Re: clever prototypes that test the fit between
 theory and experiment in a fast and cheap manner. </li>

<li>When Szilard wanted to make a final push at talking to the President about deploying 
nukes, and FDR died, he got in touch with a mathematician who had worked in the Kansas City 
political circles, got him to intro Szilard to the people there, stunned them with grandiose 
physical ideas and they arranged a meeting with the president at the White House (since Truman came from Missouri).</li>
<i>Renaissance Florence</i><br>
<li>Michelangelo managing a team of artists and apprentices through creative competition,
 negotiating affordable prices and paying enormous amounts of 
detail to sourcing quality materials, recruiting talented up-and-coming artists, marketing and branding his 
ability, pandering to wealthy Medici patrons. He knew his crew and their relationships with each other, 
choosing project teams to optimise chemistry. </li>

<li>Leonardo would loiter around public courtyards looking for ugly people with distorted facial features, 
  and then would chat them up, befriend them, invite them to his home so he could spend dinner silently studying 
  their faces and ingraining their subtle details in his mind, all the while entertaining them. As soon as they left, 
  he'd use this newfound inspiration to draw his grotesques. An astonishingly creative and charismatic way of overcoming 
  the inspiration problem. I can imagine if some entrepreneur had done stuff like this en route to starting 
  a successful company, VCs would be hailing them as the epitome of "resourceful" and "inventive", but things like this 
  were throwaway actions taken for granted amongst the great Italian masters of antiquity. 
</li>
<li>Michelangelo would make a sublime drawing of an ornate part of a cathedral to be build on one side of a paper, 
and then draft a letter ordering bushels of grain for his oxen on the other side. </li>
</ul>
<a href="#top">Back To Top</a>
<br /><br />
</p>
</div>


<!-- <button class="accordion"><h3>Why Study History?</h3></button>
<div class="panel">
  <p>

      [TODO]<br>
  This is a question that really bothered me for a long time, since the present is so different to the past 
  that extracting insights that are relevant and actionable is nigh-impossible. 
  When I asked historians, the 
  responses I got were unsatisfactory: "those who do not study history are bound to repeat it" and other such 
  empty platitudes. After some thought, I think there are two more practical answers, on two different 
  levels. On the micro level, because it clearly illustrates the fragility of human affairs, showcasing cause-and-effect 
  relationships between individual actions and societal outcomes, and on the macro level, because 
  perceptions and understandings of recent history influence public opinions and therefore 
  policy.
  content (n-dimensional vector space, orthogonal)

  <br /><br />
<a href="#top">Back To Top</a>
<br /><br />
</p>
</div> -->

<!-- <button class="accordion"><h3>Reviews for Harvard Classes</h3></button>
<div class="panel">
  <p>
  My thoughts on the classes I've taken at college, or will take in the near future. Courses I've found the most rewarding are given in <i style="color:rgb(38, 150, 255)">rgb(38, 150, 255) </i> and those that I've audited/self-studied
  are given in <i style="color: rgb(52,168,83)">green</i>. 
  <ul>

<u>FRESHMAN FALL</u><br><br>

<li class="sick"><strong>Math 25a (Theoretical Linear Algebra) </strong></li>
The professor (Arnav Tripathy) was a loveable former child prodigy, but you wouldn't be able to tell given how charming and funny he was. 
I spent around 30 hours a week on this class and 
had around 200 pages of LateX'd solutions I'd written by the end of the semester. This class taught me how to think abstractly, 
covering dual/double dual spaces, general bilinear forms, the determinant as a group homomorphism, quotient spaces, as well as the standard 
linear algebra material. I gained a <i> ton</i> of mathematical maturity, and I think this more than anything has equipped me with the ability to take 
grad economics/CS classes starting as a sophomore next year. Huge workload, but commensurately rewarding. <br><br>
<li><strong>CS 61 (Systems Programming) </strong></li>
Initially, this was a WTF-is-this class; I'd never seen systems before and felt like I'd been thrown into the deep-end. Debugging also took 
20+ hours a week and was very frustrating. The professor, though, (Eddie Kohler) was accomodating, funny, yet uncompromisingly passionate. I gained a lot of 
respect for systems, which I thought was just a bunch of nerds messing around with code before the class, especially when seeing how some clever systems improvements can 
have 10,000x improvements to program performance. It also made me a significantly better programmer through the sheer workload and time spent debugging. <br><br>


<li><strong>Stat 110 (Introduction to Probability) </strong></li>
A famous class for the professor (Joe Blitzstein) is a meme-king. It was marvellously prepared and very well run, and really not too bad compared to my other classes, 
at around less than 10h/week. I learned a good amount of fundamentals (important distributions, probabilitistic reasoning, spotting similarities between problems, 
being careful about conditioning and assumptions) that will be used in many economics/CS classes later on. <br><br>

<li><strong>Econ 10a (Principles of Microeconomics)</strong></li>
You don't quite get what people mean by "Harvard has a strong economics department" until the former chair of the Council of Economic Advisors is your 
freshman professor. This was a pretty trivial course and I should've probably taken 1011a in hindsight (the more advanced/mathematical version) but wanted an easy course 
to complement my other three. Nevertheless, the professors were very engaging and inspired a love for economics and its power as an instrument for change that is making me contemplate 
pursuing the field more deeply, having never seen it before. <br><br>

<u>FRESHMAN SPRING</u><br><br>

<li><strong>Math 22b (Vector Calculus)</strong></li>
Mixed feelings. As expected, pretty light and actually had a good amount of computations (which are not trivial, for all my abstract, proof-oriented 
brothers and sisters!). Feel like I didn't learn a whole lot besides, well, vector calculus, maybe that's just because this course had such big shoes 
to fill. <i>Maybe</i> could have taken 25b instead, but then I wouldn't have learned as much from the other courses, so who knows. I'll probably just 
read Rudin over the summer.  <br><br>
<li><strong>CS 124 (Data Structures & Algorithms)</strong></li>
Though this class has a loaded reputation, I thought it was pretty straightforward if a bit dull. Sure, a learned a good amount about 
algorithms and data structures, but do not feel like "it changed the way I think," as Zuckerberg recalls this class. The quality of the peers and TAs 
in this class was very high, which was great, but overall somewhat lukewarm. <br><br>
<li class="sick"><strong>Stat 111 (Introduction to Statistical Inference)</strong></li>
This was great! Though the material initially felt unmotivated and esoteric, the resources/teaching were excellent and 
in hindsight was taught perfectly; homeworks and exams had the perfect balance of theory and practise, constantly showing us how, when and why 
the theorems we derived were relevant in real-world modelling. Also taught me a lot about the limits of inference, exactly what I was looking for.  <br><br>
<li><strong>MIT 6.036 (Introduction to Machine Learning)</strong></li>
I see now why they call MIT a "vocational school" -- I felt almost guilty taking a course that is so "applied"; this was very well 
organized, perfectly scoped, not too much work and excellent overall. It feels weird taking a class that actually teaches you how to 
use certain libraries and perform computational tasks coming from Harvard CS classes which are much more mathematical.  <br><br>
<li><strong>Expos 20 (Writing Seminar)</strong></li>
A bunch of humanities majors jerking each other off. Very prescriptive teaching style that 
treated us like we were 5 year olds. I'd much rather learn to write from someone who 
demonstrably has mastered the craft, like Steven Pinker or Jill Lepore, both humanists at Harvard that I actually take seriously.<br><br>
<li class="audited"><strong>CS 51 (Abstraction and Design in Computation)</strong></li>
Self-studied this class; a great way to learn about some important ideas every programmer should be comfortable with (programming paradigms, readability/testing, etc)
and auditing meant I didn't have to do much of the busywork that enrolled students did. Overall, would recommend learning this material but probably not worth taking as an enrolled 
students if you've done any serious programming before.  <br><br>


<u>FRESHMAN SUMMER</u><br><br>
<li><strong>Economics Research (Full-Time)</strong></li>
Excited to do economics research as an RA this summer! Hopefully it'll tell me whether the field is a good fit for me, and what the day-to-day life of an 
academic is really like.<br><br>
<li><strong>Applied Math 115 (Mathematical Modelling)</strong></li>
A cool summer class that covers the fundamentals of building mathematical models in generality, from first principles, studying things as diverse as 
traffic prediction, cell systems, financial markets, population dynamics, and more. I think modelling/simulation is one of the rare "applicable" skills that math 
has to teach, and so it's crucial to learn it well no matter where one might end up.<br><br>
<li class="audited"><strong>Econ 10b (Principles of Macroeconomics)</strong></li>
A gentle intro to macro to make sure I have relevant background for economics research over summer, plan on going through this material in a few days after the 
end of spring semester/at the beginning of summer. <br><br>

</ul>
<a href="#top">Back To Top</a>
<br /><br />
</p>
</div> -->

<button class="accordion"><h3>Welcome To The Future</h3></button>
<div class="panel">
  <p>
    Modern, high-energy, high-tech society really is something to marvel at. 
  Today, we can use artificially intelligent machines to manufacture convincing 
  DeepFakes of politicians and celebrities doing or saying things they didn't. 
  Social media apps are used by billions of people--a level of access that puts them in the august company of institutions like the Catholic 
  Church. What's more, these apps can--and have--influence political action by pulling psychological strings in users' minds,
  and thereby potentially changing the outcome of international relations and elections.
  All of human knowledge is accessible to even the poorest villagers, at the tap of a fingertip. 
  Software that started as a web crawler became a search engine and then the planet's de-facto 
  arbiter of truth. <br><br>The difficult moral quanderies philosphers toyed with decades ago as mere hypotheticals are now 
  coming to pass. We are living in the future. <br><br>
  We send goods created locally 10,000 miles across the world because it is cheaper to have them 
  refined there and transported all the way back than to process them locally. And we still 
  get those goods on our doorstep, exactly when we want them. It has been many decades since 
  a conflict on the scale of those seen in the early 20th century, with a few lines of code 
  now able to wreak more havoc on nations than armies of men in ages past. Despite living in a 
  vastly more connected and globalized world than ever before, a terrifyingly infectious pandemic has killed 
  less people in many months than cigarettes or car crashes. We have robots stacking shelves 
  in titanic warehouses, welding parts, and helping construct buildings.
  <br><br>
  For such an advanced civilisation, we are curiously blind in places. In the 
  most advanced nation on the planet, public infrastructure is crumbling. Up to a quarter of roads and bridges are 
  rated unsafe because there isn't enough money to maintain them. Improvements in life expectancy 
  are plateauing, and our grandchildren will suffer under the weight of our environmental 
  negligence. The democracies that have brought unprecedented human literacy and social equality
  are the very same systems that incentivise politicans to please swing voters
  in the short term, at the expense of significant, irreversable environmental damage in the long-term.
  The average member of the 
  American public struggles to multiply single-digit numbers, tell the difference between a country and a continent, 
   or paraphrase a simple argument. And they're overweight. Almost half the American public doesn't "believe" in 
   evolution, and most people who do can't explain what it is!<br><br>
  We are living in the future, yes, but we have not yet fully escaped our past. I wonder what 2120 will look like. 
  I suspect less different than many think.
  <br><br>
<a href="#top">Back To Top</a>
<br /><br />
</p>
</div>

<button class="accordion"><h3>Clusters</h3></button>
<div class="panel">
  <p>
    Despite technology making it easier than ever before to source materials, inputs, employees from across the 
   world, the most innovative, iconic, and productive groups flock to central hubs. Examples of clusters are 
   Hollywood, Silicon Valley, Wall Street. But other, more niche clusters exist too, and very much influence the 
   trajectory of industries and progress: Boston for biotech, Shenzhen for hardware & electronics, South Germany for 
   automobiles, Southern California for wine, and more. And this is nothing new, historical clusters include 
   20th century GÃ¶ttingen and Berlin for theoretical physics, Renaissance Florence for fine art, ancient 
   Athens for philosophy, industrial London for engineering, and more. <br><br>
   This piece is based on 
   <a href="https://hbr.org/1998/11/clusters-and-the-new-economics-of-competition">an HBR article</a> 
   about the economics of 
   clusters, and I'll draw on some of its content while highlighting the factors I think contribute most to 
   giving clusters a disproportionate edge. I'm slightly biased towards clusters because I moved to SF thinking that 
   the internet makes it as easy to start a successful company in London or Abu Dhabi as in the Valley, only to 
   see firsthand how wrong I was, and how many decades ahead the rest of the world the Valley was. 
    <br><br>
    Historically, competitive advantages came from sourcing better inputs for a lower cost than your competitors. 
    Because the differences in input costs could be such a stark advantage, improvements in knowledge, management and technique 
    weren't as valued, and being close to a port or railway line was an immense advantage, and so companies would 
    cluster around them. As freight and shipping make procuring 
    quality goods from across the world cheap, reliable and quick, the advantages in knowledge, management and technique 
    have become decisive. Clusters having a huge advantage on this front, too, as a consequence of in-person meetings 
    being significantly more effective at inciting progress and action than virtual ones--a fact I conjecture, but one 
    that seems to be empirically true. At a high level, I think the advantages clusters hold in a modern knowledge 
    economy include: lower barriers to entry, process knowledge, peer pressure, agglomeration economies, sophisticated markets, and public investment.
</p>
    <ul>
    <li><i>Barriers To Entry:</i></li>Vertical integration; that is, doing everything yourself--like Apple did from 
   software to hardware--is hard, and usually only enormous companies have the resources to pull it off. In other words, 
   if you're starting a trading business based on your innovative algorithm, you don't want to spend time 
   doing perfunctory paperwork--you pay a lawyer to do that. Clusters have abstractions available for 
   common use cases that you can't easily get access to outside them, and this means people starting new ventures in those 
   clusters get them off the ground significantly faster. In the context of a technology startup: when you're living in 
   South Park in San Francisco, your friends tell you about another tech startup that was created to automate 
   accounting for companies. It hasn't grown outside the Bay Area yet, but because you're headquartered next door, you 
   grab a bite with the founder--you're his third ever customer--and arrange an informal deal to have them automate your accounting
   for significantly less expense and time than if you were to go through the motions and paperwork yourself. These 
   abstractions exist far beyond legal and accounting: there are obscure but powerful Python packages you might only hear about 
   from someone in the Bay Area who made them, or a novel technique for rendering more realistic shadows in paintings 
   invented by your neighbour when you're living in 15th century Florence that painters in Rome or Milan competing with you 
   simply do not know exists. <br><br>
   This access to abstractions, coupled with the availability of 
   talent & capital, high risk tolerance of experts in the area, and general atmosphere of "it's okay to fail" that is pervasive as much in 
   Hollywood as in Shenzhen make it very easy for talented employees at big companies or ambitious graduates of top universities--both of which are 
   present in copious amounts in hubs--to leave and recruit some friends to start the next big thing, whether that's making <i>Rocky</i>,  
   writing <i>A Midsummer Night's Dream</i>, or founding Tencent. <br><br>
    <li><i>Process Knowledge </i></li>I actually think this is the most important reason clusters succeed. 
    Process knowledge is the knowledge taken for granted by experts, and the type of knowledge that can't be written down. 
    If you had a master chef write out a recipe in painstaking detail, giving ample detail on exactly how to do everything, 
    would an amateur chef's output using that recipe be just as good as that of a Michelin-starred virtuoso? No, and that's because 
    of process knowledge. Dan Wang wrote an excellent piece on it <a href="https://danwang.co/definite-optimism-as-human-capital/">here. </a>
    <br><br>In another context: in Silicon Valley cafÃ©s, in casual conversation, you never have to explain was "SaaS" or "B2B" means, it's 
    as much part of the vernacular as "lmao" is to teenagers on their phones. Before I moved to the Valley, people told me the Valley 
    was ahead because it's easier to source capital and talent there than it is anywhere else. This may be true, but I don't think that's the 
    main reason it's a better place to start a high-tech company than anywhere else. I think the real reason is that starting new ventures--
    whether it's a company, artistic movement, religion, hedge fund, or film idea--is difficult. In the case of SV, if you're starting a payments 
    company, and Max Levchin lives down the road and is willing to grab coffee to talk about the problems you're having, 
    you'll have your questions about, say, early user acquisition in payments, answered by perhaps one of the only 10 people in the world equipped to answer them.
    A pow-wow over coffee or drinks with movers and shakers that have struggled through that which you're struggling reveals insights and 
    paths that just are not possible to get when you're teleconferencing from the comfort of your Colorado home.
    <br><br>This sense of informality, the ability to move fast and break things, is something you can never get through "formal partnerships" of the kind 
    mediated through email and contracts and video call. This is related to my earlier point about low barriers to entry, as well as to my 
    later point about agglomeration economies. 
        <br><br>
    <li><i>Peer Pressure </i></li>This isn't very complicated. To be great at something, you must live, eat, breathe and shit it.
    When you see the guy next door's lights on at 2am and you know he's working on the script for his movie while you just got back from doing coke 
    at a party, you can't help but feel bad and become more disciplined as a result.  <br><br>
    <li><i>Agglomeration and Economies of Scale: </i></li>By being in a cluster, small startups and creatives can reap some benefits as if 
    they were huge companies without sacrificing the agility that makes them able to be so innovative in the first place. They can 
    have their cake, and eat it too. Startups need to do less training and in-house education of their own when the big companies next door 
    do it for them; the startups just need to poach employees, which they often have no trouble doing. Another example is easier access to talent because 
    smart people reduce risk by choosing to work in a cluster: if they lose their job, it'll be easy enough to find another one. Such is not the 
    case if you're a biochemistry PhD laid off from your pharma research job in Anchorage, Alaska. Startups also have access to 
    cheaper inputs more easily obtainable because suppliers are competing for market share in a huge market. If they want to try 
    something new, materials and expertise can be on their doorstep the next day, significantly reducing the barrier to experimentation.<br><br>
    <li><i>Sophisticated Customers </i></li>Markets for products in clusters are often both more demanding and tolerant than markets for the 
    same thing elsewhere. As with the example of an accounting automation startup finding its first customer in another fledgling startup 
    located next door in San Francisco, is the case with Los Angeles movie-goers being more willing to experiment and explore unconventional 
    and potentially disruptive new media formats, and with New Yorkers being more educated about personal finance than the average American, and 
    being more willing to put their money into new and innovative instruments. The customers in a cluster have preferences that are often 
    5-10 years ahead of the world at large, such that often listening to the pulse of consumer demand in a cluster can tell you the future direction 
    of your industry: a remarkable advantage, and certainly one that cannot be "digitalized" in any meaningful way. 
    <br><br>
    <li><i>Public Investment </i></li>Clusters have large masses of people, and are therefore powerful political forces. 
    And since clusters need a functioning ecosystem to survive (a tourism cluster can't survive without functioning public 
    transport), governments are happy to support them in any way possible, since clusters often contribute a disproportionate 
    amount to national economies. Startups also benefit from workers educated on state money in the case of public universities, like 
    Berkeley in Silicon Valley or UCLA in Los Angeles. It is also telling that many developing economies don't have clusters because 
    they lack government support or have governments that actively work against them. 
    <br><br>
    </ul>
<p >
  To conclude, I think clusters are one of humanity's most powerful engines for progress. As globalization further increases, 
  I think their importance will only grow--social media led to more college parties, not less, and teleconfering compounded the 
  importance of in-person meetings instead of obviating them. Information technology makes the world more dynamic and 
  knowledge/service based, which in turns gives outsized advantages to those that can easily identify and adapt to new trends and 
  access 'insider' knowledge. It's a positive feedback loop. I'd welcome suggestion on how 
  both legislation and technology can make large cities in developing and developed countries alike potential breeding grounds for 
  clusters. 
  <br><br>
<a href="#top">Back To Top</a>
<br /><br />
</p>
</div>

<button class="accordion"><h3>Cityscapes</h3></button>
<div class="panel">
  <p>

   Tentative thoughts on cities I've lived in or visited multiple times. <br>
   [TODO]<br>
  <li><i>Cleveland, OH</i></li>Quintessential American suburbia. Sprawling yet cozy, uneventful 
    without a small-town feel. Contented.<br><br>
  <li><i>New York City, NY</i></li>Something for everyone. Brash, no-bullshit attitude in stark contrast 
  to west-coast political-correctness. Absolute cultural melÃ©e, and fully worth its reputation. <br><br>
  <li><i>Abu Dhabi, UAE</i></li>City built from sand dunes, great for children and families 
  but lacking cultural, experiential and intellectual diversity due to its youth. <br><br>
  <li><i>London, UK</i></li>Perfect. Innovative, but with room for good banter. People take 
  their work seriously, but don't take themselves too seriously. History and character. <br><br>
  <li><i>Delhi, India</i></li>Slow, lazy, yet immensely hectic in an iconically Indian way. 
  Enormous estates juxtapose mothers begging for starving children. People think in days, not years. <br><br>
  <li><i>San Francisco, CA</i></li>On paper, utopic. In reality, not quite. Forward-thinking, 
  open-minded people, fast moving culture. Closest to a meritocracy I've seen any city get, 
  but with a subtle lack of intellectual vitality and a unique strain of Silicon Valley pretentiousness. <br><br>
  <li><i>Tokyo, Japan</i></li>Gigantic, and utterly, utterly civilised. People value community and 
  courtesy without the groupthink of Chinese cities. Meticulous 
  attention to detail, but sadly culturally homogenous.<br><br>
  <li><i>Dubai, UAE</i></li>A more intense, hedonistic, artificial version of Abu Dhabi. 
  I really respect the government's effort to divserify economy into tourism. Beautiful, but lifeless. <br><br>
  <li><i>Mumbai, India</i></li>A more intense, dense version of Delhi, its people chase fortune rather 
  than let it come to them. Even the crime is more extreme. Wall Street and Hollywood meet 
  chai and auto-rickshaws. <br><br>
  <li><i>Boston, MA</i></li>TBD<br><br>
<a href="#top">Back To Top</a>
<br /><br />
</p>
</div>

<button class="accordion"><h3>War and Business as Vectors For Innovation</h3></button>
<div class="panel">
  <p>

    It's interesting to note the similarity between the type of relentless resourcefulness important in war, and in business. 
The reason the French Republic got enough artillery
to fire at the allies at Toulon was because Napoleon personally pulled strings to get more supplies, 
had new forges build on-site,
trained people to themselves train troops on using artillery. 
An example of the circumstance/skill dichotomy is that the geography
of Toulon and the layout of the Allies meant that any fighting was done by artillery, which is what Napoleon 
happened to be trained in (right person at right place at right time). <br><br>
And on the importance of implementation/distribution channelsâI would argue that profits are to 
modern business what war was to nations: mechanisms for
propagating/disseminating some radical innovation. Something as revolutionary as the French Republic and its progressive ideals
could easily have been stamped out at various points in its infancy, changing the face of the world forever 
(setting us back 50-100 years!).
It succeeded not because of the "correctness" of its ideals, in any absolute sense 
(ideals which philosophers had come up with 
long before they were enacted, just
like Xerox came up with the graphical computer interface long before it was implemented/distributed by Apple), 
but because of the enterprising and
relentlessness of a few men (like Napoleon) in changing the incumbent landscape enough to allow for introduction,
distribution and implementation of these radical new ideas that had been fully fleshed out in theory. 
<br><br>Importantly, this was done from <i>within</i> the
existing infrastructure (fighting wars with other nations that opposed Republicanism). To spread successfully, the new innovation (Republicanism) 
did not just have to be a superior method of running a state, but <i>also</i> had to be better at winning wars. 
I see this fragility at birth to also be present in businessesâevery household, 
office, and more, might look different if Jobs hadnât
recognized the potential of the GUI at PARC that one day he visited; just as how the whole worldâs methods of government
may still be leaning aristocratic, if Napoleon hadnât been enterprising enough to
requisition extra Artillery at Toulon or food for his soldiers at the bridge of Lodi (victories which led directly to him gaining power to
implement civic innovations like his Code Napoleon).<br><br>
You also see that attempting to scale any major innovation (GUI, Haber process, batillon carrÃ©)
requires lots of related, ancillary innovations <i>as well as a receptive landscape</i>âthink Apple's redefinition of 
technology in retail via Apple Stores, Carl Bosch at
Oppau/Leuna practically inventing the fields of high-pressure and catalytic chemistry, countless small but crucial 
bills passed under Napoleon. Itâs a philosophical question of which you believe to be higher
impact workâthe invention of a concept (Wozniak, Haber) or its distribution/enactment at scale (Jobs, Bosch).
An important caveat when weighing the two is that we usually know very well who 
was first to implement an innovation at scale, but the question of who was first to
conceive with the theory is usually murky and debatable.<br><br>
In short, I've changed my opinion on what makes a startup succeed (and its competitors fail). A year ago, 
I'd tell you it was the novelty of the idea, insightfulness of the founders, and receptiveness 
of the geography and market at the time. Now, I think that successful founders are measured by very different 
yardsticks. Their test isn't how novel or unique their idea is, their test isn't how insightful or intelligent they are, 
and their test isn't whether the market or geography is ready for their cool new idea at that time. They are, instead, 
testing every hour of every day. They are tested when they hire their first employee--are 
they charismatic and inspirational enough to motivate 
and persuade a phenomenal engineer to risk it all to come work for them? 
They are tested when they are running out of 
money, and don't know any VCs--can they find a way to get a warm intro to a VC or dazzle investors through some 
creative stunt? They are tested when their initial customers tell them their product is shit--do they know when 
to persevere, and when to quit? Do they make the right decision at that point, and at every other pivotal moment that 
marks the early days of starting a company (or anything, for that matter)? <br><br>In this sense, I no longer see 
startups as outlets for scientists to out-innovate each other, but what they are: businesses. 
And the ability to run a startup is just a long way of saying you know how to do business. In other words, you're 
a good businessman. <br><br> 
This is an important realization for me because this word, "businessman", 
in particular, was one that would have had me see someone as "braindead"
when first meeting them. I think many scientists/engineers share this sentiment. But now I find that 99% of what comprises 
entrepreneurial success is, unsurprisingly, entrepreneurial qualities. It seems trivially obvious in hindsight, 
but from what I've seen in the Bay Area, most technical founders, even those aged 30 and older, 
do not viscerally understand this until after their first or second company fails.
<br><br>
In other words, I see now that innovation is not the cost of startup success, but the reward. Now that Facebook has tens of 
billions in cash reserves, it can pump money into moonshots like Oculus and CTRL-Labs knowing that it can pay for all that research and 
afford those risks, to a degree that even the biggest universities cannot. I believe Google outputs more computer science research 
than Stanford and Berkeley combined. Just ONE of Google's moonshot projects, Waymo, has been given more R&D 
funding in one year ($3B) than MIT gets for ALL its departments in several years ($1B/y).
That is what Sergey Brin and Larry Page feel proud for 
when they put the kettle on to boil every morning of every day. <i>That</i>, is high leverage work. 
<br><br>
The requirements for getting to a point where the company is well enough endowed to innovate at all 
are simple to define: be an anomalously resourceful, charismatic person who can identify what is essential and 
what isn't, and act decisively and correctly in very high pressure, resource-constrained, systems. 
These are not necessarily, or indeed at all, the traits associated with "innovation" or "science", which 
are more to do with the creation of knowledge and tools, and require very different skill sets. 
Instead, these traits describe a good businessman. And, I would argue, also describe the legendary 
soldier-statesmen of eons past. Jobs, Gates and Caesar, Napoleon have more in common than it might seem. <br><br>
Put differently, business is modern warfare, and innovation is the prize.


<br /><br />
<a href="#top">Back To Top</a>
<br /><br />
</p>
</div>

<button class="accordion"><h3>Gap Year Learnings (1 of 2)</h3></button>
<div class="panel">
  <p>
  <ul>
  <strong><i>On Silicon Valley, Startups, and Software Engineering</strong></i>
  <li>Operations, partnerships, distribution channels, and other âbusinessâ buzzwords are just as important to understand as the actual theory behind your ârevolutionaryâ tech innovation. 6 months ago, Iâd dismiss anyone using these words as a braindead MBA monkey, but it turns out most companies and products have been built before, itâs just that new ones have an edge when it comes to partnerships, distribution channels, talent, or something else of the like. Practical implementation and distribution of a cool product is the bottleneck for great ideas more often than the technical innovation, which is a sad reality.</li><br />
  <li>The most money isnât necessarily made in consumer products, even though thatâs all we college students have ever really been exposed to. Everyone I know, my age, is thinking about how theyâd make the next SnapChat or TikTok. The really successful founders and Valley veterans are instead thinking about the niche, unsexy enterprise problems that generate the most value for businesses.<br />
  <li>Being able to integrate your product with existing infrastructure and not screw over important people in the supply chain, at any point, is crucial. Itâs very important to have respect for why the problem hasnât been solved so far. If you canât conjure a compelling answer to that, you havenât looked hard enough. For example, the invention of DVR by TiVo was revolutionary, but they didnât own the content distribution channels (cable TV) that their product would be used on, Comcast did. And so they failed, despite their breakthrough technology.</li><br />
  <li>A lot of the work involved in software engineering sounds complicated and world-changing, but itâs pretty mindless. Much is simply rebuilding or refactoring pedestrian products that already exist in some flavor or capacity, but with a small new change or addition to address some novel market need. For example, companies will invest thousands of engineering hours into copying each othersâ products from scratch because no open-source equivalent exists. The work can be technically challenging, absolutely, but much of it is not creating anything fundamentally new in the universe. Front-end engineers often spend time working to make a certain animation marginally more memory-efficient or rearranging how the edges of a certain box look â quite a far cry from studying circuit architecture and red-black trees at college.</li><br />
  <li>Most brilliant people in the valley are excellent at solving problems but spend almost no time thinking about what problem they are solving, why theyâre solving, the impact itâs likely to have, and how profitable itâs really likely to be. They just get giddy about solving something difficult. In math speak, I believe that our species has come to be very good at optimizing narrow objective functions without having any meaningful understanding of how these narrow functions relate to comprise a wider, global solution to our problems.</li><br />
  <li>Technology is almost never the inherent solution to a problem. It is just the vector through which the solution can be implemented cheaply at scale and in a clean, simple-to-use way. Uberâs breakthrough was with its business model, implemented through some neat software. Building the app was not a breakthrough. Being a 10x engineer doesnât really give you any meaningful advantage in coming up with clever solutions, merely with implementing them. Being a technical co-founder is only useful insofar as it lets you prototype and test ideas, as well as know the limitations of the technology youâre using to build solutions (which is still very useful, I might add).</li><br />
  <li>The importance of projects is often under-appreciated. Most companies, research ideas, and works of art and literature and music came from great people tinkering and playing with things in their free time. The ability and desire to build web apps on a whim, write pieces of short fiction because of a dream, or investigate a new research topic over the summer because of a book you read, are vastly important. Tinkering and building should become routine and ubiquitous for our world to embed innovation into our culture.</li><br />
  <strong><i>On Living in, and Understanding How âThe Real Worldâ Works</strong></i><br />
  <li>Iâve come to appreciate the importance of legislation and individual behavior in enacting macroscopic change. As a techno-optimist who started the year believing that more money and talent would lead to endless innovation that would solve all of humanityâs problems. By talking to academics, entrepreneurs, and reading widely about various industries, Iâm now pretty convinced that a lot of our biggest problems (climate, public education) are best solved by informed electorates who act the right way and elect responsible and intelligent leaders rather than brute force investment into technology.</li><br />
  <li>An ancillary point is that Iâve come to appreciate the complexity, beauty, and efficiency of the institutions of modernity, like democratic governments, international law, financial services, energy generation, waste management, and more world-changing machinery that would hum around under the hood of daily life, hitherto unnoticed. Everything from how VisaNet allows for instant electronic card payment between banks to how landfill pipes capture methane from trash, Iâve understood that there have been a lot of smart people who have experimented a lot of times and gone through a lot of pain and effort to come up with many of the tools and institutions we donât even know exist.</li>
  <li>Iâve come to see the understated difference between how large, bureaucratic institutions (government/finance) are ideally supposed to work and the twisted, complicated, corrupt and underhanded way in which they often do. This might seem to contradict my previous point, but I think that itâs consistent to hold the beliefs that democracies are beautiful, contrived and deliberate structures that have done a lot of good, but that they also creak and crack in places due to the inherent weakness of individual humans.</li>
  <li>College is only useful because of the people you meet â both peers and professors. Even vocational majors like mechanical engineering have limited utility in the real world, even if youâre working as a full-time mechanical engineer. The rat race to join prestigious clubs and groups and be atop the social ladder is almost laughable when you look at it as an outsider and realize how useless those races really are (including chasing grades).</li><br />
  <strong><i>General Musings</strong></i>
  <li>Reading widely and often is not a foolproof way to become an intellectual, it makes little sense to read blindly for the sake of reading. I donât know why so many people focus on chasing ânumber of books per yearâ rather than deeply internalizing new concepts through whichever medium can best teach them about those.</li><br />
  <li>Learning languages has little to no professional utility, but the way it deepens and enriches relationships between you and people who speak that language gives you uniquely rich insight into the way they live and think. This could be gratifying and enriching for you on a personal level and is the only good reason to learn a new language in my opinion. As a trilingual, I do not think this is generally a good investment in time.</li><br />
  <li>Conspiracy theories and stereotypes are often grounded in some truth. I heard that some teams on Google and Twitter had doctored some search results leading up to the election to sway voters one way, and immediately dismissed it as fiction. Then I met some people who had a background in the industry who revealed that elements of it were not too far off.</li><br />
  <li>There are no deadlines in the real world, and life after college can easily lack a sense of urgency. You donât meet people unless you go out of your way to convert casual interactions into meaningful relationships, and this is disconcerting and deeply uncomfortable at the beginning. It is easy to fall into a routine where youâre losing awareness of where youâre going in life since there are no clear deadlines or people to hold you to them when youâre in the real world.</li><br />
  <li>Great art, film, literature, and music do have an important place in society. For all the money, ideas, and opportunity gushing into Silicon Valley, people, including techies themselves, hate living here. With collapsing civil infrastructure and almost nothing to do outside of work and good ethnic food, thereâs a palpable difference between San Francisco and cities like NYC, London, Paris, Tokyo. Even if you donât go to art exhibitions or attend rock concerts, the fact that they happen all around you affects the pulse of the city in a subtle but noticeable way, and the lack thereof makes life just a little less vibrant as if people go home to take a short break before going back to work again, instead of the other way around.</li><br />
  <li>Iâve become disillusioned with top universities. Coming into the year, I would instinctively associate anyone with a brand-name school on their resume as world-class. Very quickly, I saw how most students at top schools are a product of circumstance, and only about 5% of the population, even at Harvard, is âeliteâ by Silicon Valley standards (in terms of intelligence and output). Similarly, most of the Valley âeliteâ did not attend Harvard (or Stanford or MIT and so forth). I see these credentials now as signifying moderate intelligence and good fortune and look to gauge whether people are smart by actually talking to them about ideas and looking at projects theyâve worked on.</li><br />
</ul>
<a href="#top">Back To Top</a>
<br /><br />
</p>
</div>

<button class="accordion"><h3>Gap Year Learnings (2 of 2)</h3></button>
<div class="panel">
  <p>
    <ul>
  <strong><i>On Silicon Valley, Startups, and Software Engineering</strong></i>
  <li>
    I spent a big chunk of my gap year studying the future of various industries to see if I'd like to work in any.
    A potential reason this approach is suboptimal: 
    it assumes you're motivated purely by a desire to enact the greatest good in the world. Studying across industries reveals 
    high leverage, important, work, and makes you question why you still don't want to do it. For example, after studying three industries, I think 
    working in any of climate or education or developer tools is really impactful, important work. But I still don't plan on doing it. That's because I, like almost everyone, 
    am not purely motivated by the leverage of the work. I also care about how intellectually interesting it is, how much beauracratic bullshit I'll have to wade through, 
    what kind of people work in the industry, and more. I'm lucky that doing this industrial survey has made me explicitly confront these difficult truths, because now 
    I have a lot more clarity in what I <i>am </i>looking for in the immediate future. 
  </li><br />
  <li>
    Similarly, I understand <i>viscerally </i>that successful companies are not novel ideas or problems, but old ideas and problem executed flawlessly, or in a novel way. 
    The bottleneck for startup growth in many of these industries is not product, but industry connections, and I donât 
    have any. How can I expect my execution to be better? More importantly, I've found that the initial problems and ideas you spot are not what will form the basis of
    your company. The problem that you end up finding product-market fit is a result of deeply understanding the customer, not the one you spotted initially. And therefore any
     problem list I compile is merely a list of starting points, each one having to be explored through industry contacts for months to see if thereâs anything there at all. In 
     VC-speak, Iâm not the right founder for almost all those problems, and thatâs really what makes companies succeed. 
  </li><br />
  
  <li>
        Most people working on startups are smart people that want to get rich, not because they have any "passion" for the problem.
        People are not "passionate" about enterprise software or dating apps. They are, however, passionate about making bank.
        And there's nothing wrong with that. These smart people do this by creating a lot of value for 
        customers fast (they donât really care which customers). And this is fine, too. 
  </li><br />
  
  <li>

    I finally understand where the culture of studying CS to become a tech entrepreneur came from. I think that today, with most web and mobile software, 
    CS courses are not very useful as most of what you learn is completely unrelated to the programming work you do. Many people (who have done CS degrees), 
    counter that it teaches you "general principles" that help you pick up more specific, concrete frameworks and trends faster. I have been generally unconvinced.
    But I now see that this culture is just a remnant of the past. CS really <i> was </i>important for early 2000s and 1990s founders 
    because all the abstractions that people build atop of today, 
    like web frameworks, mobile SDKs, had to be invented from the operating system and physical network level up. For exampe, Paul Graham's Viaweb had to pioneer 
    the act of sending information via HTTP to a remote server, and that required knowledge of servers, operating systems, as well as their customersâ pain points. 
    The literal idea of a web application did not exist when it was invented (tautologically), and so for the inventors, they had to build it from the fundamentals of 
    computing. This is why they actually needed to know computer science, and, similarly, we don't today (if you're building pedestrian web/mobile software, which most 
    Silicon Valley startups are).

  </li><br />
  
  <li>
    Many people in the valley are very well read, and thoughtful about their opinions in a scholarly way. 
  Perhaps my fierce opposition to the can-do & move-fast-and-break-things culture of the Valley may arise more from general shock of how quickly things can get done 
  in the Bay Area than anything else. 
  </li><br />
  
  <li>
  I always thought people pursuing startups for the sake of startups fail, and that some poetic justice in the world would mean that those who succeed had a 
  genuine love for the problem they were trying to solve. This is just fiction. Almost all founders are mercenary, and there's nothing wrong with that. 
  Information technology has meant that smart, determined people can almost-predictably get rich if they try enough times, and I can't, and shouldn't, blame people 
  for trying to get-in on this new status quo. The repeat founders are in fact theyâre the most likely to succeed, because they persistently are looking for problems people want solved, 
  and are trying to solve them across diverse fields, learning tons in the process. Given that most of the difficulty in making a company succeed is actually to do with 
  distribution and business, not product, this makes sense, even if it isn't the way I wished it was. 
  </li><br />
  
  
  
  
  <strong><i>General Musings</strong></i>
  <li>
    An important (and scary) one: to be remembered for creating, discovering or inventing anything outside of academia, 
    you have to own distribution to get credit. Philo Farnsworth invented the television, have you heard of him? No, because he was not part of the company that ended up distributing it at scale, 
    and so there was no way for people to know that, he, in fact, was the one that invented it. Whereas with Apple, there were teams of PR people using various channels to 
    tout how innovative their engineers (and Steve Jobs) were in coming up with the iPod, or iPhone. People don't famous if no-one talks about their work. Anytime you 
    associate an invention with someone (another example: Haber, Bosch with the fixation of nitrogen into Ammonia), it's because their company worked hard to have 
    you make that association (as BASF, the chemical company they worked at, did). In the real world, sales, marketing and PR <i>matter</i>. Often much more than 
    quality of product and technical ability. 
  </li><br />

  <!-- <li>
    When making important decisions, I would always think about doing things that I wouldâve done âanywayâ. I'll explain what I mean.
    For example, when debating whether or not to take a gap year having committed to Harvard, I thought about whether I would've taken a gap year if I'd, in 
    a parallel universe, gone to MIT. My answer was probably not--the things I wanted to pursue during the year were in part to make up for some things 
    I felt I wouldn't get enough of at Harvard (eg deep software immersion). At the time, I thought this an argument AGAINST taking a gap year--as if 
    the fact that I would not have taken it if things had gone differently was a reason not to take it given things went the way they did. In other words, 
    I believed that there was some "absolute" life I'd hypothetically live--and if I didn't make a decision in that life, I shouldn't make it in this one because 
    it "isn't something I really want". Now, I see that instead of being a reason NOT to take a gap year, it was a reason TO do so. You must act to optimise the 
    situation as is, as opposed to as it might hypothetically have been. 
  </li><br /> -->

    <li>
    I sometimes feel bad about being interested in startups because if I was born 50 years ago, I might have aspired to be an executive at a big company, 
    or 500 years ago, a painter. I kept telling myself I should do something I would've done regardless of when I was born, as that is what 
    you "truly" want to do but now I see there is no such thing. 
    One has certain overarching philosophical goals in life (wealth, or posterity, or interesting experiences, or fame, etc), and these are the invariants. 
      The way you go about working towards those will <i>of course </i>vary with when and where you're born. I'm only now learning that instead of 
      shirking away from this reality, accept it as the natural state of things. The methods you should use to get to those invariants vary a lot with time (founding startups was not the highest way to exert leverage 50 or 100 
    years ago, and may not be in 50 or 100 years time). And you should act appropriately. Itâs not âwrongâ or âmimeticâ, but sensible. 
  </li><br />

      <li>
        You can broadly classify all great people into two bins: creatives and businessmen. 
        Einstein, Bach, Michelangelo, Plato, all fall into the former. Caesar, Napoleon, Churchill, Rockefeller, all fall into the latter. Jobs and 
        Edison are are more ambiguous. I use the word "businessman" to describe anyone whose
        success depends more on external events and their ability to navigate them. Creative types are more 
        at war with their own intellect, constantly asked the question of: are you inventive enough to spot that breakthrough idea? Businessman are more 
        at war with external circumstances: can you motivate 
        employees not to go on strike, can you secure more military funding as your army is about to fall apart? 
        Once you put it this way, you have to ask yourself: 
        am I good enough to produce as much as the Renaissance masters or Greek philosophers of antiquity? If not, the future path is obvious: work hard at becoming great at 
        selling things, motivating people, negotiating, envisioning the future, more than working hard at any one skill or craft like a creative might.
  </li><br />



</ul>
<a href="#top">Back To Top</a>
<br /><br />
</p>
</div>

<!-- how economists think things are irrational because they don't understand constraints (evidenced by poor history of research -> policy)
  re: management consultancy -->

<button class="accordion"><h3>Great Expectations</h3></button>
<div class="panel">
  <p>
<br>
<i>What makes great institutions great?</i> This refers to high schools,
universities, big-corporates, sports teams, and even, say, royal dynasties. But the focus will be on educational institutions, informed,
inevitably, by my experience at Westminster and the things I hear when talking to friends currently at Harvard. <i>But the
  principles I derive apply to the Golden State Warriors as much as they do to MIT;
  to the Navy SEALs and Phillips Exeter Academy, in equal measure.</i>
<br /><br />
Let me be explicitly clear; there are a few things that <strong>do not</strong> confer greatness upon an institution of learning. Wealth, for starters.
Quality of facilities--of faculty, even. Connections. Brand name. All of these help, sure, but only a little bit.
In my opinion, they don't really address the heart of what makes these legendary institutions capable of so routinely
pumping out visionaries--the kind of people who are unafraid to mould history with their bare, gritty hands. It comes down to three things: <strong>peer-teaching,
recalibrating expectations, and a sense of entitlement.</strong>
<br /><br />
Let's start with the first one. Groggy, at 1am one night snuggled up in your memory-foam mattress. You find yourself facing unblinking reflection
on the laptop screen, YouTube tractoring on in the background. You're half-awake, blinking unthinkingly,
watching "What do Harvard students like MOST about Harvard"?
You hear one phrase, again and again. And then some more, pummelled mercilessly until stripped of its meaning. "The people".
<br /><br />
What exactly do they mean by this?
<br /><br />
I don't even think the Harvard students really know the answer to that. And that is that the most important learning that happens isn't the kind that happens in class or the lecture hall. That's a mere formality.
A sham, if you will. At Harvard, the real learning happens in dorm rooms at 3am, when casual midnight chit-chat about the protests
in Hong Kong might spiral into a lengthy argument with your roomate--who happens to be the leader of the 2014 Hong Kong student protests.
The real learning happens when you're sitting down for lunch with a new girl you just met, exchanging
pleasantries and small-talk, only to have to spend a few seconds after the conversation scanning the floor in silent ignominy
 after finding out that she cured a rare type of pancreatic cancer in high school. You learn about the economics of oncology--
something you had, quite literally, never spent a second thinking about--over budget meatballs and staling rice.
<br /><br />
These casual conversations add up. Years after you graduate, you forget how to derive Maxwell's equations and the
characteristic themes of colonial literature. But you don't forget those meandering chats that left you stunned,
and a little wiser. You collect and cherish these back-and-forths--no; hoarde them, as if they're rare gems glittering in the dust of your memory.
You are right to do so, for these are the breakfast debates and seaside reflections that laid the seams for the person
you grew up to become.
<br /><br />
But I think there's more to peer-education. Just as time relentlessly bites at your memory of Euclid's algorithm or
understanding of consumer surplus, it eats away at the details that made those casual conversations so magical in the first place, too.
That's alright, though, because the peer-education wasn't the most important part of your interactions with other students. And this gets
at the core of what I believe <i>really</i> makes institutions great. They recalibrate your expectations. They take what you think and know and
believe to be "normal" and "usual" and they beat it to the ground. <i></i>They bruise and batter and dismantle and rewire, bit by bit,
your expectations.<i></i> By this, I mean that <strong>they make the sensational seem pedestrian.</strong> This is how top schools and universities become
crucibles of greatness, furnaces that take bronze, smelt it and break it down, and forge it into something shinier and stronger.
If Joshua Wang or David Hogg is your roommate, activism isn't so unique anymore. If you've seen Yo-Yo Ma
drunk at a frat party, he's not a "prodigious musician" to you anymore, just another goofy friend. If you go out on a date with
Malia Obama, celebrity appearances mean a little less to you. If the Fields-Medallist professor tries teaching abstract algebra
using memes, legendary academic prizes no longer seem that esoteric.
<br /><br />
And I've felt this myself. At school in Abu Dhabi, if someone in the country--much less my high school--gets into Oxford or Stanford,
it's instantly national news. Over time, I pattern-matched these students that won international olympiads and started nonprofits in
high school as "prodigies". I put them in a bin in my head as genetically gifted, on a level I'd never reach no matter how hard I tried.
<br /><br />
And then, at Westminster, I saw them dancing at clubs and smoking in the park, groggy at breakfast and drifting
asleep in lessons.
<br /><br />
Very quickly, I came to realise the secret. There are no "prodigies" and
no-one is "genetically gifted". These boys and girls were humans, as much as I.
Humans that broke into greatness through a combination of enormous ambition, unwavering discipline, and
fortunate circumstance.
<br /><br />
In Abu Dhabi, being cross-admitted to Harvard and Trinity College, Cambridge, was a fantastical dream--one that
I wouldn't have had the audacity to remotely entertain, even when alone at night, tossing and
turning in bed. At Westminster, there's someone who does it every year (the guy in the grade
above me with the same choice is a friend of mine, now also at Harvard). A step further: if you win a Rhodes Scholarship
in Kansas, you've made history. You're instantly a local celebrity, the governer comes to your house to congratulate you. At Harvard? You get a copy-and-pasted
email congratulating you--and of course, the other 6 that also won the scholarship (that year alone). I'm convinced
it is <i>this</i> realisation being forcibly thrust upon students that makes them great. Understanding this fact
means that you come to realise that there's really no reason that <i>you</i> can't be great, and that there's nothing special about those that are.
<br /><br />
And finally, members of elite institutions so often do great things because they are arrogant and entitled enough to believe they can.
This is a corollary to the reason above, but is different enough for me to need to explain it as a separate point. When you go to
Princeton or Eton, or work at McKinsey or Google, society instantly--incessantly--brands you as "elite".
I don't think most people at these institutions inherently think they are any better than anyone else,
but time and time again, people you meet will gasp in awe, mumble in jealousy, be stunned with shock, and tell you that you're now
destined for greatness--that there's only so low you can go in life now, unthinkingly and
repeatedly showering you with profligate amounts of praise. <br /><br />
At first it's flattering. Gratifying, even. But over time,
when family, friends, old class-mates, ex-girlfriends, and all of society is telling you that you are part of the
hand-picked 0.01% that will change the world, it becomes easier and easier to believe them. And since "the people who
are crazy enough to think they can change the world are the ones who do", this is a positive feedback loop--a vicious cycle. <i><strong>You don't
get that round of seed funding from Sequioa just because your startup is that good, it's also because they saw how convinced you were
that you will succeed when you pitched to them, and liked your conviction. You don't get preferentially hired by Goldman because
you're actually much smarter than the guy who went to Ohio State, but because you come across as so self-assured--bordering on
arrogant--and entitled to success that they know you'll find a way to get it.</strong></i>
<br /><br />
All three of these reasons, together, come together to put elite institutions a hair above merely "excellent" ones. I'm confident that this is as
true of the US Women's Soccer Team as it is of competitive Russian programmers and Korean e-sports whizzes. Look carefully,
and you'll spot these factors at play everywhere, acting only to perpetuate inequality by helping
one generation of winners hand-pick the next generation of winners in society. <i>So the next time that
people tell you that Cambridge or Facebook is an elite organisation because of its wealth and resources, or even the "network" it provides,
know that they are letting on only a fraction of the real story.</i>
<br>
<br>
<a href="#top">Back To Top</a>
<br /><br />
</p>
</div>
<!-- 
<button class="accordion"><h3>What I'm Doing During My Gap Year</h3></button>
<div class="panel">
  <p>
<br>
I opted to take a gap year only weeks before being supposed to start at Harvard. Laughing, always, listening to my father first suggest the idea, and explain how it could
help me "see the world through a different lens" made me cringe. Mulling over it, though, I did learn and grow the most when left to my own devices;
when I was given the autonomy to make decisions about what <i>I thought </i>I should learn or try, and how I should learn it. And when asking myself why I wasn't taking a gap year
in the first place, the only compelling answer I could conjure up was the "it wasn't the beaten path". Not very convincing.
<br><br>
Before I knew it, I was curled up in my bed like a hot dog in a big ol' bun, furiously shaking the last few drops of ink out of a dark rgb(38, 150, 255) ball point, biting at it's
top as I grappled with what I'd make of the year ahead. I felt like an artist lost in thought, paralysed the myriad stories he might tell
through the medium of his starchy white canvas. A good starting point was to see what others did during their gap years.
<br><br>I was disappointed.
<br><br>
Most people did tokenist volunteer work--the kind that feels like it's life-changing while you're doing it but 2 weeks after you get back to your
comfy home in a developed country, is little more than a resume bullet point, leaving the very people you were "helping" just as
powerless as before you visited them. Volunteering? Cross that one off the list. Others would travel, experience culture, and learn languages. At this point,
I was fluent in 3 languages, had grown up living on almost every continent on Earth. I didn't think learning languages, in general,
was a great time investment. People do business in English around the world, and locals in many countries won't take you seriously
because they can tell you're a foreigner no matter how well you speak their language. To them, it seems cute, at best, and is utterly useless, at worst.
<br><br>
Others take gap years to work, to make some money, or to "take a breather". Many people who work during their gap years come from backgrounds where they
genuinely need to contribute to help support their families and pay for college. I admire and deeply respect those people.
Many others, however, don't think very critically about which job they're doing, and end up learning very little, as well as earning very little.
They would've been far better off adding 1 more year of work <i> after </i> a college degree. And as for those "taking a breather", I'm sure they have their reasons,
but I didn't want my year to take away from the early intellectual momentum I'd built up in the last few years of high school.
I wanted it to be my most intense year yet. I concluded that around 90% of people who take a gap year would've been better off without one.
But for the 10% that make full, deliberate use of every day of that year, and the flexibility that comes with it? It changes their life. With the most common things to
do during a gap year off the table, <i>what would I actually do, though</i>?
<br><br>
Searchingly reflectng on the years after college gave me my answer. Did I want to be a surgeon, like my father?
Perhaps work on Wall Street, living a comfortable and "prestigious" life, like so many others that walked the Harvard path. Or
parrot out condescensions towards those that "sold themselves out", convincing myself that I would do something more "meaningful"?
<br><br>
Daydreaming, I thought back to when I was young. As a little person, I always thought that society would hand-pick the best, the brightest,
and pit them against humanity's biggest challenges. If you were "special" enough to attend an elite school, you had greatness
stamped onto your destiny; you'd give up your life tackling climate change, curing cancer, exploring Mars, all for
the betterment of your species, right? Wrong. As I grew up,
<i>I witnessed the best and brightest flock to optimise ads for a search engine, or spend 80 hours a week mindlessly filling in Excel
  spreadsheets for their bulge-bracket employer.</i>
<br><br>
But it didn't have to be that way. If I wanted to live in that world, I better help build it. And that meant that if I aspired to be
part of this esoteric "best and brightest", I had a duty to put my money where my mouth was, and pit
myself against the world's most pressing challenges.<br><br>
But what were they?
<br><br>
Stepping back, I meandered through industries, problems, themes that would be indisputably important in our species' future. For starters,
climate change was a planet-scale problem we'd have to tackle sooner or later. That was for sure. And solving that required a
rewiring of the world's energy infrastructure, a multi-trillion dollar industry. I made a note of those, as industries that piqued my interest.
<br><br>
And what about meaning? Future generations would want to find meaning in a life where all our mundane tasks, like doing taxes and
constructing buildings, were encoded away to machines. I thought back to how technology might help people derive a sense of purpose
in life. If you've ever been addicted to a video game, you know how you can wake up, start playing, and realise it's midnight before
you know it. Gaming, I convinced myself, was going to become more immersive and detailed as the technology powering game engines
advanced. Virtual, augmented reality, may change the way we interact with our world. We didn't have to
travel to France or Japan in real life, we merely had to convince our mind that we were doing so. And that was
something immersive video games could do. Another industry to watch.
<br><br>
But how do we guarantee technological progress happens at this fast rate I've assumed? How do we know future generations will be
as ambitious, conniving, hopeful as we are? We must educate them. When I could learn more self-studying over a summer than I could in two
years at the most academically elite high school in the world, it was clear something was wrong. And my work in education equity with CareerFear has bound me to a core belief that the annals of history will most clearly remember not individual achievements, but generational strides: the kind that only come about when everyone is equipped to contribute.
Could technology make education more personal? More accessible? At scale? That was another set of questions I wanted answered. Add edtech to the list.
<br><br>
And these are just industries I've had some exposure to. What are some enormous industries that I'd never had any exposure to?
Well, how about the industry supported by the biggest financial institutions on Earth? Governments themselves. Military and defense
budgets are <i>quite literally </i> orders of magnitude larger than most other industries. They're just more stealthy. Which kid
grows up wanted to build weapons and technology that governments rely on? But that's the very predicate on which our planet's peace
is built--mutually assured destruction. Add that to the list.
<br><br>
And finally, I'd always had a childhood fascination with architecture. Perhaps it was my way of rebelling
against my parents, who so conspicuously were nudging me towards careers in STEM, to half-threaten them with a career in architecture.
Stunning structures like the Burj Khalifa or Golden Gate Bridge would always leave me spellbound. It begged the question:
what would buildings look like in the future? More generally, what would <i> cities </i> look like in the future? Why are we
trying to make autonomous vehicles instead of more walkable cities? How would technology change the way we interact with our urban environments,
and how did that interplay with global warming and mobility?
<br><br>
So I had my list. I wanted to spend a sizeable portion of the year learning about these 6 industries: energy/climate, agriculture/environment,
defense/military, education, gaming/media, and urban/transport technology. All studied through a lens of how software and algorithms could
transform these fields. The goal was the find problems I could spend my college education--or, indeed, a lifetime--solving. These were fields
that could conceivably change the world--and be profitable--but fields that I would never otherwised be exposed to.
I would have plenty of time in Boston to learn about fintech and biotech, but when would I seriously think about feeding the planet,
autonomous weapons, or machine learning as an educational tool? <strong> Never</strong>, <i>unless I took this year to do exactly that.</i>
<br><br>
Originally, I hoped to work for two months in each of these industries, as an intern. Quickly, I spotted a flaw in my reasoning. How much would I <i>really</i>
learn about energy working as a software engineer at an energy-tech company? Talking to employees convinced me that the answer was very little.
For example--building a front-end dashboard to display key metrics is fundamentally the same task, no matter if it's a dashboard for an energy company to look at how their oil drills are doing,
or a biotech company to understand how their drugs are working. Instead, it made more sense to learn about these industries the way I'd learn anything else
--by carefully reading books, scouring the internet, and talking to smart people working in these industries. So everyday, for 4 hours, I read and make notes,
I meet engineers, entrepreneurs, academics working in these spaces from time to time, picking their brain about what they think
the future of their industry is. Nothing has been as rewarding as reading <i> with a purpose. </i>
<br><br>
But what about the other 20 hours a day? Let's say I sleep for 9 of them, and spend 1-2 cooking, cleaning, and generally
adulting. I have around 9 more to play with. What to do? Well, if I found an industry I was interested in, I might conceivably come across
an interesting business idea while in college. After all, the 4 years you spend in college are around brilliant, ambitious peers,
enormously intelligent and accomplished professors, and the university handing you money for your projects on a platter. What better
place to change the world? But if I'm going to transform energy technology, game technology, or, indeed, any other technology, I need to
be a technologist. <br><br>Sadly, high school programming won't cut it. I need to immersively
lose myself in a crucible of tech, and come out with the ability to build out the prototype for any idea I had--whether that was a web app (like Facebook),
mobile app (like Uber), or more complicated software (like Microsoft's Windows). Think of how Gates and Zuckerberg had put in thousands of hours
<i>mastering</i> programming before they even set foot on the Harvard campus.
<br><br>
<i><strong>But Tanishq--you learn all that during college anyways. That's the point of a CS degree, right?</i></strong>
<br><br>No. Wrong. Categorically incorrect.<br><br>
I thought so too. In fact, computer science, as colleges teach it, and software engineering--the act of building the products
we use on a daily basis--have relatively little overlap. This stunned me, and it should shock you too, since most students decide to study
CS not because they want to learn about the central tenets of computational complexity, but because they want to be employed as software engineers afterwards.
When I spoke to software engineers, they told me that their jobs involved <i>only 1 or 2 courses from college</i>,
and the rest was things they learned during internships--stuff like using programming frameworks (Django, React, etc.),
getting accustomed to designing systems (writing lots of code), and studying engineering best practices (learning on the job).
<br><br>
That sealed the deal. The bulk of my day would be spent learning software engineering. How? Maybe an intensive bootcamp? Maybe by
getting a job as a software engineer? Well, to get a job, you have to know how to do it. And bootcamps were educational institutions,
no matter how intense they claimed to be, I was sure I could do it better, faster, myself. And so I lined up some online courses,
a bunch of projects I wanted to build, and got to work. This takes up around 6 hours of my day.
<br><br>
At first glance, it really does seem backwards. I'm taking a year out to...learn programming and read? <i><strong>Isn't that exactly what a college degree,
  majoring in CS, would teach you?<br><br></i></strong><img src="/static/meme1.jpg" height="200" style="display: block; margin-left: auto; margin-right: auto;"><br>
 I'm convinced this is the sort of pattern-matching thinking that
leads people astray in the first place. When you dig into the details, you see that the programming I'm doing is preparation
to build useful, marketable products, not prove abstract computational theorems (a CS degree). The reading I'm doing
is about the nitty-gritty details of industries I may very well end up working in, not grappling with esoteric literature (the kind
you do a lot of in college). Let me be explicit--I am not saying these things aren't important. In a world increasingly
using technology as a crutch, understanding the central philosophies and concepts of human thought is more important now than ever before. I will make sure to do these things
when I'm in college. But before I get there I need to know how to be useful first. So I'm taking a year to sort that out.
<br><br>
<a href="#top">Back To Top</a>
</div> -->


<!-- <h3>On the Financial Institution</h3>
<p >
<i>
Westminster and Harvard both have significant pull towards finance-whether that's at a bulge bracket investment bank (think Goldman Sachs), quantitative hedge fund (think Two Sigma), or fintech startup (think Addepar).
When all your most ambitious and accomplished peers are being sucked into an industry like specks of dust unthinkingly marching up a vacuum's long snout, you can't help but wonder what's so great about the industry in the first place. </i>
<br><br>
At the highest level of abstraction, the financial institution exists to allocate capital efficiently so society can be optimally productive. And, most of the time, it does exactly that. The big banks--yes, the very same ones that are so often vilified by miseducated members of the public--exist simply to help organisations raise capital from people that want to invest. In my opinion, they do this very well.
So much so that the US government bailed out the big banks in the wake of the 2008 crisis because the government recognized that the economy would collapse without them. If no investment banks remained, who would give companies--both big and small--access to public capital? Companies would go bankrupt, millions would be laid off. Disaster. By taking money from taxpayers at the time, the US government funded the survival of some of the big banks,
thereby stabilising the immediate economic future of American society (but also thereby incentivising bad behaviour).
<br><br>
The reason that claims about the value of (investment) banking as an industry seem so political is because even though objective facts about the importance of the industry's role exist, they are <i> incredibly complex</i>.
Perhaps an example will help. Let's say we're debating, say, whether new regulations after the 2008 crisis removed bad actors in the banking industry. Many economists will do deeply researched studies, using advanced models, and come to a conclusion
 that the regulations were, in fact, effective. Looking carefully, we find that herein lies the problem. <i>Equally reputable economists will do studies a slightly differentâ-but equally validâ-way and come out with the <strong>opposite </strong>conclusion</i>, giving both sides "facts" to fuel their respective fires.
<br><br>
  The ex-banker I spoke toâ-who inspired this pieceâ-confessed that he left because he didnât find the work meaningful. The main reason that so many people hate on banking as an industry is not because of bad actors (people embezzling funds/overcharging management fees).
  Sure, these people exist, but they are no more common than in any other industry. In fact, what many people fail to internalise is that most financial products/services are based on complex, but very sound, mathematics and robust legal architecture--even if it, at times, seems unecessarily convoluted.
  Let me restate that for emphasis. The financial products and services people in Silicon Valley find confusing and nebulous are often very effective, well-researched, vehicles to efficiently allocate society's capital.
  The reason people hate banks (and the only justifiable one), is instead that they exist to help the rich get richer. In this sense, the banks arenât directly doing good for the bottom 99%.
<br><br>
  The CFO of, say, Walmart, who wants to raise capital for a new service Walmart is looking to implement, or, say, a $4B startup looking to IPO, clearly think Goldman is worth the $10M fee theyâre charging for their M&A/fundraising services, or else the companies wouldnât pay it. And for the investors/execs at these big companies that stand to make a lot of money when stock prices rise after funds are raised, going with Goldman certainly is worth the $10M. <i>So both the bank and client come </i> out ahead.  No one seems to get âscrewedâ.<strong> Except, of course, the naÃ¯ve member of the public</strong>, who invests in the company looking to raise money without knowing all the facts. Itâs<i>their</i> money that gets channeled into the executivesâ/investors' pockets (there is, after all, only a finite of capital that exists in society). But, of course, the layperson volunteered to invest their money; no-one forced them to do it. They just didnât have an "edge" when betting on the market, and so lost their money. Then complained about it, and scapegoated the "evil corporates". It should be noted that there exceptions, of courseâ-members of the public didn't necessarily make misinformed investments when it came to the 2008 housing bubble, that really was down to miscalculations and greed on behalf of the banks.
<br><br>
  And now to tackle a common misconception: some think that employees benefit when companies raise capital. This is not the case. When a company, say Walmart, raises money, its 2.7 million employees do NOT all get a raise, or more perks. Their professional lives are left unchanged. The only argument that can even be <i> attempted </i> by the aspiring
  banker is that fuelling companies by supplying them with access to capital means that they can provide more value for their customers. More concretely put, if, say, Walmart raises $2B from investors in a transaction mediated by JP Morgan,
  it can use that money to kickstart that new autonomous 1-day delivery service that will then add convenience to millions of Americans and add value to <i> those </i> lives. My response to this argument is: no shit, that's the point of raising capital in the first place. This argument, while undeniably true,
  emphasises value-added that is, in my opinion, too far removed from the everyday human to be clearly meaningful work. The work of individual analysts at JPM and the implementation of the autonomous delivery service are many steps removed from each other, and it's difficult to know if any individual banker had a meaningful role to play
  in the whole arc at all.
<br><br>
  The dull reality is that most bankers are cogs in a machine that is-at best, contributing value that is several steps removed from wider society (like the JPM-Walmart-delivery service), and at worst, actively screwing over millions (like the 2008 crisis). Perhaps
  this becomes more apparent as you begin to bear children of your own. At this point, you start to think more carefully about the effects your actions have on other people; people like your tender, starry-eyed children. Perhaps that's why so many people leave banking with a bad taste in their
  mouths around the ages of 40-50, right as their children hit adolescence and their own futures are financially secure.
</p><hr> -->
<!--
<h3>Hard Work vs Discipline</h3>
<p >
  It's 5am. You jolt awake, exasperated. Frustrated, angry, that you-only 7 hours ago-would have the audacity
  to force your present self out of deep, peaceful sleep. But you're proud that you're up. You don't spend your usual
  30 minutes scrolling mindlessly on Instagram like you're in a semi-comatose state. You're on a mission. Even on the way to the bathroom to brush your teeth,
  you walk with a purpose, as if steadying yourself to begins sprinting in the direction of your dreams. An hour later, you find your
  citrus Nike tee soaked by fruits of your workout.
<br><br>
  Readying yourself for school, you pull your charcoal socks up to the middle of your shin like an athlete deep in focus, religiously lacing up before an
  Olympic final. While others frolick playfully in the dining hall, you give up your lunchtimes to work. After school,
  you lose yourself once again in the deep waters of focus. Only when the sun sets again at dusk do you hesitantly-as if glancing to your left and right before beginning to cross a busy intersection--take a breather. You're exhausted.
  But you've prayed to mantras of "grinding" and "outworking" too long;
  this is not when you let up. Another motivational video pushes you through yet another hour of bleary eyed study. As your city
  slips into the depths of darkness, you do too. Then you do it all over again.
<br><br>
  Welcome to the grind.
<br><br>
  This was my life for two years. In Year 11 and 12 (sophomore & junior year) of high school, I was obsessed. Obsessed with
  getting into Westminster as a way to escape the shackles of a mediocre education, obsessed with academically matching
  the very best in the world.
  To a weary but hopeful mid-teen, the phrase "hard work" was more than another empty truism. It was a lifestyle. I thought
  I was a demi-god, subtly fabricating excuses for missing friends' parties-as if my
  paternal grandfather really <i> was </i> in town for his knee surgey-only to spend quiet Friday evenings at my desk,
  in the depths of concentration. I don't know what I was looking to prove. Perhaps it was a former debater trying to convince himself
  that he was more than just an articulate speaker, that he could compete with the strongest math/science
  students on Earth. Perhaps it was
  an anxious teen at a competitive high school, refusing to "settle" when applying to college.
<br><br>
  What I <i> do </i> know is that I wasn't working the right way. Sure, I got a lot done. Through brute force,
   I completed 2 A-levels in 5 weeks during summer. I scaled an education equity nonprofit while
  sipping undergraduate biology on the side.
<br><br>
  But now, at 18, I look back and cringe. I'm disappointed that I ever really believed that
  "hard work" was what forged greatness. That becoming great at whatever you did necessarily meant devoting every waking hour,
  feeling socially asphixiated, giving up so many things you love, pushing your body to its physical limits.
<br><br>
  How childish.
<br><br>
  Some of you may be shaking your head, curious as to where exactly that logic breaks down. We see Musk touting his 100-hour work week, athletes
  doing nothing but train all day on TV. This is what the world has always said. So, surely, "hard work", does
  in fact mould champions.
<br><br>
  Nope.
<br><br>
  I'll tell you what <i> does</i>. Output. Getting very important things done. Productivity. Putting in enormous amounts of time
  has little bearing on how much you accomplish. This is an empirical observation.
<br><br>
  But how can that be? The more hours you pour into something, the better you'll be. Right? Right. <i>If, and only if,</i> you hold
  intensity (how much you achieve in every hour spent working) constant. In reality, those hours I was awake at 5am, working for two hours before school, I achieved almost nothing.
  I could barely focus. Bleary eyed, I'd stare blinkingly at textbooks, blindly trying to
  retain material like my mom trying desperately to hold up a mountain of clothes at a thrift shop, only to clasp helplessly
   as they fall by the wayside. The amount of work I got done in those 120 minutes was equivalent to around 30 minutes of
  deep, focused work at my peak.
<br><br>
  And to get to that peak consistently, I needed 9+ hours of sleep (personal observation). I needed to hang out with friends.
  And so as I grew older and-allegedly-wiser, I turned in my obsession with "hard work" and motivational videos for something far more nuanced.
  Discipline. During my gap year, I get 11 hours of sleep a night (the natural amount I sleep without setting an alarm), with no regrets.
  What I do make sure to do, however, is to have <i> habits </i> in place to make sure that, during the 13 hours I <i>am</i> awake,
  shit gets done. I split my day into three chunks. 4 hours spend coding, 4 hours spend reading, and 2 hours spent breakdancing.
<br><br>
  Let me clarify. It takes me 6 hours to put in those 4 hours writing code. By 4 hours coding, I really mean 4 hours spent, butt-in-chair,
  focused while typing furiously at the keyboard. I set a timer, and stop it every time I go to the toilet. Every time I get water.
  Every time I take a 10 minute break. And so ultimately, to excise those 8 hours of productivity every day, I have to spend 11-12 hours "working".
<br><br>
  And I'm seeing progress like never before. In 1 hour of silent, uninterrupted work, I can lose myself in the deep waters of focus.
  I get more done in an hour now than in almost a whole day of maniacal "hard work". And I advise you to do the same. Like a sinner
  worriedly creeping into a place of worship, seeking impunity, turn in your childish obsession with "hard work". Supplant it with a more
  mature understanding of the power of discipline and habit. The power of keeping to rituals, and hitting realistic daily goals.
  Every. Single. Day. Hitting the 4-4-2 target 90% of the time is <i>far</i> more productive than consistently
  getting only 50% use out of 16-hour-workdays, no matter how hard Elon Musk says he's working.
<br><br>
  Don't get me wrong. When I'm working on my PhD thesis, or about to pitch for Series A for a startup, I will have to work hard AND
  be disciplined. There are moments in life where you should sacrifice your social life and health for peak intelletual performance.
  At those points, I will have to regularly execute 16-hour-days at 100% efficiency. But I cannot do that
  for years on end. Anyone who says they can is lying to you. In those 16 hours, they probably get 8-9 hours of real work done, just
  as I do.
<br><br>
  Except, of course, I do it while sleeping 11 hours a day :)
</p><hr> -->

<!--
<h3>On Parenthood and Conversation</h3>
<p >
  <i>
A lone silver molar, nestled comfortably inside an Afghani smile, glistened in the rear-view mirror of my Uber, almost as if beckoning the midnight moon to outshine it. Outside, the night sky sat still, washing the heavens with black. As we spoke, it was as if he and I were the only two people in the world.
</i>
<br /><br />
The ability to have sustained, intense, meandering conversations is, in my opinion, one of humanityâs most distinctive traits. The stones of conversation that most leave unturned stand out to me, gleaming in with potential, begging to see the light of day.
I think that there is a conversation special to every set of peopleâone conversation, at least, that only a given set of people can have at a given place at a given time. Despite my imperfections, many of which I am working <i>desperately </i>to address, I think one thing I <i>am </i>very good at is finding that conversation.
<br /><br />
But this gift is not my own. It is, as with most other things, merely a product of good circumstance. A result of a lottery of sortsâthe âovarian lotteryâ, as Warren Buffet puts it. My father has always owned this gift, this spellbinding ability to have
interesting conversations with strangers, to engage people and make them feel valued. Every interaction I witnessed him take part in was a verbal seduction, of sorts. Watching him lead a discussion, no matter who with,
was to witness the slowest of surrenders, the kind you want to relive over and over again, as if rewatching a magicianâs hand, desperate to catch the moment of slight.
<br /><br />
But there is no trick here.
<br /><br />
It was with Afghani taxi drivers and Greek tablestaff, Kenyan construction workers and Canadian colleagues that he wove his conversational magic. I was a young, impressionable eight year old, bewitched by his charisma, watching him
masterfully play lexical chess, taking ritualistic small talk and seamlessly transfiguring it into meaningful, memorable conversation; conversation where the other person felt important, special, singularly valued and understood. Unwittingly, they
would reveal, as if hesitantlyâbut with a whisper of reliefâtheir stories, backgrounds, experiences, excited to share a glimpse of the raw, unfettered beauty dancing silently inside their skull.
Through this, he learned what it was like growing up in a civil-war stricken Kabul with a single mother, and why an Athenian man regrets leaving his lover at the alter. Vicariously, he lived in Nairobi as the son of a famous businessman who was cast out of the house
at fourteen, and as a science-obsessed young girl in Toronto whose only ambition in life was to become a pediatric surgeon.
<br /><br />
He lived a million lives in one.
<br /><br />
I wanted little more to have that ability, to weave that black magic and enchant powerless onlookers with games of words and phrases and hypnotic syntactic trickery.
<br /><br />
I wanted little more than to be like him.
<br /><br />
And slowly, I learned. I learned to ask questions when others would be silent, to disagree and challenge when others would back down. I learned to poke and prod, and lose myself in the deep waters of the other personâs life,
imbibing their experiences as if sipping the nectar of forbidden fruit; fruit that was hidden and ugly on the surface, but bracing and delicious inside. I learned to ask weird questions and give weird answers, to listen and understand as if I was
to be tested on the details of the speakerâs life. Day in and day out, living with my father would beat these subtle habits into me until I graduated from the rank of apprentice, and started to weave spells of my own.
<br /><br />
And I didnât realise this until today, while taking an Uber back from the airport with, well, an Afghani driver. Because of this gift, Iâve been blessed with an understanding of what itâs like being a Lebanese matriarch running a San Francisco cafÃ©, and
what itâs like being a Gambian immigrant struggling to make ends meet in an extended family. Through conversation, Iâve looked the eyes of south Indian husbands as they catch sight of their arranged wives-to-be,
and smelled the terrifying scent of lead lining the roads of a war-torn Aleppo. These conversations have been hidden in Uber drives from the airportâsureâbut theyâve also worn the mask of hairdressers shaving my head and waiters blowing clouds while on break,
nervous algorithmists coding furiously in the solace of libraries, and unruly breakdancers taking a rare moment to reflect on childhoods spent dancing with their grandparents.
<br /><br />
And all of these are strands of a human narrative I wouldâve been powerless to play with if it werenât for my fatherâs gift, and my childhood spent watching him exercise it. This is why I believe that your parents control a large part of your fate.
By that, I don't mean that their wealth defines your education and opportunities.
No, I mean instead that nepotism most wreaks havoc by perpetuating skills that canât be learned online. If someone shy and antisocial tried to hone their ability to weave memorable conversations with virtual strangers,
it would be very hard to catch up with me simply because I have a near two-decade head start. And that comes down to who my parents were, and how they behaved as I grew up around them.
Equally, my friends wield gifts I can only dream of beholding. I know boys who walk with blind, iron-clad confidence, girls who can seduce on a dime. They bear the gifts of <i>their </i>parents, and the years they spent learning them through
the ultimate apprenticeship-the one we call âchildhoodâ. So when you see someone command an audience with ease, delicately but assuredly toying with its attention and emotions, don't write them off as someone who was âborn-greatâ at public speaking.
That is not the case. It was years of practise, of watching it being done right from one role model or another, or screwing up and doing it againâall inadvertently, subconsciously, informally, before they got up on that stage and mesmerised us.
<br /><br />
This is not an essay meant to discourage. Quite the opposite. It should be empowering to know that there is no protein encoded for by any gene that makes a human charismatic. Everyone learns it the hard wayâsome people just earlier and quicker than others. So if you canât
command attention like Jobs or respect like Bond, it doesnât mean you canât be a better version of you. Much like their childhoods forced upon them thousands of hours of unwitting practise, you can put yourself through those hours too, consciously, knowing the results that await you at the end.
<br /><br />
So, go ahead. Talk to the old man sitting across you on the train, ask the cashier how her day <i>really </i>was. Push a little bit more than youâre comfortable with when your Uber driver is telling you about their background, use every opportunity to let people talk about themselves.
<br /><br />
They might surprise you.
<br /><br />
</p><hr> -->



<!-- <h3>Progress, Purpose, Impact</h3>
<p >
  The central question of what one should do with their life is so daunting that it is often inadvertently left as a
  decision to be made spontaneously and instinctively, instead of an ongoing internal thesis to formulate, grapple with, unpack.
  In this piece, I'd like to examine a few articles sent to me by a friend studying philosophy at
  Cambridge, surveying both the social scientific literature and--no less important--arguments from the humanities.
  The analysis will go through three articles, and addresses questions centred around two themes:
  </p>
  <ul>
    <li>
      What do most people want from life and how do they go about getting it? That is, why do we make the career choices we do?
      What is the relationship between the ideas, values and objects that people crave and cherish and the path they end up taking in life?
    </li>
    <li>
      What are the best ways of achieving "social impact" as an individual? What are some surprising truths and falsities about "making a difference"? How should we
       navigate choosing the highest leverage actions? Why aren't we already doing these things?
    </li>
  </ul>
  <p >
    The end goal is to come up with tentative answers to these questions; or, at the very least, other important
    questions to be explored. I also hope to make some specific, actionable recommendations,
    steering clear of philosophical waxing.
  </p>
<ul>
  <li><strong>Introduction to Effective Altruism</strong></li>
  <li><strong>80,000 Hours -- Guide to Choosing a Career</strong></li>
  <li><strong>Being Good in a World of Need (Journal of Practical Ethics) </strong></li>
</ul>

</p><hr> -->

<!-- <h3>On Failure and Rejection</h3>
<p > -->

<!--
american unis are circumstancial vs completely predictable UK/India/Chinese unis
depends on whether you pass the bar for being very accomplished, sure, and it is a spectrum
where being more accomplished increases your odds. if you're IMO gold medallist or the daughter
or Xi Jinping, you WILL get in. but most people who get in fit neither category-some just got in
because they needed a particular combination of traits to round off the class: a banjo player who is also
interested in chemistry, for example. and so you see people objectively more ambitious, hard-working, accomplished,
interesting, and qualified, get rejected left and right. conversely, you see some who fall short in all those categories
get in, for one reason or another. it is NOT a meritocracy, and so you should be outcome independent.
 -->
<!-- </p><hr> -->





<!-- <h3>Great Rivalries</h3>
<p >
  [TODO]<br>
  Here, I'm mainly interested in piecing together how and why frictions and clashes 
  between great people lead to the production of so much important, seminal work, 
  and how we might replicate that in our own lives. 
  <li>Jobs / Gates</li>
  <li>Federer / Nadal</li>
  <li>Churchill / Lloyd George</li>
  <li>Eistein / Hilbert</li>
  <li>Beethoven / Mozart</li>
  <li>Newton / Hooke</li>
  <li>Da Vinci / Michelangelo</li>
  <li>Caesar / Pompey</li> -->

  <!-- content (differences between me and the standard, elite SWE: 1) knowing that finding problems to
solve is more important than coming up w/ clever solutions 2) comfortable pushing myself into awkward/uncomfortable situations, particularly
the social type that forces character growth) -->
<!-- </p>
<hr> -->



<!-- 
<h3>Hidden Work</h3>
<p >
[TODO]<br>
It's very easy to think that people that are great at things are naturally great at them. Of course, we'll publicly
acknowledge how much hard work goes on behind the scenes to forge that exceptional ability, and sometimes we'll even 
convince ourselves that we believe it, but I think most of us, most of the time, don't have any clear idea of what that 
means or looks like. I thought a lot about this after reading a scene in <i>The Wise Man's Fear</i> (second book in my 
all time favorite fiction series), where the protagonist spends four hours rehearsing how to flick his wrist as he bows
to impress nobility (he is of low-birth in the novels). <br><br>
When I first read it, I laughed out loud, but the scene dances a delicate tango around my skull as I go through 
other books (non-fiction) that hint at similar situations. And I only got around to writing this when, again, reminded of 
much work and thought goes into the smallest things watching a French comedian rehearse different ways of saying 
"Goodnight, Paris!" to get it perfect. Here are some examples of hidden work, to serve as reminders of how 
most things that seem natural, even obvious, to onlookers, took thousands of hours to get just right. 
</p><hr> -->



<!-- <h3>Life and Linear Algebra</h3>
<p >
content (n-dimensional vector space, orthogonal)
</p><hr> -->

<!-- <h3>On Front-End Engineering</h3>
<p >

content: sure, front-end can be really technically hard at times, but ultimately, you're working
on how a FUCKING BUTTON LOOKS or a certain animation is on a page, not the actual product 
</p><hr> -->

<!-- <h3>On Silicon and Gold</h3>
<p >
content: ability to exchange ideas without hesitation-how people have conversation first
and then network later; more about learning than slime, ubiquitous understanding and spoken language
of why startups make decisions the way they do because it's so far ahead rest of the world and
there's a shared understanding of brest practices; but also groupthink on that note 
</p><hr> -->

<!-- <h3>Europe vs America</h3>
<p >
content: Ambition, inequality, intensity, status-driven society VS a more horizontal, laid-back, equal society.
which is better in terms of QoL, happiness, peace, innovation? which is better for different personality types? to live in? work in? 
</p><hr> -->

<!-- <h3>Coconuts</h3>
<p >
content (differences between me and the standard, elite SWE: 1) knowing that finding problems to
solve is more important than coming up w/ clever solutions 2) comfortable pushing myself into awkward/uncomfortable situations, particularly
the social type that forces character growth)
</p><hr> -->

<!-- <h3>Girls' Lives Are Really Different!</h3>
<p > -->
<!-- content (differences between me and the standard, elite SWE: 1) knowing that finding problems to
solve is more important than coming up w/ clever solutions 2) comfortable pushing myself into awkward/uncomfortable situations, particularly
the social type that forces character growth) -->
<!-- </p><hr> -->

<!-- <h3>The Anatomy of Posterity</h3>
<p >
content (differences between me and the standard, elite SWE: 1) knowing that finding problems to
solve is more important than coming up w/ clever solutions 2) comfortable pushing myself into awkward/uncomfortable situations, particularly
the social type that forces character growth)
</p><hr> -->

<!-- <h3>Networking in the Bay Area</h3>
<p >
content (differences between me and the standard, elite SWE: 1) knowing that finding problems to
solve is more important than coming up w/ clever solutions 2) comfortable pushing myself into awkward/uncomfortable situations, particularly
the social type that forces character growth)
</p><hr> -->

<!-- <h3>Dopamine Integration</h3>
<p >
content (differences between me and the standard, elite SWE: 1) knowing that finding problems to
solve is more important than coming up w/ clever solutions 2) comfortable pushing myself into awkward/uncomfortable situations, particularly
the social type that forces character growth)
</p><hr> -->

<!-- <h3>Something Casual</h3>
<p >
content (differences between me and the standard, elite SWE: 1) knowing that finding problems to
solve is more important than coming up w/ clever solutions 2) comfortable pushing myself into awkward/uncomfortable situations, particularly
the social type that forces character growth)
</p><hr> -->

<!-- <h3>Understanding the Nebraskan Salesman</h3>
<p >
content (differences between me and the standard, elite SWE: 1) knowing that finding problems to
solve is more important than coming up w/ clever solutions 2) comfortable pushing myself into awkward/uncomfortable situations, particularly
the social type that forces character growth)
</p><hr> -->


</div>

</body>


<script>
var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    if (panel.style.display === "block") {
      panel.style.display = "none";
    } else {
      panel.style.display = "block";
    }
  });
}
</script>

<style>
.panel {
  padding: 0 18px;
  background-color: black;
  color: white;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.2s ease-out;
}
</style>

<script>
var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    if (panel.style.maxHeight) {
      panel.style.maxHeight = null;
    } else {
      panel.style.maxHeight = panel.scrollHeight + "px";
    }
  });
}
</script>

</html>
