<html>
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153791322-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-153791322-1');
  </script>
    <!-- endof Global site tag (gtag.js) - Google Analytics -->



<style>
.accordion {
  background-color: #000;
  color: #fff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: 1px solid white;
  /* border-top: 0.1px solid black; */
  text-align: left;
  outline: none;
  font-size: 15px;
  transition: 0.4s;
}

.active, .accordion:hover {
  background-color: #000; 
}

/* panel css and js at bottom */

li.sick {
  color: rgb(38, 150, 255);
}

li.audited {
  color: rgb(52,168,83);
}

</style>




  <link rel="stylesheet" href="/static/style.css">
  <meta charset="utf-8">
  <title>Papers · Tanishq Kumar</title>
</head>
<body>
<div id="menu">
<span class="title">Tanishq Kumar</span>
<ul>
  <li><a href="/index.html">Home</a></li>
  <li><a href="/about.html">About</a></li>
  <!-- <li><a href="/culture">Culture</a></li> -->
  <!-- <li><a href="/books.html">Bookshelf</a></li> -->
  <li><a href="/essays.html">Writing</a></li>
    <!-- <li><a href="/notes.html">Notes</a></li> -->
    <!-- <li><a href="/articles.html">Articles</a></li> -->
      <li><a href="/courses.html">Courses</a>
        <li><a href="/papers.html">Research</a>
</ul>
</div>
<div id="left"></div>
<div id="content">

    <h1>Research</h1>
    <p>I work on the science of deep learning. 
      Most recently I have been thinking about synthetic data and statistical models of evaluations.
    I've had the great privilege of learning to do science from <a href="https://scholar.google.com/citations?user=0HuMHFwAAAAJ&hl=en">Sam Gershman</a> and <a href="https://scholar.google.com/citations?user=Ch9iRwQAAAAJ&hl=en">Aditi Raghunathan</a>.
    </p>
    <button class="accordion" style="width: 200px;">Research Philosophy</button>
    <div class="panel">
      <p>
        I got my start in deep learning only 1.5 years ago, at which point I was rather more interested in neuroscience. My research interests in deep learning include 
        <ul>
          <li>What <i>systems</i> centered around foundation models will look like in 5-10 years, and how they will touch our lives in unthinkable ways
            <ul>
              <li>In my opinion, understanding this requires being familiar with everything from pre and post training to inference and deployment
                to have a good sense of what can improve in what ways and where there are structural limitations. 
                This is what drives me to work at all levels of the stack.</li>
              <li>Two very high level questions that particularly motivate the concrete research questions I study 
                <ul>
                  <li>What will we scale after the well of inference-time compute dries up? RAG? Multi-agent systems? Data quality via synthetic data?</li>
                  
                  <li>I think the answer to the above question depends a lot on how foundation models are being deployed in a year from now, and how we humans want to use them.
                    <ul>
                      <li>Will 90% of all forward passes be done inside agentic pipelines that orchestrate me getting a pizza I ordered, or in sales agents responding to my refund request on Amazon?</li>
                      <li>Are math/software uniquely verifiable, and will FMs accelerate those two domains disproportionately?</li>
                    </ul>
                </ul>
              </li>
            </ul>
          </li>
          
          <li>Similarities and differences between artificial and natural learning systems
            <ul>
              <li>My view is roughly that deep learning tells us the right <i>questions</i> to experimentally test in biological systems</li>
              <li>I have mostly worked at Marr level 2 and on neuroscience (of both brains and artificial networks), but am coming more to think Marr level 3/the cognitive level of abstraction may be the right level to understand these systems</li>
              <li>The questions I find interesting often center around universality 
                <ul>
                  <li>How does the brain learn without backprop? The fact that both gradient (LMs) and non-gradient methods (brains) work at scale means that the magic is in the function class (neural networks) being optimized, rather than the optimization method</li>
                  <li>Relatedly, to what extend are representations learned by brains and machine universal, and why? My work has touched on this, and the implicit bias literature in ML 
                    offers one type of answer, but for some reason I never felt satisfied with this.</li>
                  <li>When we see our foundation models make mistakes and reason in a similar way to humans, is that a sign of something deeper at play 
                    or merely us anthropomorphizing these systems? 
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
        
        
      </p>

        <p>
        I think deep learning moves fast enough that anyone with strong fundamentals in mathematics 
        and good engineering skills can contribute to the frontier. I have been fortunate to be 
        mentored by folks working on topics ranging from foundation model pretraining to high-dimensional probability to 
        mouse olfaction. For exposure to this diversity of experiences and points of view I am immensely grateful. </p>
    </div>

      

    
    </p>

  <button class="accordion" style="width: 200px;">Papers</button>
  <div class="panel">
    <ul>
      
      <li><a href="https://arxiv.org/abs/2411.04330">Scaling Laws for Precision</a></li>
      <strong>In Submission.</strong><br><u>Tanishq Kumar*</u>, Zachary Ankner*, Benjamin F. Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, Aditi Raghunathan.<br><br>
      
      <li><a href="https://arxiv.org/pdf/2411.03541v1">Do Mice Grok? Unveiling Hidden Progress in Sensory Cortex During Overtraining</a></li>
      <strong>In Submission.</strong><br><u>Tanishq Kumar</u>, Blake Bordelon, Cengiz Pehlevan, Venkatesh Murthy, Samuel J. Gershman.<br><br>
      
      <li><a href="https://tanishqkumar.github.io/papers.html">Lower Data Diversity Accelerates Training: Case Studies in Synthetic Tasks</a></li>
      <strong>In Submission.</strong><br>Suhas Kotha*, Uzay Girit*, <u>Tanishq Kumar*</u>, Gauran Ghosal, Aditi Raghunathan.<br><br>

      <li><a href="./papers.html">Asymptotic Dynamics for Delayed Feature Learning on a Toy Model</a></li>
      <strong>HiLD at ICML 2024.</strong><br>Blake Bordelon, <u>Tanishq Kumar</u>, Samuel J. Gershman, and Cengiz Pehlevan.<br><br>

      <li><a href="https://arxiv.org/pdf/2402.01089.pdf">No Free Prune: Information-Theoretic Barriers to Pruning at Initialization</a></li>
      <strong>ICML 2024.</strong><br><u>Tanishq Kumar*</u>, Kevin Luo*, Mark Sellke.<br><br>

      <li><a href="https://arxiv.org/abs/2310.06110">Grokking as the Transition from Lazy to Rich Training Dynamics</a></li>
      <strong>ICLR 2024.</strong><br><u>Tanishq Kumar</u>, Blake Bordelon, Samuel J. Gershman*, Cengiz Pehlevan*.<br><br>

      <li><a href="https://arxiv.org/abs/2211.13087">Human or Machine? Turing Tests for Vision and Language</a></li>
      <strong>arXiv.</strong><br>Mengmi Zhang, ... <u>Tanishq Kumar</u>, ... Gabriel Kreiman.<br><br>
    </ul>
  </div>





</div>

</body>


<script>
var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    if (panel.style.display === "block") {
      panel.style.display = "none";
    } else {
      panel.style.display = "block";
    }
  });
}
</script>

<style>
.panel {
  padding: 0 18px;
  background-color: black;
  color: white;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.2s ease-out;
}
</style>

<script>
var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    if (panel.style.maxHeight) {
      panel.style.maxHeight = null;
    } else {
      panel.style.maxHeight = panel.scrollHeight + "px";
    }
  });
}
</script>

</html>



