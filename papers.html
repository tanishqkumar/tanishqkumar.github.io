<html>
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153791322-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-153791322-1');
  </script>
    <!-- endof Global site tag (gtag.js) - Google Analytics -->



<style>
.accordion {
  background-color: #000;
  color: #fff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: 1px solid white;
  /* border-top: 0.1px solid black; */
  text-align: left;
  outline: none;
  font-size: 15px;
  transition: 0.4s;
}

.active, .accordion:hover {
  background-color: #000; 
}

/* panel css and js at bottom */

li.sick {
  color: rgb(38, 150, 255);
}

li.audited {
  color: rgb(52,168,83);
}

</style>




  <link rel="stylesheet" href="/static/style.css">
  <meta charset="utf-8">
  <title>Papers · Tanishq Kumar</title>
</head>
<body>
<div id="menu">
<span class="title">Tanishq Kumar</span>
<ul>
  <li><a href="/index.html">Home</a></li>
  <li><a href="/about.html">About</a></li>
  <!-- <li><a href="/culture">Culture</a></li> -->
  <!-- <li><a href="/books.html">Bookshelf</a></li> -->
  <li><a href="/essays.html">Writing</a></li>
    <!-- <li><a href="/notes.html">Notes</a></li> -->
    <!-- <li><a href="/articles.html">Articles</a></li> -->
      <li><a href="/courses.html">Courses</a>
        <li><a href="/papers.html">Research</a>
</ul>
</div>
<div id="left"></div>
<div id="content">

    <h1>Research</h1>
    <p>I work on the science of deep learning. Most recently I have been thinking about synthetic data.</p>
    <button class="accordion" style="width: 200px;">Research Philosophy</button>
    <div class="panel">
      <p>
        I work on the science of deep learning and flit between two largely disjoint communities who care about neural networks. 
          <br><br>
        One sees foundation models as oracles to automate the enterprise; this community thinks in terms of KV caches 
          and fast CUDA kernels, in terms of prefill and decoding and ensuring low latency user experiences for billions of 
          users around the world being exposed to the magic of next-token prediction for the first time. This one 
          might say a neural network is roughly a composed set of matrix multiplications done by GPUs. 
          <br><br>
          The other sees neural networks as a new Platonic object. This community thinks in terms of asymptotics and approximations, 
          in terms of kernel methods and VC dimension, of neural networks as coupled units of computation where loss functions 
          are just energy functions in disguise and high-dimensional optimization landscapes are the core object of inquiry.
          This one would say neural networks are a type of parametric function class that is unexpectedly expressive. 
            <br><br>
          I feel lucky to be part of both. 
      </p>
    </div>

      

    
    </p>

  <button class="accordion" style="width: 200px;">Papers</button>
  <div class="panel">
    <ul>
      <li><a href="https://arxiv.org/abs/2411.04330">Scaling Laws for Precision</a></li>
      <strong>arXiv.</strong><br><u>Tanishq Kumar*</u>, Zachary Ankner*, Benjamin F. Spector, Blake Bordelon, Cengiz Pehlevan, Christopher Ré, Aditi Raghunathan.<br><br>

      <li><a href="https://arxiv.org/pdf/2411.03541v1">Do Mice Grok? Unveiling Hidden Progress in Sensory Cortex During Overtraining</a></li>
      <strong>arXiv.</strong><br><u>Tanishq Kumar</u>, Blake Bordelon, Cengiz Pehlevan, Venkatesh Murthy, Samuel J. Gershman.<br><br>

      <li><a href="./papers.html">Asymptotic Dynamics for Delayed Feature Learning on a Toy Model</a></li>
      <strong>HiLD at ICML 2024.</strong><br>Blake Bordelon, <u>Tanishq Kumar</u>, Samuel J. Gershman, and Cengiz Pehlevan.<br><br>

      <li><a href="https://arxiv.org/pdf/2402.01089.pdf">No Free Prune: Information-Theoretic Barriers to Pruning at Initialization</a></li>
      <strong>ICML 2024.</strong><br><u>Tanishq Kumar*</u>, Kevin Luo*, Mark Sellke.<br><br>

      <li><a href="https://arxiv.org/abs/2310.06110">Grokking as the Transition from Lazy to Rich Training Dynamics</a></li>
      <strong>ICLR 2024.</strong><br><u>Tanishq Kumar</u>, Blake Bordelon, Samuel J. Gershman*, Cengiz Pehlevan*.<br><br>

      <li><a href="https://arxiv.org/abs/2211.13087">Human or Machine? Turing Tests for Vision and Language</a></li>
      <strong>arXiv.</strong><br>Mengmi Zhang, ... <u>Tanishq Kumar</u>, ... Gabriel Kreiman.<br><br>
    </ul>
  </div>





</div>

</body>


<script>
var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    if (panel.style.display === "block") {
      panel.style.display = "none";
    } else {
      panel.style.display = "block";
    }
  });
}
</script>

<style>
.panel {
  padding: 0 18px;
  background-color: black;
  color: white;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.2s ease-out;
}
</style>

<script>
var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    if (panel.style.maxHeight) {
      panel.style.maxHeight = null;
    } else {
      panel.style.maxHeight = panel.scrollHeight + "px";
    }
  });
}
</script>

</html>
