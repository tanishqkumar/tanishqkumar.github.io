<html>
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153791322-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-153791322-1');
  </script>
    <!-- endof Global site tag (gtag.js) - Google Analytics -->



<style>
.accordion {
  background-color: #000;
  color: #fff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: 1px solid white;
  /* border-top: 0.1px solid black; */
  text-align: left;
  outline: none;
  font-size: 15px;
  transition: 0.4s;
}

.active, .accordion:hover {
  background-color: #000; 
}

/* panel css and js at bottom */

li.sick {
  color: rgb(38, 150, 255);
}

li.audited {
  color: rgb(52,168,83);
}

</style>




  <link rel="stylesheet" href="/static/style.css">
  <meta charset="utf-8">
  <title>Papers · Tanishq Kumar</title>
</head>
<body>
<div id="menu">
<span class="title">Tanishq Kumar</span>
<ul>
  <li><a href="/index.html">Home</a></li>
  <li><a href="/about.html">About</a></li>
  <!-- <li><a href="/culture">Culture</a></li> -->
  <!-- <li><a href="/books.html">Bookshelf</a></li> -->
  <li><a href="/essays.html">Writing</a></li>
    <!-- <li><a href="/notes.html">Notes</a></li> -->
    <!-- <li><a href="/articles.html">Articles</a></li> -->
      <li><a href="/courses.html">Courses</a>
        <li><a href="/papers.html">Research</a>
</ul>
</div>
<div id="left"></div>
<div id="content">

    <h1>Research</h1>
    <p>I work on the science of deep learning. Most recently I have been thinking about synthetic data.</p>
    <button class="accordion" style="width: 200px;">Research Philosophy</button>
    <div class="panel">
      <p>
        I flit between two largely disjoint communities who care about neural networks. 
          <br><br>
        

        The first sees foundation models as oracles to automate the enterprise; this community thinks in terms of KV caches 
          and fast CUDA kernels, in terms of prefill and decoding and ensuring low latency user experiences for billions of 
          users around the world being exposed to the magic of next-token prediction for the first time. This one 
          might say a neural network is roughly a composed set of matrix multiplications done on tensor cores. 
          This community speaks to me because of the year I spent in the Bay Area working on software before college. 
        
          <br><br>
          The second is comprised mostly of physicists and mathematicians who care about neural and cognitive systems. 
          This community sees neural networks as new Platonic objects. This community thinks in terms 
          of mean-field approximations and kernel methods, considering 
           neural networks as coupled units of computation, and high-dimensional loss (energy)
           landscapes are the core object of inquiry.
          This one would say neural networks are a type of parametric function class that is unexpectedly expressive, and 
          speaks to me because the phenomenology of using simple mathematical models to make non-obvious yet true predictions 
          about systems at scale is tantalizing. 
            <br><br>

            I feel immensely lucky to have been mentored by people in both.
      </p>
    </div>

      

    
    </p>

  <button class="accordion" style="width: 200px;">Papers</button>
  <div class="panel">
    <ul>
      <li><a href="https://arxiv.org/abs/2411.04330">Scaling Laws for Precision</a></li>
      <strong>arXiv.</strong><br><u>Tanishq Kumar*</u>, Zachary Ankner*, Benjamin F. Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, Aditi Raghunathan.<br><br>

      <li><a href="https://arxiv.org/pdf/2411.03541v1">Do Mice Grok? Unveiling Hidden Progress in Sensory Cortex During Overtraining</a></li>
      <strong>arXiv.</strong><br><u>Tanishq Kumar</u>, Blake Bordelon, Cengiz Pehlevan, Venkatesh Murthy, Samuel J. Gershman.<br><br>

      <li><a href="./papers.html">Asymptotic Dynamics for Delayed Feature Learning on a Toy Model</a></li>
      <strong>HiLD at ICML 2024.</strong><br>Blake Bordelon, <u>Tanishq Kumar</u>, Samuel J. Gershman, and Cengiz Pehlevan.<br><br>

      <li><a href="https://arxiv.org/pdf/2402.01089.pdf">No Free Prune: Information-Theoretic Barriers to Pruning at Initialization</a></li>
      <strong>ICML 2024.</strong><br><u>Tanishq Kumar*</u>, Kevin Luo*, Mark Sellke.<br><br>

      <li><a href="https://arxiv.org/abs/2310.06110">Grokking as the Transition from Lazy to Rich Training Dynamics</a></li>
      <strong>ICLR 2024.</strong><br><u>Tanishq Kumar</u>, Blake Bordelon, Samuel J. Gershman*, Cengiz Pehlevan*.<br><br>

      <li><a href="https://arxiv.org/abs/2211.13087">Human or Machine? Turing Tests for Vision and Language</a></li>
      <strong>arXiv.</strong><br>Mengmi Zhang, ... <u>Tanishq Kumar</u>, ... Gabriel Kreiman.<br><br>
    </ul>
  </div>





</div>

</body>


<script>
var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    if (panel.style.display === "block") {
      panel.style.display = "none";
    } else {
      panel.style.display = "block";
    }
  });
}
</script>

<style>
.panel {
  padding: 0 18px;
  background-color: black;
  color: white;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.2s ease-out;
}
</style>

<script>
var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    if (panel.style.maxHeight) {
      panel.style.maxHeight = null;
    } else {
      panel.style.maxHeight = panel.scrollHeight + "px";
    }
  });
}
</script>

</html>



